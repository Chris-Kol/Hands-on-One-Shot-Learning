{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Turing Machines: Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from time import time\n",
    "from tensorboardX import SummaryWriter\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's define controller part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, num_hiddens):\n",
    "        super(Controller, self).__init__()\n",
    "\n",
    "        print(\"--- Initialize Controller\")\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hiddens)\n",
    "        self.fc2 = nn.Linear(num_hiddens, num_outputs)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Initialize the linear layers\n",
    "        nn.init.xavier_uniform_(self.fc1.weight, gain=1.4)\n",
    "        nn.init.normal(self.fc1.bias, std=0.01)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.fc2.weight, gain=1.4)\n",
    "        nn.init.normal(self.fc2.bias, std=0.01)\n",
    "\n",
    "    def forward(self, x, last_read):\n",
    "\n",
    "        x = torch.cat((x, last_read), dim=1)\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory Part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(nn.Module):\n",
    "    def __init__(self, M, N, controller_out):\n",
    "        super(Memory, self).__init__()\n",
    "\n",
    "        self.N = N\n",
    "        self.M = M\n",
    "        self.read_lengths = self.N + 1 + 1 + 3 + 1\n",
    "        self.write_lengths = self.N + 1 + 1 + 3 + 1 + self.N + self.N\n",
    "        self.w_last = []\n",
    "        self.reset_memory()\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.w_last\n",
    "\n",
    "    def reset_memory(self):\n",
    "        self.w_last = []\n",
    "        self.w_last.append(torch.zeros([1, self.M], dtype=torch.float32))\n",
    "\n",
    "    def address(self, k, beta, g, s, gamma, memory, w_last):\n",
    "        # Content focus\n",
    "        wc = self._similarity(k, beta, memory)\n",
    "        # Location focus\n",
    "        wg = self._interpolate(wc, g, w_last)\n",
    "        w_hat = self._shift(wg, s)\n",
    "        w = self._sharpen(w_hat, gamma)\n",
    "\n",
    "        return w\n",
    "\n",
    "    def _similarity(self, k, beta, memory):\n",
    "        # Similarit√† coseno\n",
    "        w = F.cosine_similarity(memory, k, -1, 1e-16)\n",
    "        w = F.softmax(beta * w, dim=-1)\n",
    "        return w\n",
    "\n",
    "    def _interpolate(self, wc, g, w_last):\n",
    "        return g * wc + (1 - g) * w_last\n",
    "\n",
    "    def _shift(self, wg, s):\n",
    "        result = torch.zeros(wg.size())\n",
    "        result = _convolve(wg, s)\n",
    "        return result\n",
    "\n",
    "    def _sharpen(self, w_hat, gamma):\n",
    "        w = w_hat ** gamma\n",
    "        w = torch.div(w, torch.sum(w, dim=-1) + 1e-16)\n",
    "        return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadHead(Memory):\n",
    "\n",
    "    def __init__(self, M, N, controller_out):\n",
    "        super(ReadHead, self).__init__(M, N, controller_out)\n",
    "\n",
    "        print(\"--- Initialize Memory: ReadHead\")\n",
    "        self.fc_read = nn.Linear(controller_out, self.read_lengths)\n",
    "        self.reset_parameters();\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Initialize the linear layers\n",
    "        nn.init.xavier_uniform_(self.fc_read.weight, gain=1.4)\n",
    "        nn.init.normal(self.fc_read.bias, std=0.01)\n",
    "\n",
    "    def read(self, memory, w):\n",
    "        \"\"\"Read from memory (according to section 3.1).\"\"\"\n",
    "        return torch.matmul(w, memory)\n",
    "\n",
    "    def forward(self, x, memory):\n",
    "        param = self.fc_read(x)\n",
    "        k, beta, g, s, gamma = torch.split(param, [self.N, 1, 1, 3, 1], dim=1)\n",
    "\n",
    "        k = F.tanh(k)\n",
    "        beta = F.softplus(beta)\n",
    "        g = F.sigmoid(g)\n",
    "        s = F.softmax(s, dim=1)\n",
    "        gamma = 1 + F.softplus(gamma)\n",
    "\n",
    "        w = self.address(k, beta, g, s, gamma, memory, self.w_last[-1])\n",
    "        self.w_last.append(w)\n",
    "        mem = self.read(memory, w)\n",
    "        return mem, w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### write memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WriteHead(Memory):\n",
    "\n",
    "    def __init__(self, M, N, controller_out):\n",
    "        super(WriteHead, self).__init__(M, N, controller_out)\n",
    "\n",
    "        print(\"--- Initialize Memory: WriteHead\")\n",
    "        self.fc_write = nn.Linear(controller_out, self.write_lengths)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Initialize the linear layers\n",
    "        nn.init.xavier_uniform_(self.fc_write.weight, gain=1.4)\n",
    "        nn.init.normal_(self.fc_write.bias, std=0.01)\n",
    "\n",
    "    def write(self, memory, w, e, a):\n",
    "        \"\"\"write to memory (according to section 3.2).\"\"\"\n",
    "        w = torch.squeeze(w)\n",
    "        e = torch.squeeze(e)\n",
    "        a = torch.squeeze(a)\n",
    "\n",
    "        erase = torch.ger(w, e)\n",
    "        add = torch.ger(w, a)\n",
    "\n",
    "        m_tilde = memory * (1 - erase)\n",
    "        memory_update = m_tilde + add\n",
    "\n",
    "        return memory_update\n",
    "\n",
    "    def forward(self, x, memory):\n",
    "        param = self.fc_write(x)\n",
    "\n",
    "        k, beta, g, s, gamma, a, e = torch.split(param, [self.N, 1, 1, 3, 1, self.N, self.N], dim=1)\n",
    "\n",
    "        k = F.tanh(k)\n",
    "        beta = F.softplus(beta)\n",
    "        g = F.sigmoid(g)\n",
    "        s = F.softmax(s, dim=-1)\n",
    "        gamma = 1 + F.softplus(gamma)\n",
    "        a = F.tanh(a)\n",
    "        e = F.sigmoid(e)\n",
    "\n",
    "        w = self.address(k, beta, g, s, gamma, memory, self.w_last[-1])\n",
    "        self.w_last.append(w)\n",
    "        mem = self.write(memory, w, e, a)\n",
    "        return mem, w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convolve(w, s):\n",
    "    \"\"\"Circular convolution implementation.\"\"\"\n",
    "    b, d = s.shape\n",
    "    assert b == 1, 'does _convolve work for b != 1?'\n",
    "    assert d == 3\n",
    "    w = torch.squeeze(w)\n",
    "    t = torch.cat([w[-1:], w, w[:1]])\n",
    "    c = F.conv1d(t.view(1, 1, -1), s.view(1, 1, -1)).view(b, -1)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTM(nn.Module):\n",
    "    def __init__(self, M, N, num_inputs, num_outputs, controller_out_dim, controller_hid_dim):\n",
    "        super(NTM, self).__init__()\n",
    "\n",
    "        print(\"----------- Build Neural Turing machine -----------\")\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "        self.M = M\n",
    "        self.N = N\n",
    "\n",
    "        self.memory = torch.zeros(self.M, self.N)\n",
    "        self.last_read = torch.zeros(1, self.N)\n",
    "\n",
    "        self.controller = Controller(self.num_inputs + self.N, controller_out_dim, controller_hid_dim)\n",
    "        self.read_head = ReadHead(self.M, self.N, controller_out_dim)\n",
    "        self.write_head = WriteHead(self.M, self.N, controller_out_dim)\n",
    "\n",
    "        self.fc_out = nn.Linear(self.num_inputs + N, self.num_outputs)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, X=None):\n",
    "\n",
    "        if X is None:\n",
    "            X = torch.zeros(1, self.num_inputs)\n",
    "\n",
    "        controller_out = self.controller(X, self.last_read)\n",
    "        self._read_write(controller_out)\n",
    "\n",
    "        out = torch.cat((X, self.last_read), -1)\n",
    "        out = F.sigmoid(self.fc_out(out))\n",
    "\n",
    "        return out\n",
    "\n",
    "    def _read_write(self, controller_out):\n",
    "        # READ\n",
    "        read, w = self.read_head(controller_out, self.memory)\n",
    "        self.last_read = read\n",
    "\n",
    "        # WRITE\n",
    "        mem, w = self.write_head(controller_out, self.memory)\n",
    "        self.memory = mem\n",
    "\n",
    "    def initalize_state(self):\n",
    "        stdev = 1 / (np.sqrt(self.N + self.M))\n",
    "        self.memory = nn.init.uniform_((torch.Tensor(self.M, self.N)), -stdev, stdev)\n",
    "        self.last_read = F.tanh(torch.randn(1, self.N))\n",
    "\n",
    "        self.read_head.reset_memory()\n",
    "        self.write_head.reset_memory()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Initialize the linear layers\n",
    "        nn.init.xavier_uniform_(self.fc_out.weight, gain=1.4)\n",
    "        nn.init.normal_(self.fc_out.bias, std=0.5)\n",
    "\n",
    "    def get_memory_info(self):\n",
    "        return self.memory, self.read_head.get_weights(), self.write_head.get_weights()\n",
    "\n",
    "    def calculate_num_params(self):\n",
    "        \"\"\"Returns the total number of parameters.\"\"\"\n",
    "        num_params = 0\n",
    "        for p in self.parameters():\n",
    "            num_params += p.data.view(-1).size(0)\n",
    "        return num_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaySeqDataset(Dataset):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        self.seq_len = args['sequence_length']\n",
    "        self.seq_width = args['token_size']\n",
    "        self.dataset_dim = args['training_samples']\n",
    "\n",
    "    def _generate_seq(self):\n",
    "        seq = np.random.binomial(1, 0.5, (self.seq_len, self.seq_width))\n",
    "        seq = torch.from_numpy(seq)\n",
    "        # Add start and end token\n",
    "        inp = torch.zeros(self.seq_len + 2, self.seq_width)\n",
    "        inp[1:self.seq_len + 1, :self.seq_width] = seq.clone()\n",
    "        inp[0, 0] = 1.0\n",
    "        inp[self.seq_len + 1, self.seq_width - 1] = 1.0\n",
    "        outp = seq.data.clone()\n",
    "\n",
    "        return inp.float(), outp.float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_dim\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inp, out = self._generate_seq()\n",
    "        return inp, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_grads(net):\n",
    "    parameters = list(filter(lambda p: p.grad is not None, net.parameters()))\n",
    "    for p in parameters:\n",
    "        p.grad.data.clamp_(args['min_grad'], args['max_grad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- Build Neural Turing machine -----------\n",
      "--- Initialize Controller\n",
      "--- Initialize Memory: ReadHead\n",
      "--- Initialize Memory: WriteHead\n",
      "NTM(\n",
      "  (controller): Controller(\n",
      "    (fc1): Linear(in_features=138, out_features=512, bias=True)\n",
      "    (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  )\n",
      "  (read_head): ReadHead(\n",
      "    (fc_read): Linear(in_features=256, out_features=134, bias=True)\n",
      "  )\n",
      "  (write_head): WriteHead(\n",
      "    (fc_write): Linear(in_features=256, out_features=390, bias=True)\n",
      "  )\n",
      "  (fc_out): Linear(in_features=138, out_features=10, bias=True)\n",
      ")\n",
      "--------- Number of parameters -----------\n",
      "338554\n",
      "--------- Start training -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sjadon/anaconda3/envs/mysite/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  if sys.path[0] == '':\n",
      "/Users/sjadon/anaconda3/envs/mysite/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  del sys.path[0]\n",
      "/Users/sjadon/anaconda3/envs/mysite/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  from ipykernel import kernelapp as app\n",
      "/Users/sjadon/anaconda3/envs/mysite/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  app.launch_new_instance()\n",
      "/Users/sjadon/anaconda3/envs/mysite/lib/python3.6/site-packages/ipykernel_launcher.py:53: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "/Users/sjadon/anaconda3/envs/mysite/lib/python3.6/site-packages/ipykernel_launcher.py:54: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  1.266266107559204\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-dc3253b7cf0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_seq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mysite/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-9823d45587d1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mcontroller_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontroller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_read\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontroller_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mysite/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-28e9ed8d3109>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, last_read)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_read\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mysite/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mysite/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mysite/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1406\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1407\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    args = {'sequence_length':300,'token_size':10,'memory_capacity':64,'memory_vector_size':128,'training_samples':99,\n",
    "            'controller_output_dim':256,'controller_hidden_dim':512,'learning_rate':1e-4,'min_grad':-10,'max_grad':10,\n",
    "           'logdir':'./','loadmodel':'','savemodel':'checkpoint.model'}\n",
    "    writer = SummaryWriter()\n",
    "    dataset = BinaySeqDataset(args)\n",
    "    dataloader = DataLoader(dataset, batch_size=1,\n",
    "                            shuffle=True, num_workers=4)\n",
    "\n",
    "    model = NTM(M=args['memory_capacity'],\n",
    "                N=args['memory_vector_size'],\n",
    "                num_inputs=args['token_size'],\n",
    "                num_outputs=args['token_size'],\n",
    "                controller_out_dim=args['controller_output_dim'],\n",
    "                controller_hid_dim=args['controller_hidden_dim'],\n",
    "                )\n",
    "\n",
    "    print(model)\n",
    "\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=args['learning_rate'])\n",
    "\n",
    "    print(\"--------- Number of parameters -----------\")\n",
    "    print(model.calculate_num_params())\n",
    "    print(\"--------- Start training -----------\")\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    if args['loadmodel'] != '':\n",
    "        model.load_state_dict(torch.load(args['loadmodel']))\n",
    "\n",
    "    for e, (X, Y) in enumerate(dataloader):\n",
    "        tmp = time()\n",
    "        model.initalize_state()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inp_seq_len = args['sequence_length'] + 2\n",
    "        out_seq_len = args['sequence_length']\n",
    "\n",
    "        X.requires_grad = True\n",
    "\n",
    "        # Input rete: sequenza\n",
    "        for t in range(0, inp_seq_len):\n",
    "            model(X[:, t])\n",
    "\n",
    "        # Input rete: null\n",
    "        y_pred = torch.zeros(Y.size())\n",
    "        for i in range(0, out_seq_len):\n",
    "            y_pred[:, i] = model()\n",
    "\n",
    "        loss = criterion(y_pred, Y)\n",
    "        loss.backward()\n",
    "        clip_grads(model)\n",
    "        optimizer.step()\n",
    "        losses += [loss.item()]\n",
    "\n",
    "        if e % 50 == 0:\n",
    "            mean_loss = np.array(losses[-50:]).mean()\n",
    "            print(\"Loss: \", loss.item())\n",
    "#             writer.add_scalar('Mean loss', loss.item(), e)\n",
    "            if e % 1000 == 0:\n",
    "#                 for name, param in model.named_parameters():\n",
    "#                     writer.add_histogram(name, param.clone().cpu().data.numpy(), e)\n",
    "                mem_pic, read_pic, write_pic = model.get_memory_info()\n",
    "#                 pic1 = vutils.make_grid(y_pred, normalize=True, scale_each=True)\n",
    "#                 pic2 = vutils.make_grid(Y, normalize=True, scale_each=True)\n",
    "#                 pic3 = vutils.make_grid(mem_pic, normalize=True, scale_each=True)\n",
    "#                 pic4 = vutils.make_grid(read_pic, normalize=True, scale_each=True)\n",
    "#                 pic5 = vutils.make_grid(write_pic, normalize=True, scale_each=True)\n",
    "#                 writer.add_image('NTM output', pic1, e)\n",
    "#                 writer.add_image('True output', pic2, e)\n",
    "#                 writer.add_image('Memory', pic3, e)\n",
    "#                 writer.add_image('Read weights', pic4, e)\n",
    "#                 writer.add_image('Write weights', pic5, e)\n",
    "#                 torch.save(model.state_dict(), args.savemodel)\n",
    "            losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
