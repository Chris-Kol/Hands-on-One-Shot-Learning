{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching Networks Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "np.random.seed(2191)  # for reproducibility\n",
    "\n",
    "class OmniglotNShotDataset():\n",
    "    def __init__(self,batch_size,classes_per_set=5,samples_per_class=1,trainsize=1000,valsize=200):\n",
    "\n",
    "        \"\"\"\n",
    "        Constructs an N-Shot omniglot Dataset\n",
    "        :param batch_size: Experiment batch_size\n",
    "        :param classes_per_set: Integer indicating the number of classes per set\n",
    "        :param samples_per_class: Integer indicating samples per class\n",
    "        e.g. For a 20-way, 1-shot learning task, use classes_per_set=20 and samples_per_class=1\n",
    "             For a 5-way, 10-shot learning task, use classes_per_set=5 and samples_per_class=10\n",
    "        \"\"\"\n",
    "        self.x = np.load(\"./data/data.npy\")\n",
    "        self.x = np.reshape(self.x, [-1, 20, 28, 28, 1])\n",
    "        shuffle_classes = np.arange(self.x.shape[0])\n",
    "        np.random.shuffle(shuffle_classes)\n",
    "        self.x = self.x[shuffle_classes]\n",
    "        self.x_train, self.x_val  = self.x[:1200], self.x[1200:]\n",
    "        self.normalization()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.n_classes = self.x.shape[0]\n",
    "        self.classes_per_set = classes_per_set\n",
    "        self.samples_per_class = samples_per_class\n",
    "\n",
    "        self.indexes = {\"train\": 0, \"val\": 0}\n",
    "        self.datasets = {\"train\": self.x_train, \"val\": self.x_val}\n",
    "        self.datasets_cache = {\"train\": self.packslice(self.datasets[\"train\"],trainsize),\n",
    "                               \"val\": self.packslice(self.datasets[\"val\"],valsize)}\n",
    "\n",
    "    def normalization(self):\n",
    "        \"\"\"\n",
    "        Normalizes our data, to have a mean of 0 and sd of 1\n",
    "        \"\"\"\n",
    "        self.mean = np.mean(self.x_train)\n",
    "        self.std = np.std(self.x_train)\n",
    "        self.max = np.max(self.x_train)\n",
    "        self.min = np.min(self.x_train)\n",
    "        print(\"train_shape\", self.x_train.shape, \"val_shape\", self.x_val.shape)\n",
    "        print(\"before_normalization\", \"mean\", self.mean, \"max\", self.max, \"min\", self.min, \"std\", self.std)\n",
    "        self.x_train = (self.x_train - self.mean) / self.std\n",
    "        self.x_val = (self.x_val - self.mean) / self.std\n",
    "        self.mean = np.mean(self.x_train)\n",
    "        self.std = np.std(self.x_train)\n",
    "        self.max = np.max(self.x_train)\n",
    "        self.min = np.min(self.x_train)\n",
    "        print(\"after_normalization\", \"mean\", self.mean, \"max\", self.max, \"min\", self.min, \"std\", self.std)\n",
    "        \n",
    "    def packslice(self, data_pack, numsamples):\n",
    "        \"\"\"\n",
    "        Collects 1000 batches data for N-shot learning\n",
    "        :param data_pack: Data pack to use (any one of train, val, test)\n",
    "        :return: A list with [support_set_x, support_set_y, target_x, target_y] ready to be fed to our networks\n",
    "        \"\"\"\n",
    "        n_samples = self.samples_per_class * self.classes_per_set\n",
    "        support_cacheX = []\n",
    "        support_cacheY = []\n",
    "        target_cacheY = []\n",
    "        \n",
    "        for iiii in range(numsamples):\n",
    "            slice_x = np.zeros((n_samples+1,28,28,1))\n",
    "            slice_y = np.zeros((n_samples,))\n",
    "            \n",
    "            ind = 0\n",
    "            pinds = np.random.permutation(n_samples)\n",
    "            classes = np.random.choice(data_pack.shape[0],self.classes_per_set,False) # chosen classes\n",
    "            \n",
    "            x_hat_class = np.random.randint(self.classes_per_set) # target class\n",
    "            \n",
    "            for j, cur_class in enumerate(classes):  # each class\n",
    "                example_inds = np.random.choice(data_pack.shape[1],self.samples_per_class,False)\n",
    "                \n",
    "                for eind in example_inds:\n",
    "                    slice_x[pinds[ind],:,:,:] = data_pack[cur_class][eind]\n",
    "                    slice_y[pinds[ind]] = j\n",
    "                    ind += 1\n",
    "                \n",
    "                if j == x_hat_class:\n",
    "                    slice_x[n_samples,:,:,:] = data_pack[cur_class][np.random.choice(data_pack.shape[1])]\n",
    "                    target_y = j\n",
    "\n",
    "            support_cacheX.append(slice_x)\n",
    "            support_cacheY.append(keras.utils.to_categorical(slice_y,self.classes_per_set))\n",
    "            target_cacheY.append(keras.utils.to_categorical(target_y,self.classes_per_set))\n",
    "            \n",
    "        return np.array(support_cacheX), np.array(support_cacheY), np.array(target_cacheY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers.merge import _Merge\n",
    "\n",
    "class MatchCosine(_Merge):\n",
    "    \"\"\"\n",
    "        Matching network with cosine similarity metric\n",
    "    \"\"\"\n",
    "    def __init__(self,nway=5,**kwargs):\n",
    "        super(MatchCosine,self).__init__(**kwargs)\n",
    "        self.eps = 1e-10\n",
    "        self.nway = nway\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if not isinstance(input_shape, list) or len(input_shape) != self.nway+2:\n",
    "            raise ValueError('A ModelCosine layer should be called on a list of inputs of length %d'%(self.nway+2))\n",
    "\n",
    "    def call(self,inputs):\n",
    "        \"\"\"\n",
    "        inputs in as array which contains the support set the embeddings, \n",
    "        the target embedding as the second last value in the array, and true class of target embedding as the last value in the array\n",
    "        \"\"\" \n",
    "        similarities = []\n",
    "\n",
    "        targetembedding = inputs[-2] # embedding of the query image\n",
    "        numsupportset = len(inputs)-2\n",
    "        for ii in range(numsupportset):\n",
    "            supportembedding = inputs[ii] # embedding for i^{th} member in the support set\n",
    "\n",
    "            sum_support = tf.reduce_sum(tf.square(supportembedding), 1, keepdims=True)\n",
    "            supportmagnitude = tf.rsqrt(tf.clip_by_value(sum_support, self.eps, float(\"inf\"))) #reciprocal of the magnitude of the member of the support \n",
    "\n",
    "            sum_query = tf.reduce_sum(tf.square(targetembedding), 1, keepdims=True)\n",
    "            querymagnitude = tf.rsqrt(tf.clip_by_value(sum_query, self.eps, float(\"inf\"))) #reciprocal of the magnitude of the query image\n",
    "\n",
    "            dot_product = tf.matmul(tf.expand_dims(targetembedding,1),tf.expand_dims(supportembedding,2))\n",
    "            dot_product = tf.squeeze(dot_product,[1])\n",
    "\n",
    "            cosine_similarity = dot_product*supportmagnitude*querymagnitude\n",
    "            similarities.append(cosine_similarity)\n",
    "\n",
    "        similarities = tf.concat(axis=1,values=similarities)\n",
    "        softmax_similarities = tf.nn.softmax(similarities)\n",
    "        preds = tf.squeeze(tf.matmul(tf.expand_dims(softmax_similarities,1),inputs[-1]))\n",
    "        \n",
    "        preds.set_shape((inputs[0].shape[0],self.nway))\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        input_shapes = input_shape\n",
    "        return (input_shapes[0][0],self.nway)\n",
    "\n",
    "# Bonus: Matching network with Euclidean metrtic\n",
    "class MatchEuclidean(_Merge):\n",
    "    \"\"\"\n",
    "        Matching network with Euclidean metric\n",
    "    \"\"\"\n",
    "    def __init__(self,nway=5,**kwargs):\n",
    "        super(MatchEuclidean,self).__init__(**kwargs)\n",
    "        self.eps = 1e-10\n",
    "        self.nway = nway\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if not isinstance(input_shape, list) or len(input_shape) != self.nway+2:\n",
    "            raise ValueError('A ModelEuclidean layer should be called on a list of inputs of length %d'%(self.nway+2))\n",
    "\n",
    "    def call(self,inputs):\n",
    "        \"\"\"\n",
    "        inputs in as array which contains the support set the embeddings, the target embedding as the second last value in the array, and true class of target embedding as the last value in the array\n",
    "        \"\"\" \n",
    "        similarities = []\n",
    "\n",
    "        targetembedding = inputs[-2]\n",
    "        numsupportset = len(inputs)-2\n",
    "        for ii in range(numsupportset):\n",
    "            supportembedding = inputs[ii]\n",
    "            dd = tf.negative(tf.sqrt(tf.reduce_sum(tf.square(supportembedding-targetembedding),1,keepdims=True)))\n",
    "\n",
    "            similarities.append(dd)\n",
    "\n",
    "        similarities = tf.concat(axis=1,values=similarities)\n",
    "        softmax_similarities = tf.nn.softmax(similarities)\n",
    "        preds = tf.squeeze(tf.matmul(tf.expand_dims(softmax_similarities,1),inputs[-1]))\n",
    "        \n",
    "        preds.set_shape((inputs[0].shape[0],self.nway))\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        input_shapes = input_shape\n",
    "        return (input_shapes[0][0],self.nway)\n",
    "\n",
    "# Siamese network like interaction\n",
    "class Siamify(_Merge):\n",
    "    def _merge_function(self,inputs):\n",
    "        return tf.negative(tf.abs(inputs[0]-inputs[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_shape (1200, 20, 28, 28, 1) val_shape (423, 20, 28, 28, 1)\n",
      "before_normalization mean 234.2152224702381 max 255 min 0 std 56.17165918668325\n",
      "after_normalization mean -1.6715880380560316e-16 max 0.37002249587616587 min -4.169633332208283 std 1.0000000000000009\n",
      "WARNING:tensorflow:From /Users/sjadon/anaconda3/envs/project09/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 64000 samples, validate on 20000 samples\n",
      "Epoch 1/50\n",
      "16224/64000 [======>.......................] - ETA: 8:23 - loss: 1.4968 - acc: 0.4159"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Flatten, Input, Lambda\n",
    "from keras.layers.merge import Maximum\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "\n",
    "bsize = 32 # batch size\n",
    "classes_per_set = 5 # classes per set or 5-way\n",
    "samples_per_class = 1 # samples per class 1-short\n",
    "\n",
    "data = OmniglotNShotDataset(batch_size=bsize,classes_per_set=classes_per_set,samples_per_class=samples_per_class,trainsize=64000,valsize=20000)\n",
    "\n",
    "# Image embedding using Deep Convolutional Network\n",
    "conv1 = Conv2D(64,(3,3),padding='same',activation='relu')\n",
    "bnorm1 = BatchNormalization()\n",
    "mpool1 = MaxPooling2D((2,2),padding='same')\n",
    "conv2 = Conv2D(64,(3,3),padding='same',activation='relu')\n",
    "bnorm2 = BatchNormalization()\n",
    "mpool2 = MaxPooling2D((2,2),padding='same')\n",
    "conv3 = Conv2D(64,(3,3),padding='same',activation='relu')\n",
    "bnorm3 = BatchNormalization()\n",
    "mpool3 = MaxPooling2D((2,2),padding='same')\n",
    "conv4 = Conv2D(64,(3,3),padding='same',activation='relu')\n",
    "bnorm4 = BatchNormalization()\n",
    "mpool4 = MaxPooling2D((2,2),padding='same')\n",
    "fltn = Flatten()\n",
    "\n",
    "# Function that generarates Deep CNN embedding given the input image x\n",
    "def convembedding(x):\n",
    "    x = conv1(x)\n",
    "    x = bnorm1(x)\n",
    "    x = mpool1(x)\n",
    "    x = conv2(x)\n",
    "    x = bnorm2(x)\n",
    "    x = mpool2(x)\n",
    "    x = conv3(x)\n",
    "    x = bnorm3(x)\n",
    "    x = mpool3(x)\n",
    "    x = conv4(x)\n",
    "    x = bnorm4(x)\n",
    "    x = mpool4(x)\n",
    "    x = fltn(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Relational embedding comprising a 4 layer MLP\n",
    "d1 = Dense(64,activation='relu')\n",
    "dbnrm1 = BatchNormalization()\n",
    "d2 = Dense(64,activation='relu')\n",
    "dbnrm2 = BatchNormalization()\n",
    "d3 = Dense(64,activation='relu')\n",
    "dbnrm3 = BatchNormalization()\n",
    "d4 = Dense(64,activation='relu')\n",
    "dbnrm4 = BatchNormalization()\n",
    "\n",
    "def relationalembedding(x):\n",
    "    x = d1(x)\n",
    "    x = dbnrm1(x)\n",
    "    x = d2(x)\n",
    "    x = dbnrm2(x)\n",
    "    x = d3(x)\n",
    "    x = dbnrm3(x)\n",
    "    x = d4(x)\n",
    "    x = dbnrm4(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "numsupportset = samples_per_class*classes_per_set\n",
    "input1 = Input((numsupportset+1,28,28,1))\n",
    "\n",
    "# CNN embedding support set and query image\n",
    "convolutionlayers = []\n",
    "for lidx in range(numsupportset):\n",
    "    convolutionlayers.append(convembedding(Lambda(lambda x: x[:,lidx,:,:,:])(input1)))\n",
    "targetembedding = convembedding(Lambda(lambda x: x[:,-1,:,:,:])(input1))\n",
    "\n",
    "# Siamese like pairwise interactions\n",
    "siam = Siamify()\n",
    "pairwiseinteractions = defaultdict(list)\n",
    "\n",
    "# Get all pairwise Siamese interactions in the support set and generate of list of interactions\n",
    "# for each member of support set \n",
    "for tt in combinations(range(numsupportset),2):\n",
    "    aa = siam([convolutionlayers[tt[0]],convolutionlayers[tt[1]]])\n",
    "    pairwiseinteractions[tt[0]].append(aa)\n",
    "    pairwiseinteractions[tt[1]].append(aa)\n",
    "\n",
    "# Get Siamese interactions for query image\n",
    "targetinteractions = []\n",
    "for i in range(numsupportset):\n",
    "    aa = siam([targetembedding,convolutionlayers[i]])\n",
    "    targetinteractions.append(aa)  \n",
    "    pairwiseinteractions[i].append(aa) # add this interaction to the set of interaction for this member\n",
    "\n",
    "# Take 4 layer MLP transform on Max pooling of interactions to serve as Full Context Embedding (FCE)\n",
    "maxi = Maximum()\n",
    "modelinputs = []\n",
    "for i in range(numsupportset):\n",
    "    modelinputs.append(relationalembedding(maxi(pairwiseinteractions[i])))\n",
    "modelinputs.append(relationalembedding(maxi(targetinteractions)))\n",
    "\n",
    "supportlabels = Input((numsupportset,classes_per_set))\n",
    "modelinputs.append(supportlabels)\n",
    "\n",
    "knnsimilarity = MatchEuclidean(nway=classes_per_set)(modelinputs)\n",
    "\n",
    "model = Model(inputs=[input1,supportlabels],outputs=knnsimilarity)\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.fit([data.datasets_cache[\"train\"][0],data.datasets_cache[\"train\"][1]],data.datasets_cache[\"train\"][2],\n",
    "          validation_data=[[data.datasets_cache[\"val\"][0],data.datasets_cache[\"val\"][1]],data.datasets_cache[\"val\"][2]],\n",
    "          epochs=50,batch_size=32,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
