{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese Network Architecture using MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- encoding: utf-8 -*-\n",
    "import argparse\n",
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "For Siamese Network, we want data in pairs: 1 pair-> similar, 1 pair-> dissimilar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, x0, x1, label):\n",
    "        self.size = label.shape[0]\n",
    "        self.x0 = torch.from_numpy(x0)\n",
    "        self.x1 = torch.from_numpy(x1)\n",
    "        self.label = torch.from_numpy(label)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.x0[index],\n",
    "                self.x1[index],\n",
    "                self.label[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairs(data, digit_indices):\n",
    "    x0_data = []\n",
    "    x1_data = []\n",
    "    label = []\n",
    "    n = min([len(digit_indices[d]) for d in range(10)]) - 1\n",
    "    for d in range(10):\n",
    "        for i in range(n):\n",
    "            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]\n",
    "            x0_data.append(data[z1] / 255.)\n",
    "            x1_data.append(data[z2] / 255.)\n",
    "            label.append(1)\n",
    "            inc = random.randrange(1, 10)\n",
    "            dn = (d + inc) % 10\n",
    "            z1, z2 = digit_indices[d][i], digit_indices[dn][i]\n",
    "            x0_data.append(data[z1] / 255.)\n",
    "            x1_data.append(data[z2] / 255.)\n",
    "            label.append(0)\n",
    "\n",
    "    x0_data = np.array(x0_data, dtype=np.float32)\n",
    "    x0_data = x0_data.reshape([-1, 1, 28, 28])\n",
    "    x1_data = np.array(x1_data, dtype=np.float32)\n",
    "    x1_data = x1_data.reshape([-1, 1, 28, 28])\n",
    "    label = np.array(label, dtype=np.int32)\n",
    "    return x0_data, x1_data, label\n",
    "\n",
    "# create iterator\n",
    "def create_iterator(data, label, batchsize, shuffle=False):\n",
    "    digit_indices = [np.where(label == i)[0] for i in range(10)]\n",
    "    x0, x1, label = create_pairs(data, digit_indices)\n",
    "    ret = Dataset(x0, x1, label)\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function: Contrastive Loss Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss_function(x0, x1, y, margin=1.0):\n",
    "    # euclidian distance\n",
    "    diff = x0 - x1\n",
    "    dist_sq = torch.sum(torch.pow(diff, 2), 1)\n",
    "    dist = torch.sqrt(dist_sq)\n",
    "\n",
    "    mdist = margin - dist\n",
    "    dist = torch.clamp(mdist, min=0.0)\n",
    "    loss = y * dist_sq + (1 - y) * torch.pow(dist, 2)\n",
    "    loss = torch.sum(loss) / 2.0 / x0.size()[0]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Siamese Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self,flag_kaf=False):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 20, kernel_size=5),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(20, 50, kernel_size=5),\n",
    "            nn.MaxPool2d(2, stride=2))\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(50 * 4 * 4, 500),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(500,10),\n",
    "            nn.Linear(10, 2))\n",
    "        \n",
    "    def forward_once(self, x):\n",
    "        output = self.cnn1(x)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        output = self.fc1(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        return output1, output2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Function of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mnist(numpy_all, numpy_labels,name=\"./embeddings_plot.png\"):\n",
    "        c = ['#ff0000', '#ffff00', '#00ff00', '#00ffff', '#0000ff',\n",
    "             '#ff00ff', '#990000', '#999900', '#009900', '#009999']\n",
    "\n",
    "        for i in range(10):\n",
    "            f = numpy_all[np.where(numpy_labels == i)]\n",
    "            plt.plot(f[:, 0], f[:, 1], '.', c=c[i])\n",
    "        plt.legend(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'])\n",
    "        plt.savefig(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduce MNIST dataset, choose random 2000 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sjadon/anaconda3/envs/mysite/lib/python3.6/site-packages/torchvision/datasets/mnist.py:43: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n",
      "/Users/sjadon/anaconda3/envs/mysite/lib/python3.6/site-packages/torchvision/datasets/mnist.py:48: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "batchsize=128\n",
    "train = dsets.MNIST(root='../data/',train=True,download=True)\n",
    "test = dsets.MNIST(root='../data/',train=False,transform=transforms.Compose([transforms.ToTensor(),]))\n",
    "indices= np.random.choice(len(train.train_labels.numpy()), 2000, replace=False)\n",
    "indices_test= np.random.choice(len(test.test_labels.numpy()), 50, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sjadon/anaconda3/envs/mysite/lib/python3.6/site-packages/torchvision/datasets/mnist.py:53: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n",
      "/Users/sjadon/anaconda3/envs/mysite/lib/python3.6/site-packages/torchvision/datasets/mnist.py:58: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n"
     ]
    }
   ],
   "source": [
    "train_iter = create_iterator(train.train_data.numpy()[indices],train.train_labels.numpy()[indices],batchsize)\n",
    "test_iter = create_iterator(test.test_data.numpy()[indices_test],test.test_labels.numpy()[indices_test],batchsize)\n",
    "# model\n",
    "model = SiameseNetwork()\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "# Loss and Optimizer\n",
    "criterion = contrastive_loss_function\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate,momentum=momentum)\n",
    "kwargs = {}\n",
    "train_loader = torch.utils.data.DataLoader(train_iter,batch_size=batchsize, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test,batch_size=batchsize, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:0------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.234510\n",
      "Train Epoch:1------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.143157\n",
      "Train Epoch:2------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.098708\n",
      "Train Epoch:3------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.103307\n",
      "Train Epoch:4------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.096415\n",
      "Train Epoch:5------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.092508\n",
      "Train Epoch:6------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.087364\n",
      "Train Epoch:7------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.071047\n",
      "Train Epoch:8------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.083233\n",
      "Train Epoch:9------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.094526\n",
      "Train Epoch:10------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.067362\n",
      "Train Epoch:11------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.067557\n",
      "Train Epoch:12------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.071135\n",
      "Train Epoch:13------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.065369\n",
      "Train Epoch:14------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.057168\n",
      "Train Epoch:15------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.055312\n",
      "Train Epoch:16------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.059490\n",
      "Train Epoch:17------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.049783\n",
      "Train Epoch:18------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.042970\n",
      "Train Epoch:19------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.047377\n",
      "Train Epoch:20------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.050676\n",
      "Train Epoch:21------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.039866\n",
      "Train Epoch:22------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.049852\n",
      "Train Epoch:23------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.034178\n",
      "Train Epoch:24------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.037324\n",
      "Train Epoch:25------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.038315\n",
      "Train Epoch:26------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.031461\n",
      "Train Epoch:27------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.044078\n",
      "Train Epoch:28------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.032450\n",
      "Train Epoch:29------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.025457\n",
      "Train Epoch:30------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.025986\n",
      "Train Epoch:31------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.024708\n",
      "Train Epoch:32------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.027973\n",
      "Train Epoch:33------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.025025\n",
      "Train Epoch:34------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.025972\n",
      "Train Epoch:35------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.022447\n",
      "Train Epoch:36------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.024347\n",
      "Train Epoch:37------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.019646\n",
      "Train Epoch:38------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.020046\n",
      "Train Epoch:39------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.015663\n",
      "Train Epoch:40------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.018249\n",
      "Train Epoch:41------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.019848\n",
      "Train Epoch:42------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.014284\n",
      "Train Epoch:43------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.016123\n",
      "Train Epoch:44------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.016899\n",
      "Train Epoch:45------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.016327\n",
      "Train Epoch:46------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.010723\n",
      "Train Epoch:47------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.016575\n",
      "Train Epoch:48------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.016700\n",
      "Train Epoch:49------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.016617\n",
      "Train Epoch:50------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.011231\n",
      "Train Epoch:51------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.016193\n",
      "Train Epoch:52------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.008748\n",
      "Train Epoch:53------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.009042\n",
      "Train Epoch:54------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.011640\n",
      "Train Epoch:55------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.011959\n",
      "Train Epoch:56------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.013640\n",
      "Train Epoch:57------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.011012\n",
      "Train Epoch:58------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.009920\n",
      "Train Epoch:59------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.007684\n",
      "Train Epoch:60------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.007029\n",
      "Train Epoch:61------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.009635\n",
      "Train Epoch:62------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.010923\n",
      "Train Epoch:63------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.008234\n",
      "Train Epoch:64------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.008179\n",
      "Train Epoch:65------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.004798\n",
      "Train Epoch:66------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.008371\n",
      "Train Epoch:67------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.012184\n",
      "Train Epoch:68------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.008055\n",
      "Train Epoch:69------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.005836\n",
      "Train Epoch:70------------------>\n",
      "Batch id: 0 [0/3400 (0%)]\tLoss: 0.006911\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "running_loss=0.0\n",
    "epochs =100\n",
    "for epoch in range(epochs):\n",
    "    print('Train Epoch:'+str(epoch)+\"------------------>\")\n",
    "    for batch_idx, (x0, x1, labels) in enumerate(train_loader):\n",
    "        labels = labels.float()\n",
    "        x0, x1, labels = Variable(x0), Variable(x1), Variable(labels)\n",
    "        output1, output2 = model(x0, x1)\n",
    "        loss = criterion(output1, output2, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.item())\n",
    "        if batch_idx % batchsize == 0:\n",
    "            print('Batch id: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                batch_idx, batch_idx * len(labels), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    #torch.save(model.state_dict(), './model-epoch-%s.pth' % epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(train_loss,name=\"train_loss.png\"):\n",
    "    plt.plot(train_loss, label=\"train loss\")\n",
    "    plt.legend()\n",
    "plot_loss(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Embeddings of MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model):\n",
    "        model.eval()\n",
    "        all_ = []\n",
    "        all_labels = []\n",
    "        #original_labels=[]\n",
    "        for batch_idx, (x, labels) in enumerate(test_loader):\n",
    "            x, labels = Variable(x, volatile=True), Variable(labels)\n",
    "            output = model.forward_once(x)\n",
    "            all_.extend(output.data.cpu().numpy().tolist())\n",
    "            all_labels.extend(labels.data.cpu().numpy().tolist())\n",
    "\n",
    "        numpy_all = np.array(all_)\n",
    "        numpy_labels = np.array(all_labels)\n",
    "        return numpy_all, numpy_labels\n",
    "\n",
    "def testing_plots(name_file,model):\n",
    "        filehandler = open(name_file,\"wb\")\n",
    "        dict_pickle={}\n",
    "        numpy_all, numpy_labels = test_model(model)\n",
    "        dict_pickle[\"numpy_all\"]=numpy_all\n",
    "        dict_pickle[\"numpy_labels\"]=numpy_labels\n",
    "        pickle.dump(dict_pickle,filehandler)\n",
    "        plot_mnist(numpy_all, numpy_labels)\n",
    "        filehandler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_plots(\"embeddings.pickle\",model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
