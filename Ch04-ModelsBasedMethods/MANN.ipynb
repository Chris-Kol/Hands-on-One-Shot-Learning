{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Augmented Neural Network using Omniglot Dataset\n",
    "\n",
    "As we showcased that a Neural Turing Machine's controller is capable of using content-based addressing, location-based addressing or both. Whereas, here MANN works on using a pure content-based memory writer. \n",
    "\n",
    "MANN also use a new addressing schema called least recently used access. The idea behind the scene is that the least recently used memory location is determined by the read operation and the read operation is performed by content-based addressing. So, we basically perform content-based addressing for reading and write to the location that was least recently used.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[picture credits: MANN Paper(https://arxiv.org/pdf/1605.06065.pdf)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"mann.png\" width=\"1500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will do following things step by step:\n",
    "1. Data Preprocessing: Creating Pairs.\n",
    "2. Create a Memory Augmented Neural Network\n",
    "3. Train it using Omniglot dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy==1.1.0 in /Users/sjadon/.local/lib/python3.6/site-packages\n",
      "Requirement already satisfied: numpy>=1.8.2 in /opt/anaconda3/envs/project09/lib/python3.6/site-packages (from scipy==1.1.0)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/envs/project09/lib/python3.6/site-packages\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scipy==1.1.0\n",
    "!pip install Pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step1: Lets first import all libraries needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import utils as ut\n",
    "import os\n",
    "import time\n",
    "from scipy.misc import imresize\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step2 : Load Data\n",
    "\n",
    "We are reading images from two folders named 'image_background', 'image_evaluation' defined in 'data' directory\n",
    "\n",
    "Dataset is divided into 1423 charcters images for training and rest for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1623 total character classes\n",
      "1423 characters assigned for training, 200 characters assigned for validation\n"
     ]
    }
   ],
   "source": [
    "width = 20\n",
    "\n",
    "# gather data paths\n",
    "subfolds = ut.extend_children('data','')\n",
    "datafolds = [subfolds[0],subfolds[1]]\n",
    "alphabets = ut.extend_generation(datafolds,'')\n",
    "charpaths = ut.extend_generation(alphabets,'')\n",
    "chars_dataset = [v.split('/')[2]+'/'+v.split('/')[3] for v in charpaths]\n",
    "\n",
    "# index-value conversion dictionaries for character set\n",
    "i2v = {i:v for i, v in enumerate(chars_dataset)}\n",
    "v2i = {v:i for i, v in enumerate(chars_dataset)}\n",
    "\n",
    "# get size of dataset\n",
    "mc_dataset = len(charpaths)\n",
    "print(mc_dataset,'total character classes')\n",
    "\n",
    "# train split\n",
    "mc_train = 1423\n",
    "chars_train = chars_dataset[:mc_train]\n",
    "classes_train = [v2i[v] for v in chars_train]\n",
    "\n",
    "# validation split\n",
    "mc_val = mc_dataset-mc_train\n",
    "chars_val = chars_dataset[-mc_val:]\n",
    "classes_val = [v2i[v] for v in chars_val]\n",
    "\n",
    "\n",
    "\n",
    "print('%s characters assigned for training, %s characters assigned for validation'%(mc_train,mc_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Read Data\n",
    "\n",
    "All character images are read and saved in the imgs_dataset variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1623 character folders loaded\n",
      "200/1623 character folders loaded\n",
      "400/1623 character folders loaded\n",
      "600/1623 character folders loaded\n",
      "800/1623 character folders loaded\n",
      "1000/1623 character folders loaded\n",
      "1200/1623 character folders loaded\n",
      "1400/1623 character folders loaded\n",
      "1600/1623 character folders loaded\n"
     ]
    }
   ],
   "source": [
    "# load images from character paths\n",
    "imgs_dataset = []\n",
    "for i, charfold in enumerate(charpaths):\n",
    "    if i%200==0:\n",
    "        print('%s/%s character folders loaded'%(i,mc_dataset))\n",
    "    imgs_dataset.append([ ut.load_image(imgpath,(width,width))/127.5-1 for imgpath in ut.extend_children(charfold,'.png') ] )\n",
    "# access imgs_dataset [ character index ] [ sample index ] [ row,col ]\n",
    "\n",
    "# split images between train and validation sets\n",
    "imgs_train = imgs_dataset[:mc_train]\n",
    "imgs_val = imgs_dataset[-mc_val:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Let's Initialize our Hyper-paramters\n",
    "\n",
    "We define all the hyper-parameters and dimensions of all the variables here.\n",
    "* n_classes is 5 (in each batch we have 5 types of characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_classes = 5\n",
    "memory_size = 128 # number of features per entry\n",
    "memory_dim = 40 # number of entries in memory\n",
    "learning_rate = 1e-3\n",
    "batch_size = 16\n",
    "\n",
    "n_inputs = width*width+n_classes # input to LSTM cell.\n",
    "n_hnodes = 200 # LSTM cell size\n",
    "n_outputs = n_classes\n",
    "mem_size = memory_size # number of rows in external memory \n",
    "mem_dim = memory_dim # number of columns in external memory\n",
    "n_reads = 4 # number of heads. \n",
    "\n",
    "n_xh = n_inputs+n_hnodes # inputs to LSTM cell (previous state + input image dim)\n",
    "n_rd = n_reads*mem_dim\n",
    "n_hr = n_hnodes+n_rd\n",
    "gamma = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Let's Create all Trainable parameters for network and initialize them.\n",
    "\n",
    "We define all the hyper-parameters and dimensions of all the variables here.\n",
    "\n",
    "This defines three types of weights:\n",
    "* LSTM weights (we use LSTM controller, a feed-forward controller could also be used)\n",
    "* Read and Write key weights\n",
    "* Output fully connected layer weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Define Model\n",
    "\n",
    "The next cell defines the forward pass of the network. \n",
    "It works as follows:\n",
    "* the net() function receives input X which is sliced along dim 0 which is the time dimension\n",
    "* we sequentially process each time step in run_one_step() function and collect state vectors of each step's output\n",
    "* The predictions are made at each time step with fully connected output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MANN:\n",
    "    def __init__(self):\n",
    "        print (\"MANN Initlaized\")\n",
    "    \n",
    "    def weight_intialization(self):\n",
    "        # LSTM gates (4 of them: weights and biases for each)\n",
    "        W_gf = torch.Tensor(n_xh, n_hnodes).uniform_(-1., 1.).requires_grad_()\n",
    "        b_gf = torch.Tensor(n_hnodes).uniform_(-1., 1.).requires_grad_()\n",
    "        W_gi = torch.Tensor(n_xh,n_hnodes).uniform_(-1., 1.).requires_grad_()\n",
    "        b_gi = torch.Tensor(n_hnodes).uniform_(-1., 1.).requires_grad_()\n",
    "        W_go = torch.Tensor(n_xh,n_hnodes).uniform_(-1., 1.).requires_grad_()\n",
    "        b_go = torch.Tensor(n_hnodes).uniform_(-1., 1.).requires_grad_()\n",
    "        W_u = torch.Tensor(n_xh,n_hnodes).uniform_(-1., 1.).requires_grad_()\n",
    "        b_u = torch.Tensor(n_hnodes).uniform_(-1., 1.).requires_grad_()\n",
    "        # Controller Weights\n",
    "        W_kr = torch.Tensor(n_hnodes,n_rd).uniform_(-1., 1.).requires_grad_()\n",
    "        b_kr = torch.Tensor(n_rd).uniform_(-1., 1.).requires_grad_()\n",
    "        W_kw = torch.Tensor(n_hnodes,n_rd).uniform_(-1., 1.).requires_grad_()\n",
    "        b_kw = torch.Tensor(n_rd).uniform_(-1., 1.).requires_grad_()\n",
    "        W_ga = torch.Tensor(n_hnodes,n_reads).uniform_(-1., 1.).requires_grad_()\n",
    "        b_ga = torch.Tensor(n_reads).uniform_(-1., 1.).requires_grad_()\n",
    "        # logit weights\n",
    "        W_o = torch.Tensor(n_hr,n_outputs).uniform_(-1., 1.).requires_grad_()\n",
    "        b_o = torch.Tensor(n_outputs).uniform_(-1., 1.).requires_grad_()\n",
    "    \n",
    "    def get_state0(self):\n",
    "        # memory variables (not trainable.)\n",
    "        # initialize memory and LSTM states with zero. \n",
    "        return(\n",
    "            torch.FloatTensor(1e-6*np.random.rand(batch_size,mem_size,mem_dim)),\n",
    "            torch.FloatTensor(np.zeros((batch_size,n_hnodes))),\n",
    "            torch.FloatTensor(np.zeros((batch_size,n_hnodes))),\n",
    "            torch.FloatTensor(np.zeros((batch_size,mem_size))),\n",
    "            torch.FloatTensor(np.zeros((batch_size,n_reads,mem_size))),\n",
    "            torch.FloatTensor(np.zeros((batch_size,n_reads,mem_dim))),\n",
    "        )\n",
    "    \n",
    "    def run_one_step(self, X_t, state):\n",
    "        # Run one step of the episode. \n",
    "        # This consists of Reading, Writing, and Controller\n",
    "        M_tm1, h_tm1, c_tm1, wu_tm1, wr_tm1, r_tm1 = state\n",
    "        X_t_r = X_t.view(-1,n_inputs)\n",
    "        xh = torch.cat((X_t_r,h_tm1),1)\n",
    "        gf = torch.sigmoid(torch.matmul(xh,W_gf) + b_gf)\n",
    "        gi = torch.sigmoid(torch.matmul(xh,W_gi) + b_gi)\n",
    "        go = torch.sigmoid(torch.matmul(xh,W_go) + b_go)\n",
    "        u_t = torch.tanh(torch.matmul(xh,W_u) + b_u)\n",
    "        c_t = c_tm1*gf + u_t*gi\n",
    "        h_t = c_t*go\n",
    "        kr_t = torch.tanh(torch.matmul(c_t,W_kr) + b_kr).view(batch_size,n_reads,mem_dim)\n",
    "        kw_t = torch.tanh(torch.matmul(c_t,W_kw) + b_kw).view(batch_size,n_reads,mem_dim)\n",
    "        k_norm = torch.norm(kr_t, dim=2, keepdim=True)\n",
    "        m_norm = torch.norm(M_tm1, dim=2, keepdim=True)\n",
    "        inner_prod = torch.matmul(kr_t, M_tm1.permute(0,2,1))\n",
    "        norm_prod = torch.matmul(k_norm, m_norm.permute(0,2,1))\n",
    "        wr_t = F.softmax(inner_prod/norm_prod)\n",
    "        wu_1 = wu_tm1*gamma + torch.sum(wr_t, dim=1)\n",
    "        r_t = torch.matmul(wr_t,M_tm1)\n",
    "        ga = torch.unsqueeze(torch.sigmoid(torch.matmul(h_t,W_ga)+b_ga),2)\n",
    "        _, wlu_inds = torch.topk(-1*wu_1,k=n_reads)\n",
    "        wlu_t = torch.sum(F.one_hot(wlu_inds, mem_size).type(torch.FloatTensor),dim=1,keepdim=True)\n",
    "        ww_t = wr_t*ga + wlu_t*(1-ga)\n",
    "        wu_t = wu_1 + torch.sum(ww_t, dim=1)\n",
    "        M_1 = M_tm1 * (-1*wlu_t).permute(0,2,1)\n",
    "        M_t = M_1 + torch.matmul(ww_t.permute(0,2,1), kw_t)\n",
    "        st8_t = (M_t, h_t, c_t, wu_t, wr_t, r_t)\n",
    "        return st8_t\n",
    "    \n",
    "    def net(self, X=None, y=None):\n",
    "        # X is of shape (batch_size, None, width, width)\n",
    "        self.weight_intialization()\n",
    "        a = np.arange(100*16*405).reshape((100,16,405)).astype(np.float32)\n",
    "        X = torch.from_numpy(a)\n",
    "\n",
    "        state0 = self.get_state0()\n",
    "        curr_state = state0\n",
    "\n",
    "        # Collect output of state vectors in each time step.\n",
    "        M_f = []\n",
    "        h_f = []\n",
    "        c_f = []\n",
    "        wu_f = []\n",
    "        wr_f = []\n",
    "        r_f = []\n",
    "\n",
    "        for i in range(X.shape[0]):\n",
    "            curr_state = self.run_one_step(X[i], curr_state)\n",
    "            M_f.append(curr_state[0])\n",
    "            h_f.append(curr_state[1])\n",
    "            c_f.append(curr_state[2])\n",
    "            wu_f.append(curr_state[3])\n",
    "            wr_f.append(curr_state[4])\n",
    "            r_f.append(curr_state[5])\n",
    "\n",
    "        M_f = torch.stack(M_f)\n",
    "        h_f = torch.stack(h_f)\n",
    "        c_f = torch.stack(c_f)\n",
    "        wu_f = torch.stack(wu_f)\n",
    "        wr_f = torch.stack(wr_f)\n",
    "        r_f = torch.stack(r_f)\n",
    "\n",
    "        hr = torch.cat((h_f, r_f.view(-1,batch_size,n_rd)),2)\n",
    "        o_f = torch.tensordot(hr,W_o,1)+b_o\n",
    "        return (M_f, h_f, c_f, wu_f, wr_f, r_f, o_f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 7: Initialize optimizer and criteria for  training\n",
    "\n",
    "We use Adam optimizer on cross entropy loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam([W_gf, b_gf, W_gi, b_gi, W_go, b_go, W_u, b_u, W_kr,b_kr, W_kw, b_kw,\n",
    "                      W_ga, b_ga, W_o, b_o], lr=learning_rate)\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare graph input\n",
    "\n",
    "Y-labels predicted at each time step are appended to input at next time step for better signal.\n",
    "* During training, we have access to y_labels at time step. Shift the y_labels by one time step and append this to the input to prepare input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_graph_input(X_train, y_train):\n",
    "    X = np.transpose(X_train.reshape(batch_size,-1,width*width),(1,0,2))\n",
    "    y_labels = np.transpose(y_train, (1,0,2))\n",
    "    y_labels_shifted = np.concatenate((np.zeros((1,batch_size,n_classes)), y_labels[:-1,:]),0)\n",
    "    X = np.concatenate((X,y_labels_shifted),-1)\n",
    "    y = np.argmax(y_labels, -1)\n",
    "    return torch.from_numpy(X), torch.from_numpy(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 starting..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/project09/lib/python3.6/site-packages/ipykernel_launcher.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "106.89529418945312\n",
      "0.20022058823529412\n",
      "Epoch 2 starting..\n",
      "Epoch 2\n",
      "183.316676420324\n",
      "0.1975735294117647\n",
      "Epoch 3 starting..\n",
      "Epoch 3\n",
      "236.93377999698416\n",
      "0.19897058823529412\n",
      "Epoch 4 starting..\n",
      "Epoch 4\n",
      "276.43969950956455\n",
      "0.2013235294117647\n",
      "Epoch 5 starting..\n",
      "Epoch 5\n",
      "303.2496592577766\n",
      "0.20044117647058823\n",
      "Epoch 6 starting..\n",
      "Epoch 6\n",
      "319.4746466243968\n",
      "0.20268382352941178\n",
      "Epoch 7 starting..\n",
      "Epoch 7\n",
      "328.77172296187456\n",
      "0.2001470588235294\n",
      "Epoch 8 starting..\n",
      "Epoch 8\n",
      "333.403308097054\n",
      "0.19375\n",
      "Epoch 9 starting..\n",
      "Epoch 9\n",
      "335.71365515624774\n",
      "0.19466911764705883\n",
      "Epoch 10 starting..\n",
      "Epoch 10\n",
      "337.55918713878185\n",
      "0.20091911764705883\n",
      "Epoch 11 starting..\n",
      "Epoch 11\n",
      "339.31427636567287\n",
      "0.2038970588235294\n",
      "Epoch 12 starting..\n",
      "Epoch 12\n",
      "341.01413709275863\n",
      "0.20022058823529412\n",
      "Epoch 13 starting..\n",
      "Epoch 13\n",
      "342.697394174688\n",
      "0.19963235294117648\n",
      "Epoch 14 starting..\n",
      "Epoch 14\n",
      "344.3768786472433\n",
      "0.19897058823529412\n",
      "Epoch 15 starting..\n",
      "Epoch 15\n",
      "346.05062656542833\n",
      "0.1998529411764706\n",
      "Epoch 16 starting..\n",
      "Epoch 16\n",
      "347.7195925011354\n",
      "0.19463235294117648\n",
      "Epoch 17 starting..\n",
      "Epoch 17\n",
      "349.37587917552275\n",
      "0.19919117647058823\n",
      "Epoch 18 starting..\n",
      "Epoch 18\n",
      "351.0350583791733\n",
      "0.20283088235294117\n",
      "Epoch 19 starting..\n",
      "Epoch 19\n",
      "352.6982515839969\n",
      "0.19933823529411765\n",
      "Epoch 20 starting..\n",
      "Epoch 20\n",
      "354.3544201780768\n",
      "0.19974264705882352\n",
      "Epoch 21 starting..\n",
      "Epoch 21\n",
      "356.0099679442013\n",
      "0.19761029411764705\n",
      "Epoch 22 starting..\n",
      "Epoch 22\n",
      "357.66588702622585\n",
      "0.19878676470588236\n",
      "Epoch 23 starting..\n",
      "Epoch 23\n",
      "359.31928640253403\n",
      "0.20154411764705882\n",
      "Epoch 24 starting..\n",
      "Epoch 24\n",
      "360.98114842527053\n",
      "0.19856617647058825\n",
      "Epoch 25 starting..\n",
      "Epoch 25\n",
      "362.6362964966718\n",
      "0.1975735294117647\n",
      "Epoch 26 starting..\n",
      "Epoch 26\n",
      "364.28617845563326\n",
      "0.2044485294117647\n",
      "Epoch 27 starting..\n",
      "Epoch 27\n",
      "365.93866487811596\n",
      "0.19779411764705881\n",
      "Epoch 28 starting..\n",
      "Epoch 28\n",
      "367.587837008869\n",
      "0.20036764705882354\n",
      "Epoch 29 starting..\n",
      "Epoch 29\n",
      "369.2341894752839\n",
      "0.19863970588235294\n",
      "Epoch 30 starting..\n",
      "Epoch 30\n",
      "370.88007295131683\n",
      "0.20183823529411765\n",
      "Epoch 31 starting..\n",
      "Epoch 31\n",
      "372.5324286993812\n",
      "0.19941176470588234\n",
      "Epoch 32 starting..\n",
      "Epoch 32\n",
      "374.18246597402236\n",
      "0.20297794117647058\n",
      "Epoch 33 starting..\n",
      "Epoch 33\n",
      "375.8262628597372\n",
      "0.20283088235294117\n",
      "Epoch 34 starting..\n",
      "Epoch 34\n",
      "377.4711028407602\n",
      "0.20022058823529412\n",
      "Epoch 35 starting..\n",
      "Epoch 35\n",
      "379.1170777503182\n",
      "0.19716911764705883\n",
      "Epoch 36 starting..\n",
      "Epoch 36\n",
      "380.76330580430874\n",
      "0.19797794117647058\n",
      "Epoch 37 starting..\n",
      "Epoch 37\n",
      "382.40754161862765\n",
      "0.19816176470588234\n",
      "Epoch 38 starting..\n",
      "Epoch 38\n",
      "384.0486758737003\n",
      "0.20047794117647058\n",
      "Epoch 39 starting..\n",
      "Epoch 39\n",
      "385.6883153915405\n",
      "0.20091911764705883\n",
      "Epoch 40 starting..\n",
      "Epoch 40\n",
      "387.3316236313652\n",
      "0.19775735294117647\n",
      "Epoch 41 starting..\n",
      "Epoch 41\n",
      "388.9767449533238\n",
      "0.20327205882352942\n",
      "Epoch 42 starting..\n",
      "Epoch 42\n",
      "390.61776562298047\n",
      "0.20172794117647058\n",
      "Epoch 43 starting..\n",
      "Epoch 43\n",
      "392.25732041106505\n",
      "0.1965441176470588\n",
      "Epoch 44 starting..\n",
      "Epoch 44\n",
      "393.89925862761106\n",
      "0.2000735294117647\n",
      "Epoch 45 starting..\n",
      "Epoch 45\n",
      "395.5377829495598\n",
      "0.20044117647058823\n",
      "Epoch 46 starting..\n",
      "Epoch 46\n",
      "397.17874284351575\n",
      "0.198125\n",
      "Epoch 47 starting..\n",
      "Epoch 47\n",
      "398.8289031351314\n",
      "0.20143382352941178\n",
      "Epoch 48 starting..\n",
      "Epoch 48\n",
      "400.4733402588788\n",
      "0.2036029411764706\n",
      "Epoch 49 starting..\n",
      "Epoch 49\n",
      "402.1108717637903\n",
      "0.20161764705882354\n",
      "Epoch 50 starting..\n",
      "Epoch 50\n",
      "403.7518463835997\n",
      "0.19727941176470587\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "loss_values =[]\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch {} starting..'.format(epoch+1))\n",
    "    epoch_start = time.time()\n",
    "    classes_epoch, imgs_epoch = ut.shuffle_xy(classes_train,imgs_train) \n",
    "\n",
    "    n_batches = len(classes_epoch)//(n_classes*batch_size)\n",
    "    total_correct=0\n",
    "    total_batch=0\n",
    "    for batch in range(n_batches):\n",
    "        classes_batch = classes_epoch[batch*n_classes*batch_size:(batch+1)*n_classes*batch_size]\n",
    "        imgs_batch = imgs_epoch[batch*n_classes*batch_size:(batch+1)*n_classes*batch_size]\n",
    "\n",
    "        Xl_batch, yl_batch = [], []\n",
    "        for episode in range(batch_size):\n",
    "            imgs_ep = imgs_batch[episode*n_classes:(episode+1)*n_classes]\n",
    "            Xl_ep, yl_ep = [], []\n",
    "            for ind, cat in enumerate(imgs_ep):\n",
    "                for arr in cat:\n",
    "                    Xl_ep.append(arr)\n",
    "                    yl_ep.append(ut.one_hot(ind,n_classes))\n",
    "            Xl_shuff, yl_shuff = ut.shuffle_xy(Xl_ep,yl_ep)\n",
    "            X_arr, y_arr = np.asarray(Xl_shuff), np.asarray(yl_shuff)\n",
    "            Xl_batch.append(X_arr)\n",
    "            yl_batch.append(y_arr)\n",
    "        X_train, y_train = np.asarray(Xl_batch), np.asarray(yl_batch)\n",
    "        X_mann, y_mann = get_graph_input(X_train, y_train)\n",
    "        optimizer.zero_grad()\n",
    "        M, h, c, wu, wr, r, o = net(X_mann)\n",
    "        outs = o.view(100*16, 5)\n",
    "        gt = y_mann.view(100*16)\n",
    "        loss = criterion(outs, gt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_d = loss.item()\n",
    "        _, predicted = torch.max(outs.data, 1)\n",
    "        correct = (predicted == gt).sum().item()\n",
    "        total_correct+=correct\n",
    "        total_batch+=len(predicted)\n",
    "        loss_values.append(loss_d)\n",
    "#         if(epoch%10==0):\n",
    "#             print('Epoch %d'%(epoch+1),'/ Batch (%d/%d)'%(batch+1,n_batches),'/ Loss %f'%(loss_d))\n",
    "\n",
    "#     epoch_end = time.time()\n",
    "#     time_elapsed = epoch_end-epoch_start\n",
    "#     print('Epoch {} complete,'.format(epoch+1),time_elapsed,'seconds elapsed')\n",
    "    print('Epoch %d'%(epoch+1))\n",
    "    print (sum(loss_values)/n_batches)\n",
    "    print (total_correct*1.0/total_batch)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Visualizations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x128ef8518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_loss(train,name1=\"train_loss\"):\n",
    "    plt.plot(train, label=name1)\n",
    "    plt.legend()\n",
    "\n",
    "plot_loss(loss_values)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
