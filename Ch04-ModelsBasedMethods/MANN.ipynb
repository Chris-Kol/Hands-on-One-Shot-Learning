{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Augmented Neural Network using Omniglot Dataset\n",
    "\n",
    "\n",
    "In this tutorial, we will do following things step by step:\n",
    "1. Data Preprocessing: Creating Pairs.\n",
    "2. Create a Memory Augmented Enural Network\n",
    "3. Train it using Omniglot dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step1: Lets first import all libraries needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import utils as ut\n",
    "import os\n",
    "import time\n",
    "from scipy.misc import imread,imresize\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "We are reading images from two folders named 'image_background', 'image_evaluation' defined in 'data' directory\n",
    "\n",
    "Dataset is divided into 1423 charcters images for training and rest for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1623, 'total character classes')\n",
      "1423 characters assigned for training, 200 characters assigned for validation\n"
     ]
    }
   ],
   "source": [
    "width = 20\n",
    "\n",
    "# gather data paths\n",
    "subfolds = ut.extend_children('data','')\n",
    "datafolds = [subfolds[0],subfolds[1]]\n",
    "alphabets = ut.extend_generation(datafolds,'')\n",
    "charpaths = ut.extend_generation(alphabets,'')\n",
    "chars_dataset = [v.split('/')[2]+'/'+v.split('/')[3] for v in charpaths]\n",
    "\n",
    "# index-value conversion dictionaries for character set\n",
    "i2v = {i:v for i, v in enumerate(chars_dataset)}\n",
    "v2i = {v:i for i, v in enumerate(chars_dataset)}\n",
    "\n",
    "# get size of dataset\n",
    "mc_dataset = len(charpaths)\n",
    "print(mc_dataset,'total character classes')\n",
    "\n",
    "# train split\n",
    "mc_train = 1423\n",
    "chars_train = chars_dataset[:mc_train]\n",
    "classes_train = [v2i[v] for v in chars_train]\n",
    "\n",
    "# validation split\n",
    "mc_val = mc_dataset-mc_train\n",
    "chars_val = chars_dataset[-mc_val:]\n",
    "classes_val = [v2i[v] for v in chars_val]\n",
    "\n",
    "\n",
    "\n",
    "print('%s characters assigned for training, %s characters assigned for validation'%(mc_train,mc_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Reading\n",
    "\n",
    "All character images are read and saved in the imgs_dataset variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1623 character folders loaded\n",
      "200/1623 character folders loaded\n",
      "400/1623 character folders loaded\n",
      "600/1623 character folders loaded\n",
      "800/1623 character folders loaded\n",
      "1000/1623 character folders loaded\n",
      "1200/1623 character folders loaded\n",
      "1400/1623 character folders loaded\n",
      "1600/1623 character folders loaded\n"
     ]
    }
   ],
   "source": [
    "# load images from character paths\n",
    "imgs_dataset = []\n",
    "for i, charfold in enumerate(charpaths):\n",
    "    if i%200==0:\n",
    "        print('%s/%s character folders loaded'%(i,mc_dataset))\n",
    "    imgs_dataset.append([ ut.load_image(imgpath,(width,width))/127.5-1 for imgpath in ut.extend_children(charfold,'.png') ] )\n",
    "# access imgs_dataset [ character index ] [ sample index ] [ row,col ]\n",
    "\n",
    "# split images between train and validation sets\n",
    "imgs_train = imgs_dataset[:mc_train]\n",
    "imgs_val = imgs_dataset[-mc_val:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-paramters\n",
    "\n",
    "We define all the hyper-parameters and dimensions of all the variables here.\n",
    "* n_classes is 5 (in each batch we have 5 types of characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_classes = 5\n",
    "memory_size = 128 # number of features per entry\n",
    "memory_dim = 40 # number of entries in memory\n",
    "learning_rate = 1e-3\n",
    "batch_size = 16\n",
    "\n",
    "n_inputs = width*width+n_classes # input to LSTM cell.\n",
    "n_hnodes = 200 # LSTM cell size\n",
    "n_outputs = n_classes\n",
    "mem_size = memory_size # number of rows in external memory \n",
    "mem_dim = memory_dim # number of columns in external memory\n",
    "n_reads = 4 # number of heads. \n",
    "\n",
    "n_xh = n_inputs+n_hnodes # inputs to LSTM cell (previous state + input image dim)\n",
    "n_rd = n_reads*mem_dim\n",
    "n_hr = n_hnodes+n_rd\n",
    "gamma = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainable parameters\n",
    "\n",
    "We define all the hyper-parameters and dimensions of all the variables here.\n",
    "\n",
    "This defines three types of weights:\n",
    "* LSTM weights (we use LSTM controller, a feed-forward controller could also be used)\n",
    "* Read and Write key weights\n",
    "* Output fully connected layer weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LSTM gates (4 of them: weights and biases for each)\n",
    "W_gf = torch.Tensor(n_xh, n_hnodes).uniform_(-1., 1.).requires_grad_()\n",
    "b_gf = torch.Tensor(n_hnodes).uniform_(-1., 1.).requires_grad_()\n",
    "W_gi = torch.Tensor(n_xh,n_hnodes).uniform_(-1., 1.).requires_grad_()\n",
    "b_gi = torch.Tensor(n_hnodes).uniform_(-1., 1.).requires_grad_()\n",
    "W_go = torch.Tensor(n_xh,n_hnodes).uniform_(-1., 1.).requires_grad_()\n",
    "b_go = torch.Tensor(n_hnodes).uniform_(-1., 1.).requires_grad_()\n",
    "W_u = torch.Tensor(n_xh,n_hnodes).uniform_(-1., 1.).requires_grad_()\n",
    "b_u = torch.Tensor(n_hnodes).uniform_(-1., 1.).requires_grad_()\n",
    "# Controller Weights\n",
    "W_kr = torch.Tensor(n_hnodes,n_rd).uniform_(-1., 1.).requires_grad_()\n",
    "b_kr = torch.Tensor(n_rd).uniform_(-1., 1.).requires_grad_()\n",
    "W_kw = torch.Tensor(n_hnodes,n_rd).uniform_(-1., 1.).requires_grad_()\n",
    "b_kw = torch.Tensor(n_rd).uniform_(-1., 1.).requires_grad_()\n",
    "W_ga = torch.Tensor(n_hnodes,n_reads).uniform_(-1., 1.).requires_grad_()\n",
    "b_ga = torch.Tensor(n_reads).uniform_(-1., 1.).requires_grad_()\n",
    "# logit weights\n",
    "W_o = torch.Tensor(n_hr,n_outputs).uniform_(-1., 1.).requires_grad_()\n",
    "b_o = torch.Tensor(n_outputs).uniform_(-1., 1.).requires_grad_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "The next cell defines the forward pass of the network. \n",
    "It works as follows:\n",
    "* the net() function receives input X which is sliced along dim 0 which is the time dimension\n",
    "* we sequentially process each time step in run_one_step() function and collect state vectors of each step's output\n",
    "* The predictions are made at each time step with fully connected output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_state0():\n",
    "    # memory variables (not trainable.)\n",
    "    # initialize memory and LSTM states with zero. \n",
    "    return(\n",
    "        torch.FloatTensor(1e-6*np.random.rand(batch_size,mem_size,mem_dim)),\n",
    "        torch.FloatTensor(np.zeros((batch_size,n_hnodes))),\n",
    "        torch.FloatTensor(np.zeros((batch_size,n_hnodes))),\n",
    "        torch.FloatTensor(np.zeros((batch_size,mem_size))),\n",
    "        torch.FloatTensor(np.zeros((batch_size,n_reads,mem_size))),\n",
    "        torch.FloatTensor(np.zeros((batch_size,n_reads,mem_dim))),\n",
    "    )\n",
    "\n",
    "def run_one_step(X_t, state):\n",
    "    # Run one step of the episode.\n",
    "    M_tm1, h_tm1, c_tm1, wu_tm1, wr_tm1, r_tm1 = state\n",
    "    X_t_r = X_t.view(-1,n_inputs)\n",
    "    xh = torch.cat((X_t_r,h_tm1),1)\n",
    "    gf = torch.sigmoid(torch.matmul(xh,W_gf) + b_gf)\n",
    "    gi = torch.sigmoid(torch.matmul(xh,W_gi) + b_gi)\n",
    "    go = torch.sigmoid(torch.matmul(xh,W_go) + b_go)\n",
    "    u_t = torch.tanh(torch.matmul(xh,W_u) + b_u)\n",
    "    c_t = c_tm1*gf + u_t*gi\n",
    "    h_t = c_t*go\n",
    "    kr_t = torch.tanh(torch.matmul(c_t,W_kr) + b_kr).view(batch_size,n_reads,mem_dim)\n",
    "    kw_t = torch.tanh(torch.matmul(c_t,W_kw) + b_kw).view(batch_size,n_reads,mem_dim)\n",
    "    k_norm = torch.norm(kr_t, dim=2, keepdim=True)\n",
    "    m_norm = torch.norm(M_tm1, dim=2, keepdim=True)\n",
    "    inner_prod = torch.matmul(kr_t, M_tm1.permute(0,2,1))\n",
    "    norm_prod = torch.matmul(k_norm, m_norm.permute(0,2,1))\n",
    "    wr_t = F.softmax(inner_prod/norm_prod)\n",
    "    wu_1 = wu_tm1*gamma + torch.sum(wr_t, dim=1)\n",
    "    r_t = torch.matmul(wr_t,M_tm1)\n",
    "    ga = torch.unsqueeze(torch.sigmoid(torch.matmul(h_t,W_ga)+b_ga),2)\n",
    "    _, wlu_inds = torch.topk(-1*wu_1,k=n_reads)\n",
    "    wlu_t = torch.sum(F.one_hot(wlu_inds, mem_size).type(torch.FloatTensor),dim=1,keepdim=True)\n",
    "    ww_t = wr_t*ga + wlu_t*(1-ga)\n",
    "    wu_t = wu_1 + torch.sum(ww_t, dim=1)\n",
    "    M_1 = M_tm1 * (-1*wlu_t).permute(0,2,1)\n",
    "    M_t = M_1 + torch.matmul(ww_t.permute(0,2,1), kw_t)\n",
    "    st8_t = (M_t, h_t, c_t, wu_t, wr_t, r_t)\n",
    "    return st8_t\n",
    "    \n",
    "\n",
    "    \n",
    "def net(X=None, y=None):\n",
    "    # X is of shape (batch_size, None, width, width)\n",
    "    a = np.arange(100*16*405).reshape((100,16,405)).astype(np.float32)\n",
    "    X = torch.from_numpy(a)\n",
    "    \n",
    "    state0 = get_state0()\n",
    "    curr_state = state0\n",
    "    \n",
    "    # Collect output of state vectors in each time step.\n",
    "    M_f = []\n",
    "    h_f = []\n",
    "    c_f = []\n",
    "    wu_f = []\n",
    "    wr_f = []\n",
    "    r_f = []\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        curr_state = run_one_step(X[i], curr_state)\n",
    "        M_f.append(curr_state[0])\n",
    "        h_f.append(curr_state[1])\n",
    "        c_f.append(curr_state[2])\n",
    "        wu_f.append(curr_state[3])\n",
    "        wr_f.append(curr_state[4])\n",
    "        r_f.append(curr_state[5])\n",
    "        \n",
    "    M_f = torch.stack(M_f)\n",
    "    h_f = torch.stack(h_f)\n",
    "    c_f = torch.stack(c_f)\n",
    "    wu_f = torch.stack(wu_f)\n",
    "    wr_f = torch.stack(wr_f)\n",
    "    r_f = torch.stack(r_f)\n",
    "\n",
    "    hr = torch.cat((h_f, r_f.view(-1,batch_size,n_rd)),2)\n",
    "    o_f = torch.tensordot(hr,W_o,1)+b_o\n",
    "    return (M_f, h_f, c_f, wu_f, wr_f, r_f, o_f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We use Adam optimizer on cross entropy loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam([W_gf, b_gf, W_gi, b_gi, W_go, b_go, W_u, b_u, W_kr,b_kr, W_kw, b_kw,\n",
    "                      W_ga, b_ga, W_o, b_o], lr=learning_rate)\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare graph input\n",
    "\n",
    "Y-labels predicted at each time step are appended to input at next time step for better signal.\n",
    "* During training, we have access to y_labels at time step. Shift the y_labels by one time step and append this to the input to prepare input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_graph_input(X_train, y_train):\n",
    "    X = np.transpose(X_train.reshape(batch_size,-1,width*width),(1,0,2))\n",
    "    y_labels = np.transpose(y_train, (1,0,2))\n",
    "    y_labels_shifted = np.concatenate((np.zeros((1,batch_size,n_classes)), y_labels[:-1,:]),0)\n",
    "    X = np.concatenate((X,y_labels_shifted),-1)\n",
    "    y = np.argmax(y_labels, -1)\n",
    "    return torch.from_numpy(X), torch.from_numpy(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets begin training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 starting..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ankush/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch 1', '/ Batch (1/17)', '/ Loss 200.170456')\n",
      "('Epoch 1', '/ Batch (2/17)', '/ Loss 202.340668')\n",
      "('Epoch 1', '/ Batch (3/17)', '/ Loss 200.380417')\n",
      "('Epoch 1', '/ Batch (4/17)', '/ Loss 193.550079')\n",
      "('Epoch 1', '/ Batch (5/17)', '/ Loss 190.457809')\n",
      "('Epoch 1', '/ Batch (6/17)', '/ Loss 187.611465')\n",
      "('Epoch 1', '/ Batch (7/17)', '/ Loss 186.677292')\n",
      "('Epoch 1', '/ Batch (8/17)', '/ Loss 185.825378')\n",
      "('Epoch 1', '/ Batch (9/17)', '/ Loss 177.252838')\n",
      "('Epoch 1', '/ Batch (10/17)', '/ Loss 178.797775')\n",
      "('Epoch 1', '/ Batch (11/17)', '/ Loss 178.708374')\n",
      "('Epoch 1', '/ Batch (12/17)', '/ Loss 175.738464')\n",
      "('Epoch 1', '/ Batch (13/17)', '/ Loss 167.388611')\n",
      "('Epoch 1', '/ Batch (14/17)', '/ Loss 171.979233')\n",
      "('Epoch 1', '/ Batch (15/17)', '/ Loss 165.307419')\n",
      "('Epoch 1', '/ Batch (16/17)', '/ Loss 168.458649')\n",
      "('Epoch 1', '/ Batch (17/17)', '/ Loss 165.044144')\n",
      "('Epoch 1 complete,', 18.86061406135559, 'seconds elapsed')\n",
      "Epoch 2 starting..\n",
      "('Epoch 2', '/ Batch (1/17)', '/ Loss 162.092514')\n",
      "('Epoch 2', '/ Batch (2/17)', '/ Loss 162.351059')\n",
      "('Epoch 2', '/ Batch (3/17)', '/ Loss 156.032089')\n",
      "('Epoch 2', '/ Batch (4/17)', '/ Loss 152.118561')\n",
      "('Epoch 2', '/ Batch (5/17)', '/ Loss 153.896851')\n",
      "('Epoch 2', '/ Batch (6/17)', '/ Loss 149.561584')\n",
      "('Epoch 2', '/ Batch (7/17)', '/ Loss 147.276535')\n",
      "('Epoch 2', '/ Batch (8/17)', '/ Loss 145.922104')\n",
      "('Epoch 2', '/ Batch (9/17)', '/ Loss 143.409027')\n",
      "('Epoch 2', '/ Batch (10/17)', '/ Loss 141.427887')\n",
      "('Epoch 2', '/ Batch (11/17)', '/ Loss 141.532654')\n",
      "('Epoch 2', '/ Batch (12/17)', '/ Loss 141.506638')\n",
      "('Epoch 2', '/ Batch (13/17)', '/ Loss 139.341980')\n",
      "('Epoch 2', '/ Batch (14/17)', '/ Loss 132.274887')\n",
      "('Epoch 2', '/ Batch (15/17)', '/ Loss 133.196213')\n",
      "('Epoch 2', '/ Batch (16/17)', '/ Loss 132.282532')\n",
      "('Epoch 2', '/ Batch (17/17)', '/ Loss 130.170227')\n",
      "('Epoch 2 complete,', 17.76338815689087, 'seconds elapsed')\n",
      "Epoch 3 starting..\n",
      "('Epoch 3', '/ Batch (1/17)', '/ Loss 132.114014')\n",
      "('Epoch 3', '/ Batch (2/17)', '/ Loss 130.675735')\n",
      "('Epoch 3', '/ Batch (3/17)', '/ Loss 124.745667')\n",
      "('Epoch 3', '/ Batch (4/17)', '/ Loss 125.082336')\n",
      "('Epoch 3', '/ Batch (5/17)', '/ Loss 123.249237')\n",
      "('Epoch 3', '/ Batch (6/17)', '/ Loss 124.547676')\n",
      "('Epoch 3', '/ Batch (7/17)', '/ Loss 126.770966')\n",
      "('Epoch 3', '/ Batch (8/17)', '/ Loss 120.306564')\n",
      "('Epoch 3', '/ Batch (9/17)', '/ Loss 117.717560')\n",
      "('Epoch 3', '/ Batch (10/17)', '/ Loss 120.861710')\n",
      "('Epoch 3', '/ Batch (11/17)', '/ Loss 116.814804')\n",
      "('Epoch 3', '/ Batch (12/17)', '/ Loss 113.780167')\n",
      "('Epoch 3', '/ Batch (13/17)', '/ Loss 114.086563')\n",
      "('Epoch 3', '/ Batch (14/17)', '/ Loss 113.832870')\n",
      "('Epoch 3', '/ Batch (15/17)', '/ Loss 109.371330')\n",
      "('Epoch 3', '/ Batch (16/17)', '/ Loss 111.203705')\n",
      "('Epoch 3', '/ Batch (17/17)', '/ Loss 105.127686')\n",
      "('Epoch 3 complete,', 17.91055917739868, 'seconds elapsed')\n",
      "Epoch 4 starting..\n",
      "('Epoch 4', '/ Batch (1/17)', '/ Loss 106.193703')\n",
      "('Epoch 4', '/ Batch (2/17)', '/ Loss 106.543747')\n",
      "('Epoch 4', '/ Batch (3/17)', '/ Loss 105.325150')\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 400\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch {} starting..'.format(epoch+1))\n",
    "    epoch_start = time.time()\n",
    "    classes_epoch, imgs_epoch = ut.shuffle_xy(classes_train,imgs_train) \n",
    "\n",
    "    n_batches = len(classes_epoch)//(n_classes*batch_size)\n",
    "    for batch in range(n_batches):\n",
    "        classes_batch = classes_epoch[batch*n_classes*batch_size:(batch+1)*n_classes*batch_size]\n",
    "        imgs_batch = imgs_epoch[batch*n_classes*batch_size:(batch+1)*n_classes*batch_size]\n",
    "\n",
    "        Xl_batch, yl_batch = [], []\n",
    "        for episode in range(batch_size):\n",
    "            imgs_ep = imgs_batch[episode*n_classes:(episode+1)*n_classes]\n",
    "            Xl_ep, yl_ep = [], []\n",
    "            for ind, cat in enumerate(imgs_ep):\n",
    "                for arr in cat:\n",
    "                    Xl_ep.append(arr)\n",
    "                    yl_ep.append(ut.one_hot(ind,n_classes))\n",
    "            Xl_shuff, yl_shuff = ut.shuffle_xy(Xl_ep,yl_ep)\n",
    "            X_arr, y_arr = np.asarray(Xl_shuff), np.asarray(yl_shuff)\n",
    "            Xl_batch.append(X_arr)\n",
    "            yl_batch.append(y_arr)\n",
    "        X_train, y_train = np.asarray(Xl_batch), np.asarray(yl_batch)\n",
    "        X_mann, y_mann = get_graph_input(X_train, y_train)\n",
    "        optimizer.zero_grad()\n",
    "        M, h, c, wu, wr, r, o = net(X_mann)\n",
    "        outs = o.view(100*16, 5)\n",
    "        gt = y_mann.view(100*16)\n",
    "        loss = criterion(outs, gt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_d = loss.item()\n",
    "        _, predicted = torch.max(outs.data, 1)\n",
    "        correct = (predicted == gt).sum().item()\n",
    "\n",
    "        print('Epoch %d'%(epoch+1),'/ Batch (%d/%d)'%(batch+1,n_batches),'/ Loss %f'%(loss_d))\n",
    "\n",
    "    epoch_end = time.time()\n",
    "    time_elapsed = epoch_end-epoch_start\n",
    "    print('Epoch {} complete,'.format(epoch+1),time_elapsed,'seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
