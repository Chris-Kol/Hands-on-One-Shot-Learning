{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Augmented Neural Network using Omniglot Dataset\n",
    "\n",
    "As we showcased that a Neural Turing Machine's controller is capable of using content-based addressing, location-based addressing or both. Whereas, here MANN works on using a pure content-based memory writer. \n",
    "\n",
    "MANN also use a new addressing schema called least recently used access. The idea behind the scene is that the least recently used memory location is determined by the read operation and the read operation is performed by content-based addressing. So, we basically perform content-based addressing for reading and write to the location that was least recently used.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[picture credits: MANN Paper(https://arxiv.org/pdf/1605.06065.pdf)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"mann.png\" width=\"1500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will do following things step by step:\n",
    "1. Implement Read Operation\n",
    "2. Implement Write Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step1: Lets first import all libraries needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Implementing Read Operation\n",
    "Here, We will define read heads which access memory and updates memory according to read operations we discuss in chapter above.\n",
    "\n",
    "<img src=\"MANN_read.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1623 total character classes\n",
      "1423 characters assigned for training, 200 characters assigned for validation\n"
     ]
    }
   ],
   "source": [
    "class ReadHead(Memory):\n",
    "\n",
    "    def __init__(self, M, N, controller_out):\n",
    "        super(ReadHead, self).__init__(M, N, controller_out)\n",
    "        self.fc_read = nn.Linear(controller_out, self.read_lengths)\n",
    "        self.intialize_parameters();\n",
    "\n",
    "    def intialize_parameters(self):\n",
    "        # Initialize the linear layers\n",
    "        nn.init.xavier_uniform_(self.fc_read.weight, gain=1.4)\n",
    "        nn.init.normal_(self.fc_read.bias, std=0.01)\n",
    "\n",
    "    def read(self, memory, w):\n",
    "        \"\"\"Read from memory.\"\"\"\n",
    "        return torch.matmul(w, memory)\n",
    "\n",
    "    def forward(self, x, memory):\n",
    "        param = self.fc_read(x)\n",
    "        k, beta, g, s, gamma = torch.split(param, [self.N, 1, 1, 3, 1], dim=1)\n",
    "\n",
    "        k = torch.tanh(k)\n",
    "        g = F.sigmoid(g)\n",
    "        s = F.softmax(s, dim=1)\n",
    "        gamma = 1 + F.softplus(gamma)\n",
    "\n",
    "        w = self.address(k, g, s, gamma, memory, self.w_last[-1])\n",
    "        self.w_last.append(w)\n",
    "        mem = self.read(memory, w)\n",
    "        return mem, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 5: Implement Write Operation\n",
    "Similar to Read Operation, here we will implement write operation.\n",
    "\n",
    "Note: Both read and write heads use fully connected layer to produce paremeters (k, beta, g, s, gamma) for content addressing. \n",
    "\n",
    "<img src=\"MANN_write.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WriteHead(Memory):\n",
    "\n",
    "    def __init__(self, M, N, controller_out):\n",
    "        super(WriteHead, self).__init__(M, N, controller_out)\n",
    "        self.fc_write = nn.Linear(controller_out, self.write_lengths)\n",
    "        self.intialize_parameters()\n",
    "\n",
    "    def intialize_parameters(self):\n",
    "        # Initialize the linear layers\n",
    "        nn.init.xavier_uniform_(self.fc_write.weight, gain=1.4)\n",
    "        nn.init.normal_(self.fc_write.bias, std=0.01)\n",
    "    \n",
    "    def mann_write(self, memory, w, e, a):\n",
    "        # usage weight vector\n",
    "        w_u = w_read + w_write # keep track of last value\n",
    "        w_u_current = w_u_last+w_read+w_write\n",
    "        w_write = torch.sigmoid(a)*w_read_last + (1-torch.sigmoid(a))*w_least_used_weight_vector_last\n",
    "        memory_update = m_last + w_write*k_t\n",
    "        \n",
    "    def forward(self, x, memory):\n",
    "        param = self.fc_write(x)\n",
    "\n",
    "        k, beta, g, s, gamma, a, e = torch.split(param, [self.N, 1, 1, 3, 1, self.N, self.N], dim=1)\n",
    "\n",
    "        k = F.tanh(k)\n",
    "        beta = F.softplus(beta)\n",
    "        g = F.sigmoid(g)\n",
    "        s = F.softmax(s, dim=-1)\n",
    "        gamma = 1 + F.softplus(gamma)\n",
    "        a = F.tanh(a)\n",
    "        e = F.sigmoid(e)\n",
    "\n",
    "        w = self.address(k, beta, g, s, gamma, memory, self.w_last[-1])\n",
    "        self.w_last.append(w)\n",
    "        mem = self.write(memory, w, e, a)\n",
    "        return mem, w\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
