{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Turing Machines: Tutorial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from time import time\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controller\n",
    "\n",
    "* Controller is 2-layer feedforward network.\n",
    "* Initialize weights with xavier initialization\n",
    "* Use sigmoid non-linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Controller(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, num_hiddens):\n",
    "        super(Controller, self).__init__()\n",
    "\n",
    "        print(\"--- Initialize Controller\")\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hiddens)\n",
    "        self.fc2 = nn.Linear(num_hiddens, num_outputs)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Initialize the linear layers\n",
    "        nn.init.xavier_uniform_(self.fc1.weight, gain=1.4)\n",
    "        nn.init.normal(self.fc1.bias, std=0.01)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.fc2.weight, gain=1.4)\n",
    "        nn.init.normal(self.fc2.bias, std=0.01)\n",
    "\n",
    "    def forward(self, x, last_read):\n",
    "\n",
    "        x = torch.cat((x, last_read), dim=1)\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory\n",
    "\n",
    "* Memory M is a 2-D matrix, with M rows, and N columns\n",
    "* the function address() does the the memory addressing which is composed of four functions\n",
    "    * similarity\n",
    "    * interpolation\n",
    "    * shifting\n",
    "    * sharpening "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Memory(nn.Module):\n",
    "    def __init__(self, M, N, controller_out):\n",
    "        super(Memory, self).__init__()\n",
    "\n",
    "        self.N = N\n",
    "        self.M = M\n",
    "        self.read_lengths = self.N + 1 + 1 + 3 + 1\n",
    "        self.write_lengths = self.N + 1 + 1 + 3 + 1 + self.N + self.N\n",
    "        self.w_last = []\n",
    "        self.reset_memory()\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.w_last\n",
    "\n",
    "    def reset_memory(self):\n",
    "        self.w_last = []\n",
    "        self.w_last.append(torch.zeros([1, self.M], dtype=torch.float32))\n",
    "\n",
    "    def address(self, k, beta, g, s, gamma, memory, w_last):\n",
    "        # Content focus\n",
    "        wc = self._similarity(k, beta, memory)\n",
    "        # Location focus\n",
    "        wg = self._interpolate(wc, g, w_last)\n",
    "        w_hat = self._shift(wg, s)\n",
    "        w = self._sharpen(w_hat, gamma)\n",
    "\n",
    "        return w\n",
    "\n",
    "    def _similarity(self, k, beta, memory):\n",
    "        # Similarit√† coseno\n",
    "        w = F.cosine_similarity(memory, k, -1, 1e-16)\n",
    "        w = F.softmax(beta * w, dim=-1)\n",
    "        return w\n",
    "\n",
    "    def _interpolate(self, wc, g, w_last):\n",
    "        return g * wc + (1 - g) * w_last\n",
    "\n",
    "    def _shift(self, wg, s):\n",
    "        result = torch.zeros(wg.size())\n",
    "        result = _convolve(wg, s)\n",
    "        return result\n",
    "\n",
    "    def _sharpen(self, w_hat, gamma):\n",
    "        w = w_hat ** gamma\n",
    "        w = torch.div(w, torch.sum(w, dim=-1) + 1e-16)\n",
    "        return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and Write operation\n",
    "* In the next two cells we define read and write operations as we discussed in the chapter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReadHead(Memory):\n",
    "\n",
    "    def __init__(self, M, N, controller_out):\n",
    "        super(ReadHead, self).__init__(M, N, controller_out)\n",
    "\n",
    "        print(\"--- Initialize Memory: ReadHead\")\n",
    "        self.fc_read = nn.Linear(controller_out, self.read_lengths)\n",
    "        self.reset_parameters();\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Initialize the linear layers\n",
    "        nn.init.xavier_uniform_(self.fc_read.weight, gain=1.4)\n",
    "        nn.init.normal(self.fc_read.bias, std=0.01)\n",
    "\n",
    "    def read(self, memory, w):\n",
    "        \"\"\"Read from memory (according to section 3.1).\"\"\"\n",
    "        return torch.matmul(w, memory)\n",
    "\n",
    "    def forward(self, x, memory):\n",
    "        param = self.fc_read(x)\n",
    "        k, beta, g, s, gamma = torch.split(param, [self.N, 1, 1, 3, 1], dim=1)\n",
    "\n",
    "        k = F.tanh(k)\n",
    "        beta = F.softplus(beta)\n",
    "        g = F.sigmoid(g)\n",
    "        s = F.softmax(s, dim=1)\n",
    "        gamma = 1 + F.softplus(gamma)\n",
    "\n",
    "        w = self.address(k, beta, g, s, gamma, memory, self.w_last[-1])\n",
    "        self.w_last.append(w)\n",
    "        mem = self.read(memory, w)\n",
    "        return mem, w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WriteHead(Memory):\n",
    "\n",
    "    def __init__(self, M, N, controller_out):\n",
    "        super(WriteHead, self).__init__(M, N, controller_out)\n",
    "\n",
    "        print(\"--- Initialize Memory: WriteHead\")\n",
    "        self.fc_write = nn.Linear(controller_out, self.write_lengths)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Initialize the linear layers\n",
    "        nn.init.xavier_uniform_(self.fc_write.weight, gain=1.4)\n",
    "        nn.init.normal_(self.fc_write.bias, std=0.01)\n",
    "\n",
    "    def write(self, memory, w, e, a):\n",
    "        \"\"\"write to memory (according to section 3.2).\"\"\"\n",
    "        w = torch.squeeze(w)\n",
    "        e = torch.squeeze(e)\n",
    "        a = torch.squeeze(a)\n",
    "\n",
    "        erase = torch.ger(w, e)\n",
    "        add = torch.ger(w, a)\n",
    "\n",
    "        m_tilde = memory * (1 - erase)\n",
    "        memory_update = m_tilde + add\n",
    "\n",
    "        return memory_update\n",
    "\n",
    "    def forward(self, x, memory):\n",
    "        param = self.fc_write(x)\n",
    "\n",
    "        k, beta, g, s, gamma, a, e = torch.split(param, [self.N, 1, 1, 3, 1, self.N, self.N], dim=1)\n",
    "\n",
    "        k = F.tanh(k)\n",
    "        beta = F.softplus(beta)\n",
    "        g = F.sigmoid(g)\n",
    "        s = F.softmax(s, dim=-1)\n",
    "        gamma = 1 + F.softplus(gamma)\n",
    "        a = F.tanh(a)\n",
    "        e = F.sigmoid(e)\n",
    "\n",
    "        w = self.address(k, beta, g, s, gamma, memory, self.w_last[-1])\n",
    "        self.w_last.append(w)\n",
    "        mem = self.write(memory, w, e, a)\n",
    "        return mem, w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolve is needed in the shifting step of content addressing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _convolve(w, s):\n",
    "    \"\"\"Circular convolution implementation.\"\"\"\n",
    "    b, d = s.shape\n",
    "    assert b == 1, 'does _convolve work for b != 1?'\n",
    "    assert d == 3\n",
    "    w = torch.squeeze(w)\n",
    "    t = torch.cat([w[-1:], w, w[:1]])\n",
    "    c = F.conv1d(t.view(1, 1, -1), s.view(1, 1, -1)).view(b, -1)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network here which includes:\n",
    "* fully connected controller\n",
    "* read and write heads\n",
    "* memory paramters\n",
    "* utility functions to operate on memory which is not trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NTM(nn.Module):\n",
    "    def __init__(self, M, N, num_inputs, num_outputs, controller_out_dim, controller_hid_dim):\n",
    "        super(NTM, self).__init__()\n",
    "\n",
    "        print(\"----------- Build Neural Turing machine -----------\")\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "        self.M = M\n",
    "        self.N = N\n",
    "\n",
    "        self.memory = torch.zeros(self.M, self.N)\n",
    "        self.last_read = torch.zeros(1, self.N)\n",
    "\n",
    "        self.controller = Controller(self.num_inputs + self.N, controller_out_dim, controller_hid_dim)\n",
    "        self.read_head = ReadHead(self.M, self.N, controller_out_dim)\n",
    "        self.write_head = WriteHead(self.M, self.N, controller_out_dim)\n",
    "\n",
    "        self.fc_out = nn.Linear(self.num_inputs + N, self.num_outputs)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, X=None):\n",
    "\n",
    "        if X is None:\n",
    "            X = torch.zeros(1, self.num_inputs)\n",
    "\n",
    "        controller_out = self.controller(X, self.last_read)\n",
    "        self._read_write(controller_out)\n",
    "\n",
    "        out = torch.cat((X, self.last_read), -1)\n",
    "        out = F.sigmoid(self.fc_out(out))\n",
    "\n",
    "        return out\n",
    "\n",
    "    def _read_write(self, controller_out):\n",
    "        # READ\n",
    "        read, w = self.read_head(controller_out, self.memory)\n",
    "        self.last_read = read\n",
    "\n",
    "        # WRITE\n",
    "        mem, w = self.write_head(controller_out, self.memory)\n",
    "        self.memory = mem\n",
    "\n",
    "    def initalize_state(self):\n",
    "        stdev = 1 / (np.sqrt(self.N + self.M))\n",
    "        self.memory = nn.init.uniform_((torch.Tensor(self.M, self.N)), -stdev, stdev)\n",
    "        self.last_read = F.tanh(torch.randn(1, self.N))\n",
    "\n",
    "        self.read_head.reset_memory()\n",
    "        self.write_head.reset_memory()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Initialize the linear layers\n",
    "        nn.init.xavier_uniform_(self.fc_out.weight, gain=1.4)\n",
    "        nn.init.normal_(self.fc_out.bias, std=0.5)\n",
    "\n",
    "    def get_memory_info(self):\n",
    "        return self.memory, self.read_head.get_weights(), self.write_head.get_weights()\n",
    "\n",
    "    def calculate_num_params(self):\n",
    "        \"\"\"Returns the total number of parameters.\"\"\"\n",
    "        num_params = 0\n",
    "        for p in self.parameters():\n",
    "            num_params += p.data.view(-1).size(0)\n",
    "        return num_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation for COPY task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BinaySeqDataset(Dataset):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        self.seq_len = args['sequence_length']\n",
    "        self.seq_width = args['token_size']\n",
    "        self.dataset_dim = args['training_samples']\n",
    "\n",
    "    def _generate_seq(self):\n",
    "        seq = np.random.binomial(1, 0.5, (self.seq_len, self.seq_width))\n",
    "        seq = torch.from_numpy(seq)\n",
    "        # Add start and end token\n",
    "        inp = torch.zeros(self.seq_len + 2, self.seq_width)\n",
    "        inp[1:self.seq_len + 1, :self.seq_width] = seq.clone()\n",
    "        inp[0, 0] = 1.0\n",
    "        inp[self.seq_len + 1, self.seq_width - 1] = 1.0\n",
    "        outp = seq.data.clone()\n",
    "\n",
    "        return inp.float(), outp.float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_dim\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inp, out = self._generate_seq()\n",
    "        return inp, out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient clipping\n",
    "* Its generally a good idea to clip gradients so that the network is numerically stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clip_grads(net):\n",
    "    parameters = list(filter(lambda p: p.grad is not None, net.parameters()))\n",
    "    for p in parameters:\n",
    "        p.grad.data.clamp_(args['min_grad'], args['max_grad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- Build Neural Turing machine -----------\n",
      "--- Initialize Controller\n",
      "--- Initialize Memory: ReadHead\n",
      "--- Initialize Memory: WriteHead\n",
      "NTM(\n",
      "  (controller): Controller(\n",
      "    (fc1): Linear(in_features=138, out_features=512, bias=True)\n",
      "    (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  )\n",
      "  (read_head): ReadHead(\n",
      "    (fc_read): Linear(in_features=256, out_features=134, bias=True)\n",
      "  )\n",
      "  (write_head): WriteHead(\n",
      "    (fc_write): Linear(in_features=256, out_features=390, bias=True)\n",
      "  )\n",
      "  (fc_out): Linear(in_features=138, out_features=10, bias=True)\n",
      ")\n",
      "--------- Number of parameters -----------\n",
      "338554\n",
      "--------- Start training -----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ankush/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:13: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "/Users/Ankush/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:16: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "/Users/Ankush/anaconda/lib/python2.7/site-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/Users/Ankush/anaconda/lib/python2.7/site-packages/torch/nn/functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss: ', 0.7290575504302979)\n",
      "('Loss: ', 0.7290575504302979)\n",
      "('Loss: ', 0.7143115997314453)\n",
      "('Loss: ', 0.702458918094635)\n",
      "('Loss: ', 0.756542980670929)\n",
      "('Loss: ', 0.7057434320449829)\n",
      "('Loss: ', 0.704180896282196)\n",
      "('Loss: ', 0.7027608752250671)\n",
      "('Loss: ', 0.7021108269691467)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    args = {'sequence_length':300,'token_size':10,'memory_capacity':64,'memory_vector_size':128,'training_samples':99,\n",
    "            'controller_output_dim':256,'controller_hidden_dim':512,'learning_rate':1e-4,'min_grad':-10,'max_grad':10,\n",
    "           'logdir':'./','loadmodel':'','savemodel':'checkpoint.model'}\n",
    "    # writer = SummaryWriter()\n",
    "    dataset = BinaySeqDataset(args)\n",
    "    dataloader = DataLoader(dataset, batch_size=1,\n",
    "                            shuffle=True, num_workers=4)\n",
    "\n",
    "    model = NTM(M=args['memory_capacity'],\n",
    "                N=args['memory_vector_size'],\n",
    "                num_inputs=args['token_size'],\n",
    "                num_outputs=args['token_size'],\n",
    "                controller_out_dim=args['controller_output_dim'],\n",
    "                controller_hid_dim=args['controller_hidden_dim'],\n",
    "                )\n",
    "\n",
    "    print(model)\n",
    "\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=args['learning_rate'])\n",
    "\n",
    "    print(\"--------- Number of parameters -----------\")\n",
    "    print(model.calculate_num_params())\n",
    "    print(\"--------- Start training -----------\")\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    if args['loadmodel'] != '':\n",
    "        model.load_state_dict(torch.load(args['loadmodel']))\n",
    "\n",
    "    for e, (X, Y) in enumerate(dataloader):\n",
    "        tmp = time()\n",
    "        model.initalize_state()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inp_seq_len = args['sequence_length'] + 2\n",
    "        out_seq_len = args['sequence_length']\n",
    "\n",
    "        X.requires_grad = True\n",
    "\n",
    "        # Input rete: sequenza\n",
    "        for t in range(0, inp_seq_len):\n",
    "            model(X[:, t])\n",
    "\n",
    "        # Input rete: null\n",
    "        y_pred = torch.zeros(Y.size())\n",
    "        for i in range(0, out_seq_len):\n",
    "            y_pred[:, i] = model()\n",
    "\n",
    "        loss = criterion(y_pred, Y)\n",
    "        loss.backward()\n",
    "        clip_grads(model)\n",
    "        optimizer.step()\n",
    "        losses += [loss.item()]\n",
    "        print(\"Loss: \", loss.item())\n",
    "\n",
    "        if e % 50 == 0:\n",
    "            mean_loss = np.array(losses[-50:]).mean()\n",
    "            print(\"Loss: \", loss.item())\n",
    "#             writer.add_scalar('Mean loss', loss.item(), e)\n",
    "            if e % 1000 == 0:\n",
    "#                 for name, param in model.named_parameters():\n",
    "#                     writer.add_histogram(name, param.clone().cpu().data.numpy(), e)\n",
    "                mem_pic, read_pic, write_pic = model.get_memory_info()\n",
    "#                 pic1 = vutils.make_grid(y_pred, normalize=True, scale_each=True)\n",
    "#                 pic2 = vutils.make_grid(Y, normalize=True, scale_each=True)\n",
    "#                 pic3 = vutils.make_grid(mem_pic, normalize=True, scale_each=True)\n",
    "#                 pic4 = vutils.make_grid(read_pic, normalize=True, scale_each=True)\n",
    "#                 pic5 = vutils.make_grid(write_pic, normalize=True, scale_each=True)\n",
    "#                 writer.add_image('NTM output', pic1, e)\n",
    "#                 writer.add_image('True output', pic2, e)\n",
    "#                 writer.add_image('Memory', pic3, e)\n",
    "#                 writer.add_image('Read weights', pic4, e)\n",
    "#                 writer.add_image('Write weights', pic5, e)\n",
    "#                 torch.save(model.state_dict(), args.savemodel)\n",
    "            losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
