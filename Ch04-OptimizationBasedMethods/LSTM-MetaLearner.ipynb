{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Meta Learner on MiniImageNet Dataset\n",
    "\n",
    "Please Download data using link and save it in save folder as of this notebook after extracting: https://drive.google.com/file/d/1rV3aj_hgfNTfCakffpPm7Vhpr1in87CR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from __future__ import division, print_function, absolute_import\n",
    "import os\n",
    "import re\n",
    "import pdb\n",
    "import copy\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import PIL.Image as PILI\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "import logging\n",
    "import os\n",
    "from torchvision.datasets.utils import download_file_from_google_drive, extract_archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Data Loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EpisodeDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, root, phase='train', n_shot=5, n_eval=15, transform=None):\n",
    "        \"\"\"Args:\n",
    "            root (str): path to data\n",
    "            phase (str): train, val or test\n",
    "            n_shot (int): how many examples per class for training (k/n_support)\n",
    "            n_eval (int): how many examples per class for evaluation\n",
    "                - n_shot + n_eval = batch_size for data.DataLoader of ClassDataset\n",
    "            transform (torchvision.transforms): data augmentation\n",
    "        \"\"\"\n",
    "        root = os.path.join(root, phase)\n",
    "        self.labels = sorted(os.listdir(root))\n",
    "        images = [glob.glob(os.path.join(root, label, '*')) for label in self.labels]\n",
    "#         print (len(images))\n",
    "#         for s in self.labels:\n",
    "#             print (s)\n",
    "#         for i in range(0,len(images)):\n",
    "#             if len(images)<1:\n",
    "#                 print (\"found true\")\n",
    "#                 del images[i]\n",
    "#                 del self.labels[i]\n",
    "        self.episode_loader = [data.DataLoader(\n",
    "            ClassDataset(images=images[idx], label=idx, transform=transform),\n",
    "            batch_size=n_shot+n_eval, shuffle=True, num_workers=0) for idx, _ in enumerate(self.labels)]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return next(iter(self.episode_loader[idx]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ClassDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, images, label, transform=None):\n",
    "        \"\"\"Args:\n",
    "            images (list of str): each item is a path to an image of the same label\n",
    "            label (int): the label of all the images\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.label = label\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = PILI.open(self.images[idx]).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, self.label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EpisodicSampler(data.Sampler):\n",
    "\n",
    "    def __init__(self, total_classes, n_class, n_episode):\n",
    "        self.total_classes = total_classes\n",
    "        self.n_class = n_class\n",
    "        self.n_episode = n_episode\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(self.n_episode):\n",
    "            yield torch.randperm(self.total_classes)[:self.n_class]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(args):\n",
    "\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "    transform1= transforms.Compose([\n",
    "            transforms.Resize(args['image_size'] * 8 // 7),\n",
    "            transforms.CenterCrop(args['image_size']),\n",
    "            transforms.ToTensor(),\n",
    "            normalize])\n",
    "    \n",
    "    transform2 = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(args['image_size']),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(\n",
    "                brightness=0.4,\n",
    "                contrast=0.4,\n",
    "                saturation=0.4,\n",
    "                hue=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            normalize])\n",
    "    \n",
    "    train_set = EpisodeDataset(args['data_root'], 'train', args['n_shot'], args['n_eval'],\n",
    "        transform=transform2)\n",
    "\n",
    "    val_set = EpisodeDataset(args['data_root'], 'val', args['n_shot'], args['n_eval'],\n",
    "        transform=transform1)\n",
    "\n",
    "    test_set = EpisodeDataset(args['data_root'], 'test', args['n_shot'], args['n_eval'],\n",
    "        transform=transform1)\n",
    "\n",
    "    train_loader = data.DataLoader(train_set, num_workers=4, pin_memory=True,\n",
    "        batch_sampler=EpisodicSampler(len(train_set), args['n_class'], args['episode']))\n",
    "\n",
    "    val_loader = data.DataLoader(val_set, num_workers=2, pin_memory=True,\n",
    "        batch_sampler=EpisodicSampler(len(val_set), args['n_class'], args['episode_val']))\n",
    "\n",
    "    test_loader = data.DataLoader(test_set, num_workers=2, pin_memory=True,\n",
    "        batch_sampler=EpisodicSampler(len(test_set), args['n_class'], args['episode_val']))\n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Learner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Learner(nn.Module):\n",
    "\n",
    "    def __init__(self, image_size, bn_eps, bn_momentum, n_classes):\n",
    "        super(Learner, self).__init__()\n",
    "        self.model = nn.ModuleDict({'features': nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(3, 32, 3, padding=1)),\n",
    "            ('norm1', nn.BatchNorm2d(32, bn_eps, bn_momentum)),\n",
    "            ('relu1', nn.ReLU(inplace=False)),\n",
    "            ('pool1', nn.MaxPool2d(2)),\n",
    "\n",
    "            ('conv2', nn.Conv2d(32, 32, 3, padding=1)),\n",
    "            ('norm2', nn.BatchNorm2d(32, bn_eps, bn_momentum)),\n",
    "            ('relu2', nn.ReLU(inplace=False)),\n",
    "            ('pool2', nn.MaxPool2d(2)),\n",
    "\n",
    "            ('conv3', nn.Conv2d(32, 32, 3, padding=1)),\n",
    "            ('norm3', nn.BatchNorm2d(32, bn_eps, bn_momentum)),\n",
    "            ('relu3', nn.ReLU(inplace=False)),\n",
    "            ('pool3', nn.MaxPool2d(2)),\n",
    "\n",
    "            ('conv4', nn.Conv2d(32, 32, 3, padding=1)),\n",
    "            ('norm4', nn.BatchNorm2d(32, bn_eps, bn_momentum)),\n",
    "            ('relu4', nn.ReLU(inplace=False)),\n",
    "            ('pool4', nn.MaxPool2d(2))]))\n",
    "        })\n",
    "\n",
    "        clr_in = image_size // 2**4\n",
    "        self.model.update({'cls': nn.Linear(32 * clr_in * clr_in, n_classes)})\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model.features(x)\n",
    "        x = torch.reshape(x, [x.size(0), -1])\n",
    "        outputs = self.model.cls(x)\n",
    "        return outputs\n",
    "\n",
    "    def get_flat_params(self):\n",
    "        return torch.cat([p.view(-1) for p in self.model.parameters()], 0)\n",
    "\n",
    "    def copy_flat_params(self, cI):\n",
    "        idx = 0\n",
    "        for p in self.model.parameters():\n",
    "            plen = p.view(-1).size(0)\n",
    "            p.data.copy_(cI[idx: idx+plen].view_as(p))\n",
    "            idx += plen\n",
    "\n",
    "    def transfer_params(self, learner_w_grad, cI):\n",
    "        # Use load_state_dict only to copy the running mean/var in batchnorm, the values of the parameters\n",
    "        #  are going to be replaced by cI\n",
    "        self.load_state_dict(learner_w_grad.state_dict())\n",
    "        #  replace nn.Parameters with tensors from cI (NOT nn.Parameters anymore).\n",
    "        idx = 0\n",
    "        for m in self.model.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.Linear):\n",
    "                wlen = m._parameters['weight'].view(-1).size(0)\n",
    "                m._parameters['weight'] = cI[idx: idx+wlen].view_as(m._parameters['weight']).clone()\n",
    "                idx += wlen\n",
    "                if m._parameters['bias'] is not None:\n",
    "                    blen = m._parameters['bias'].view(-1).size(0)\n",
    "                    m._parameters['bias'] = cI[idx: idx+blen].view_as(m._parameters['bias']).clone()\n",
    "                    idx += blen\n",
    "\n",
    "    def reset_batch_stats(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                m.reset_running_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Meta Learner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MetaLSTMCell(nn.Module):\n",
    "    \"\"\"C_t = f_t * C_{t-1} + i_t * \\tilde{C_t}\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, n_learner_params):\n",
    "        super(MetaLSTMCell, self).__init__()\n",
    "        \"\"\"Args:\n",
    "            input_size (int): cell input size, default = 20\n",
    "            hidden_size (int): should be 1\n",
    "            n_learner_params (int): number of learner's parameters\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_learner_params = n_learner_params\n",
    "        self.WF = nn.Parameter(torch.Tensor(input_size + 2, hidden_size))\n",
    "        self.WI = nn.Parameter(torch.Tensor(input_size + 2, hidden_size))\n",
    "        self.cI = nn.Parameter(torch.Tensor(n_learner_params, 1))\n",
    "        self.bI = nn.Parameter(torch.Tensor(1, hidden_size))\n",
    "        self.bF = nn.Parameter(torch.Tensor(1, hidden_size))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for weight in self.parameters():\n",
    "            nn.init.uniform_(weight, -0.01, 0.01)\n",
    "\n",
    "        # want initial forget value to be high and input value to be low so that \n",
    "        #  model starts with gradient descent\n",
    "        nn.init.uniform_(self.bF, 4, 6)\n",
    "        nn.init.uniform_(self.bI, -5, -4)\n",
    "\n",
    "    def init_cI(self, flat_params):\n",
    "        self.cI.data.copy_(flat_params.unsqueeze(1))\n",
    "\n",
    "    def forward(self, inputs, hx=None):\n",
    "        \"\"\"Args:\n",
    "            inputs = [x_all, grad]:\n",
    "                x_all (torch.Tensor of size [n_learner_params, input_size]): outputs from previous LSTM\n",
    "                grad (torch.Tensor of size [n_learner_params]): gradients from learner\n",
    "            hx = [f_prev, i_prev, c_prev]:\n",
    "                f (torch.Tensor of size [n_learner_params, 1]): forget gate\n",
    "                i (torch.Tensor of size [n_learner_params, 1]): input gate\n",
    "                c (torch.Tensor of size [n_learner_params, 1]): flattened learner parameters\n",
    "        \"\"\"\n",
    "        x_all, grad = inputs\n",
    "        batch, _ = x_all.size()\n",
    "\n",
    "        if hx is None:\n",
    "            f_prev = torch.zeros((batch, self.hidden_size)).to(self.WF.device)\n",
    "            i_prev = torch.zeros((batch, self.hidden_size)).to(self.WI.device)\n",
    "            c_prev = self.cI\n",
    "            hx = [f_prev, i_prev, c_prev]\n",
    "\n",
    "        f_prev, i_prev, c_prev = hx\n",
    "        \n",
    "        # f_t = sigmoid(W_f * [grad_t, loss_t, theta_{t-1}, f_{t-1}] + b_f)\n",
    "        f_next = torch.mm(torch.cat((x_all, c_prev, f_prev), 1), self.WF) + self.bF.expand_as(f_prev)\n",
    "        # i_t = sigmoid(W_i * [grad_t, loss_t, theta_{t-1}, i_{t-1}] + b_i)\n",
    "        i_next = torch.mm(torch.cat((x_all, c_prev, i_prev), 1), self.WI) + self.bI.expand_as(i_prev)\n",
    "        # next cell/params\n",
    "        c_next = torch.sigmoid(f_next).mul(c_prev) - torch.sigmoid(i_next).mul(grad)\n",
    "\n",
    "        return c_next, [f_next, i_next, c_next]\n",
    "\n",
    "    def extra_repr(self):\n",
    "        s = '{input_size}, {hidden_size}, {n_learner_params}'\n",
    "        return s.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class MetaLearner(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, n_learner_params):\n",
    "        super(MetaLearner, self).__init__()\n",
    "        \"\"\"Args:\n",
    "            input_size (int): for the first LSTM layer, default = 4\n",
    "            hidden_size (int): for the first LSTM layer, default = 20\n",
    "            n_learner_params (int): number of learner's parameters\n",
    "        \"\"\"\n",
    "        self.lstm = nn.LSTMCell(input_size=input_size, hidden_size=hidden_size)\n",
    "        self.metalstm = MetaLSTMCell(input_size=hidden_size, hidden_size=1, n_learner_params=n_learner_params)\n",
    "\n",
    "    def forward(self, inputs, hs=None):\n",
    "        \"\"\"Args:\n",
    "            inputs = [loss, grad_prep, grad]\n",
    "                loss (torch.Tensor of size [1, 2])\n",
    "                grad_prep (torch.Tensor of size [n_learner_params, 2])\n",
    "                grad (torch.Tensor of size [n_learner_params])\n",
    "            hs = [(lstm_hn, lstm_cn), [metalstm_fn, metalstm_in, metalstm_cn]]\n",
    "        \"\"\"\n",
    "        loss, grad_prep, grad = inputs\n",
    "        loss = loss.expand_as(grad_prep)\n",
    "        inputs = torch.cat((loss, grad_prep), 1)   # [n_learner_params, 4]\n",
    "\n",
    "        if hs is None:\n",
    "            hs = [None, None]\n",
    "\n",
    "        lstmhx, lstmcx = self.lstm(inputs, hs[0])\n",
    "        flat_learner_unsqzd, metalstm_hs = self.metalstm([lstmhx, grad], hs[1])\n",
    "\n",
    "        return flat_learner_unsqzd.squeeze(), [(lstmhx, lstmcx), metalstm_hs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res[0].item() if len(res) == 1 else [r.item() for r in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_grad_loss(x):\n",
    "    p = 10\n",
    "    indicator = (x.abs() >= np.exp(-p)).to(torch.float32)\n",
    "\n",
    "    # preproc1\n",
    "    x_proc1 = indicator * torch.log(x.abs() + 1e-8) / p + (1 - indicator) * -1\n",
    "    # preproc2\n",
    "    x_proc2 = indicator * torch.sign(x) + (1 - indicator) * np.exp(p) * x\n",
    "    return torch.stack((x_proc1, x_proc2), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def meta_test(eps, eval_loader, learner_w_grad, learner_wo_grad, metalearner, args):\n",
    "    for subeps, (episode_x, episode_y) in enumerate(tqdm(eval_loader, ascii=True)):\n",
    "        train_input = episode_x[:, :args['n_shot']].reshape(-1, *episode_x.shape[-3:]) # [n_class * n_shot, :]\n",
    "        train_target = torch.LongTensor(np.repeat(range(args['n_class']), args['n_shot'])) # [n_class * n_shot]\n",
    "        test_input = episode_x[:, args['n_shot']:].reshape(-1, *episode_x.shape[-3:]) # [n_class * n_eval, :]\n",
    "        test_target = torch.LongTensor(np.repeat(range(args['n_class']), args['n_eval'])) # [n_class * n_eval]\n",
    "\n",
    "        # Train learner with metalearner\n",
    "        learner_w_grad.reset_batch_stats()\n",
    "        learner_wo_grad.reset_batch_stats()\n",
    "        learner_w_grad.train()\n",
    "        learner_wo_grad.eval()\n",
    "        cI = train_learner(learner_w_grad, metalearner, train_input, train_target, args)\n",
    "\n",
    "        learner_wo_grad.transfer_params(learner_w_grad, cI)\n",
    "        output = learner_wo_grad(test_input)\n",
    "        loss = learner_wo_grad.criterion(output, test_target)\n",
    "        acc = accuracy(output, test_target)\n",
    "        print (\"Validation/Test Loss: {}, and Accuracy {}\".format(loss.item(), acc))\n",
    "        \n",
    "\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_learner(learner_w_grad, metalearner, train_input, train_target, args):\n",
    "    cI = metalearner.metalstm.cI.data\n",
    "    hs = [None]\n",
    "    for _ in range(args['epoch']):\n",
    "        for i in range(0, len(train_input), args['batch_size']):\n",
    "            x = train_input[i:i+args['batch_size']]\n",
    "            y = train_target[i:i+args['batch_size']]\n",
    "\n",
    "            # get the loss/grad\n",
    "            learner_w_grad.copy_flat_params(cI)\n",
    "            output = learner_w_grad(x)\n",
    "            loss = learner_w_grad.criterion(output, y)\n",
    "            acc = accuracy(output, y)\n",
    "            learner_w_grad.zero_grad()\n",
    "            loss.backward()\n",
    "            grad = torch.cat([p.grad.data.view(-1) / args['batch_size'] for p in learner_w_grad.parameters()], 0)\n",
    "\n",
    "            # preprocess grad & loss and metalearner forward\n",
    "            grad_prep = preprocess_grad_loss(grad)  # [n_learner_params, 2]\n",
    "            loss_prep = preprocess_grad_loss(loss.data.unsqueeze(0)) # [1, 2]\n",
    "            metalearner_input = [loss_prep, grad_prep, grad.unsqueeze(1)]\n",
    "            cI, h = metalearner(metalearner_input, hs[-1])\n",
    "            hs.append(h)\n",
    "            print(\"training loss: {:8.6f} acc: {:6.3f}, mean grad: {:8.6f}\".format(loss, acc, torch.mean(grad)))\n",
    "\n",
    "    return cI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    seed = 2019\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Get data\n",
    "    train_loader, val_loader, test_loader = prepare_data(args)\n",
    "    \n",
    "    # Set up learner, meta-learner\n",
    "    learner_w_grad = Learner(args['image_size'], args['bn_eps'], args['bn_momentum'], args['n_class'])\n",
    "    learner_wo_grad = copy.deepcopy(learner_w_grad)\n",
    "    metalearner = MetaLearner(args['input_size'], args['hidden_size'], learner_w_grad.get_flat_params().size(0))\n",
    "    metalearner.metalstm.init_cI(learner_w_grad.get_flat_params())\n",
    "\n",
    "    # Set up loss, optimizer, learning rate scheduler\n",
    "    optim = torch.optim.Adam(metalearner.parameters(), args['lr'])\n",
    "\n",
    "    if args['mode'] == 'test':\n",
    "        _ = meta_test(args['episode'], test_loader, learner_w_grad, learner_wo_grad, metalearner, args)\n",
    "        return\n",
    "    best_acc = 0.0\n",
    "    # Meta-training\n",
    "    for eps, (episode_x, episode_y) in enumerate(train_loader):\n",
    "        # episode_x.shape = [n_class, n_shot + n_eval, c, h, w]\n",
    "        # episode_y.shape = [n_class, n_shot + n_eval] --> NEVER USED\n",
    "        train_input = episode_x[:, :args['n_shot']].reshape(-1, *episode_x.shape[-3:]) # [n_class * n_shot, :]\n",
    "        train_target = torch.LongTensor(np.repeat(range(args['n_shot']), args['n_shot'])) # [n_class * n_shot]\n",
    "        test_input = episode_x[:, args['n_shot']:].reshape(-1, *episode_x.shape[-3:]) # [n_class * n_eval, :]\n",
    "        test_target = torch.LongTensor(np.repeat(range(args['n_shot']), args['n_eval'])) # [n_class * n_eval]\n",
    "\n",
    "        # Train learner with metalearner\n",
    "        learner_w_grad.reset_batch_stats()\n",
    "        learner_wo_grad.reset_batch_stats()\n",
    "        learner_w_grad.train()\n",
    "        learner_wo_grad.train()\n",
    "        cI = train_learner(learner_w_grad, metalearner, train_input, train_target, args)\n",
    "\n",
    "        # Train meta-learner with validation loss\n",
    "        learner_wo_grad.transfer_params(learner_w_grad, cI)\n",
    "        output = learner_wo_grad(test_input)\n",
    "        loss = learner_wo_grad.criterion(output, test_target)\n",
    "        acc = accuracy(output, test_target)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(metalearner.parameters(), args['grad_clip'])\n",
    "        optim.step()\n",
    "        \n",
    "        print (eps)\n",
    "        # Meta-validation\n",
    "        if eps % 10 == 0 and eps != 0:\n",
    "            acc = meta_test(eps, val_loader, learner_w_grad, learner_wo_grad, metalearner, args)\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                print (\"* Best accuracy so far *\\n\")\n",
    "\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " BEGIN TRAINING: \n",
      "training loss: 1.614745 acc: 31.250, mean grad: -0.000029\n",
      "training loss: 2.118628 acc:  0.000, mean grad: 0.000112\n",
      "training loss: 1.540848 acc: 37.500, mean grad: -0.000018\n",
      "training loss: 1.856331 acc: 11.111, mean grad: 0.000140\n",
      "0\n",
      "training loss: 1.668032 acc: 18.750, mean grad: -0.000022\n",
      "training loss: 1.827717 acc: 22.222, mean grad: -0.000048\n",
      "training loss: 1.620747 acc: 18.750, mean grad: -0.000035\n",
      "training loss: 1.624162 acc: 55.556, mean grad: -0.000062\n",
      "1\n",
      "training loss: 1.648508 acc: 12.500, mean grad: -0.000081\n",
      "training loss: 2.014055 acc:  0.000, mean grad: -0.000178\n",
      "training loss: 1.610496 acc: 18.750, mean grad: -0.000079\n",
      "training loss: 1.832938 acc: 11.111, mean grad: -0.000149\n",
      "2\n",
      "training loss: 1.715487 acc: 12.500, mean grad: -0.000034\n",
      "training loss: 1.952520 acc: 11.111, mean grad: -0.000026\n",
      "training loss: 1.683787 acc: 12.500, mean grad: -0.000045\n",
      "training loss: 1.769854 acc: 11.111, mean grad: -0.000026\n",
      "3\n",
      "training loss: 1.756584 acc: 18.750, mean grad: 0.000074\n",
      "training loss: 1.877935 acc:  0.000, mean grad: -0.000186\n",
      "training loss: 1.717086 acc: 18.750, mean grad: 0.000069\n",
      "training loss: 1.708358 acc:  0.000, mean grad: -0.000195\n",
      "4\n",
      "training loss: 1.662429 acc: 31.250, mean grad: 0.000170\n",
      "training loss: 1.981928 acc:  0.000, mean grad: -0.000058\n",
      "training loss: 1.624688 acc: 25.000, mean grad: 0.000138\n",
      "training loss: 1.764899 acc: 11.111, mean grad: -0.000026\n",
      "5\n",
      "training loss: 1.557144 acc: 37.500, mean grad: -0.000038\n",
      "training loss: 2.081914 acc:  0.000, mean grad: 0.000007\n",
      "training loss: 1.555045 acc: 37.500, mean grad: -0.000046\n",
      "training loss: 1.877015 acc: 11.111, mean grad: 0.000013\n",
      "6\n",
      "training loss: 1.554621 acc: 43.750, mean grad: 0.000023\n",
      "training loss: 1.685004 acc: 11.111, mean grad: -0.000054\n",
      "training loss: 1.542237 acc: 50.000, mean grad: 0.000017\n",
      "training loss: 1.559199 acc: 22.222, mean grad: -0.000046\n",
      "7\n",
      "training loss: 1.671750 acc: 25.000, mean grad: -0.000088\n",
      "training loss: 1.673193 acc: 11.111, mean grad: -0.000138\n",
      "training loss: 1.669388 acc: 25.000, mean grad: -0.000089\n",
      "training loss: 1.540256 acc: 11.111, mean grad: -0.000133\n",
      "8\n",
      "training loss: 1.484343 acc: 37.500, mean grad: 0.000041\n",
      "training loss: 1.727702 acc: 22.222, mean grad: -0.000117\n",
      "training loss: 1.476074 acc: 31.250, mean grad: 0.000040\n",
      "training loss: 1.601556 acc: 33.333, mean grad: -0.000111\n",
      "9\n",
      "training loss: 1.671319 acc: 25.000, mean grad: 0.000019\n",
      "training loss: 1.813266 acc: 33.333, mean grad: -0.000073\n",
      "training loss: 1.649809 acc: 25.000, mean grad: 0.000009\n",
      "training loss: 1.650386 acc: 33.333, mean grad: -0.000091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "training loss: 1.506445 acc: 31.250, mean grad: 0.000080\n",
      "training loss: 1.924212 acc: 33.333, mean grad: -0.000004\n",
      "training loss: 1.506989 acc: 25.000, mean grad: 0.000075\n",
      "training loss: 1.708387 acc: 33.333, mean grad: -0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|##        | 1/5 [00:02<00:09,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation/Test Loss: 1.593719720840454, and Accuracy 21.33333396911621\n",
      "training loss: 1.500121 acc: 37.500, mean grad: 0.000032\n",
      "training loss: 1.905619 acc: 11.111, mean grad: 0.000107\n",
      "training loss: 1.510576 acc: 43.750, mean grad: 0.000034\n",
      "training loss: 1.721232 acc: 33.333, mean grad: 0.000089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|####      | 2/5 [00:03<00:06,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation/Test Loss: 1.4927806854248047, and Accuracy 33.333335876464844\n",
      "training loss: 1.534272 acc: 31.250, mean grad: 0.000059\n",
      "training loss: 1.644273 acc: 33.333, mean grad: -0.000053\n",
      "training loss: 1.533480 acc: 31.250, mean grad: 0.000069\n",
      "training loss: 1.506519 acc: 33.333, mean grad: -0.000056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|######    | 3/5 [00:05<00:03,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation/Test Loss: 1.6011277437210083, and Accuracy 22.666667938232422\n",
      "training loss: 1.741906 acc: 18.750, mean grad: 0.000026\n",
      "training loss: 1.586810 acc: 11.111, mean grad: -0.000130\n",
      "training loss: 1.722555 acc: 12.500, mean grad: 0.000022\n",
      "training loss: 1.462085 acc: 44.444, mean grad: -0.000112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|########  | 4/5 [00:06<00:01,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation/Test Loss: 1.732759952545166, and Accuracy 17.33333396911621\n",
      "training loss: 1.693859 acc: 31.250, mean grad: -0.000059\n",
      "training loss: 1.936478 acc: 22.222, mean grad: 0.000046\n",
      "training loss: 1.684606 acc: 31.250, mean grad: -0.000056\n",
      "training loss: 1.776152 acc: 33.333, mean grad: 0.000030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 5/5 [00:08<00:00,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation/Test Loss: 1.7159435749053955, and Accuracy 17.33333396911621\n",
      "* Best accuracy so far *\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.700276 acc: 31.250, mean grad: 0.000071\n",
      "training loss: 2.000686 acc:  0.000, mean grad: 0.000026\n",
      "training loss: 1.675115 acc: 25.000, mean grad: 0.000052\n",
      "training loss: 1.837954 acc:  0.000, mean grad: 0.000042\n",
      "11\n",
      "training loss: 1.669216 acc: 37.500, mean grad: 0.000142\n",
      "training loss: 1.709039 acc: 11.111, mean grad: -0.000022\n",
      "training loss: 1.624813 acc: 37.500, mean grad: 0.000142\n",
      "training loss: 1.569145 acc: 22.222, mean grad: -0.000020\n",
      "12\n",
      "training loss: 1.657539 acc: 31.250, mean grad: -0.000030\n",
      "training loss: 1.824310 acc: 11.111, mean grad: -0.000055\n",
      "training loss: 1.626104 acc: 31.250, mean grad: -0.000019\n",
      "training loss: 1.705141 acc: 11.111, mean grad: -0.000063\n",
      "13\n",
      "training loss: 1.409093 acc: 31.250, mean grad: 0.000014\n",
      "training loss: 1.824450 acc:  0.000, mean grad: -0.000046\n",
      "training loss: 1.419711 acc: 37.500, mean grad: 0.000012\n",
      "training loss: 1.668188 acc: 11.111, mean grad: -0.000059\n",
      "14\n",
      "training loss: 1.565368 acc: 31.250, mean grad: -0.000007\n",
      "training loss: 1.690449 acc: 11.111, mean grad: 0.000006\n",
      "training loss: 1.556798 acc: 31.250, mean grad: -0.000016\n",
      "training loss: 1.594071 acc: 11.111, mean grad: -0.000000\n",
      "15\n",
      "training loss: 1.747355 acc: 12.500, mean grad: -0.000062\n",
      "training loss: 1.742710 acc: 11.111, mean grad: 0.000005\n",
      "training loss: 1.741954 acc: 12.500, mean grad: -0.000058\n",
      "training loss: 1.635666 acc: 22.222, mean grad: 0.000003\n",
      "16\n",
      "training loss: 1.637440 acc: 31.250, mean grad: 0.000057\n",
      "training loss: 1.828171 acc:  0.000, mean grad: -0.000008\n",
      "training loss: 1.628937 acc: 31.250, mean grad: 0.000050\n",
      "training loss: 1.703030 acc:  0.000, mean grad: -0.000002\n",
      "17\n",
      "training loss: 1.556815 acc: 25.000, mean grad: -0.000036\n",
      "training loss: 1.857153 acc: 11.111, mean grad: -0.000153\n",
      "training loss: 1.556025 acc: 31.250, mean grad: -0.000028\n",
      "training loss: 1.701364 acc: 11.111, mean grad: -0.000137\n",
      "18\n",
      "training loss: 1.613130 acc: 37.500, mean grad: -0.000024\n",
      "training loss: 1.961981 acc:  0.000, mean grad: -0.000026\n",
      "training loss: 1.603939 acc: 37.500, mean grad: -0.000034\n",
      "training loss: 1.836855 acc: 11.111, mean grad: -0.000012\n",
      "19\n",
      "training loss: 1.455443 acc: 43.750, mean grad: -0.000019\n",
      "training loss: 1.857153 acc:  0.000, mean grad: 0.000044\n",
      "training loss: 1.465267 acc: 43.750, mean grad: -0.000017\n",
      "training loss: 1.718187 acc: 11.111, mean grad: 0.000060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "training loss: 1.665912 acc:  6.250, mean grad: 0.000017\n",
      "training loss: 1.890723 acc:  0.000, mean grad: 0.000004\n",
      "training loss: 1.665951 acc:  6.250, mean grad: 0.000016\n",
      "training loss: 1.742665 acc: 11.111, mean grad: 0.000013"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|##        | 1/5 [00:02<00:08,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation/Test Loss: 1.6213276386260986, and Accuracy 25.33333396911621\n",
      "training loss: 1.635617 acc: 12.500, mean grad: 0.000032\n",
      "training loss: 2.005629 acc:  0.000, mean grad: -0.000053\n",
      "training loss: 1.624176 acc: 18.750, mean grad: 0.000035\n",
      "training loss: 1.841934 acc: 11.111, mean grad: -0.000035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|####      | 2/5 [00:03<00:05,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation/Test Loss: 1.6874781847000122, and Accuracy 13.333333969116211\n",
      "training loss: 1.529977 acc: 31.250, mean grad: 0.000043\n",
      "training loss: 1.874977 acc: 11.111, mean grad: 0.000018\n",
      "training loss: 1.538827 acc: 25.000, mean grad: 0.000039\n",
      "training loss: 1.724627 acc: 11.111, mean grad: 0.000017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|######    | 3/5 [00:04<00:03,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation/Test Loss: 1.6405235528945923, and Accuracy 18.666667938232422\n",
      "training loss: 1.724140 acc: 37.500, mean grad: -0.000021\n",
      "training loss: 1.898619 acc: 11.111, mean grad: -0.000044\n",
      "training loss: 1.706659 acc: 37.500, mean grad: -0.000018\n",
      "training loss: 1.768893 acc: 22.222, mean grad: -0.000053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|########  | 4/5 [00:06<00:01,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation/Test Loss: 1.68801748752594, and Accuracy 20.0\n",
      "training loss: 1.571136 acc: 18.750, mean grad: 0.000060\n",
      "training loss: 1.941251 acc:  0.000, mean grad: -0.000142\n",
      "training loss: 1.564841 acc: 37.500, mean grad: 0.000065\n",
      "training loss: 1.789155 acc:  0.000, mean grad: -0.000127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 5/5 [00:07<00:00,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation/Test Loss: 1.6470879316329956, and Accuracy 20.0\n",
      "* Best accuracy so far *\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.486757 acc: 31.250, mean grad: -0.000055\n",
      "training loss: 2.025447 acc:  0.000, mean grad: -0.000005\n",
      "training loss: 1.480276 acc: 37.500, mean grad: -0.000052\n",
      "training loss: 1.819491 acc:  0.000, mean grad: -0.000005\n",
      "21\n",
      "training loss: 1.632090 acc: 12.500, mean grad: 0.000004\n",
      "training loss: 1.925757 acc:  0.000, mean grad: -0.000057\n",
      "training loss: 1.629442 acc: 12.500, mean grad: 0.000001\n",
      "training loss: 1.795984 acc:  0.000, mean grad: -0.000049\n",
      "22\n",
      "training loss: 1.429525 acc: 37.500, mean grad: -0.000008\n",
      "training loss: 1.979416 acc:  0.000, mean grad: 0.000035\n",
      "training loss: 1.445377 acc: 50.000, mean grad: -0.000014\n",
      "training loss: 1.837744 acc: 11.111, mean grad: 0.000036\n",
      "23\n",
      "training loss: 1.601067 acc: 18.750, mean grad: -0.000043\n",
      "training loss: 1.831364 acc: 11.111, mean grad: -0.000025\n",
      "training loss: 1.603745 acc: 18.750, mean grad: -0.000039\n",
      "training loss: 1.715759 acc: 22.222, mean grad: -0.000024\n",
      "24\n",
      "training loss: 1.410628 acc: 31.250, mean grad: 0.000090\n",
      "training loss: 1.887584 acc:  0.000, mean grad: -0.000024\n",
      "training loss: 1.429475 acc: 31.250, mean grad: 0.000079\n",
      "training loss: 1.752368 acc: 11.111, mean grad: -0.000013\n",
      "25\n",
      "training loss: 1.490330 acc: 31.250, mean grad: 0.000005\n",
      "training loss: 2.066180 acc: 11.111, mean grad: -0.000031\n",
      "training loss: 1.496153 acc: 37.500, mean grad: 0.000003\n",
      "training loss: 1.942186 acc: 11.111, mean grad: -0.000020\n",
      "26\n",
      "training loss: 1.588477 acc: 25.000, mean grad: -0.000039\n",
      "training loss: 1.873728 acc: 22.222, mean grad: -0.000041\n",
      "training loss: 1.595765 acc: 25.000, mean grad: -0.000046\n",
      "training loss: 1.749551 acc: 22.222, mean grad: -0.000030\n",
      "27\n",
      "training loss: 1.613093 acc: 12.500, mean grad: 0.000059\n",
      "training loss: 1.927000 acc:  0.000, mean grad: 0.000020\n",
      "training loss: 1.596115 acc: 12.500, mean grad: 0.000052\n",
      "training loss: 1.819487 acc:  0.000, mean grad: 0.000005\n",
      "28\n",
      "training loss: 1.606547 acc: 12.500, mean grad: -0.000038\n",
      "training loss: 1.759517 acc: 22.222, mean grad: -0.000077\n",
      "training loss: 1.602934 acc: 12.500, mean grad: -0.000035\n",
      "training loss: 1.657761 acc: 22.222, mean grad: -0.000071\n",
      "29\n",
      "training loss: 1.736246 acc: 18.750, mean grad: -0.000005\n",
      "training loss: 2.010389 acc: 11.111, mean grad: 0.000048\n",
      "training loss: 1.733649 acc: 18.750, mean grad: -0.000011\n",
      "training loss: 1.888247 acc: 22.222, mean grad: 0.000045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "training loss: 1.643109 acc:  6.250, mean grad: -0.000026\n",
      "training loss: 1.683440 acc: 33.333, mean grad: -0.000016\n",
      "training loss: 1.646062 acc:  6.250, mean grad: -0.000028\n",
      "training loss: 1.568876 acc: 55.556, mean grad: -0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|##        | 1/5 [00:02<00:09,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation/Test Loss: 1.6116549968719482, and Accuracy 25.33333396911621\n",
      "training loss: 1.425703 acc: 37.500, mean grad: -0.000014\n",
      "training loss: 1.692899 acc: 33.333, mean grad: 0.000003\n",
      "training loss: 1.445959 acc: 37.500, mean grad: -0.000012\n",
      "training loss: 1.577181 acc: 33.333, mean grad: 0.000005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|####      | 2/5 [00:03<00:06,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation/Test Loss: 1.6073495149612427, and Accuracy 26.666667938232422\n",
      "training loss: 1.634763 acc: 31.250, mean grad: 0.000029\n",
      "training loss: 1.891266 acc: 11.111, mean grad: -0.000019\n",
      "training loss: 1.651148 acc: 31.250, mean grad: 0.000027\n",
      "training loss: 1.752326 acc: 22.222, mean grad: -0.000016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|######    | 3/5 [00:05<00:03,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation/Test Loss: 1.6562232971191406, and Accuracy 25.33333396911621\n",
      "training loss: 1.692269 acc:  6.250, mean grad: -0.000006\n",
      "training loss: 1.890070 acc: 11.111, mean grad: -0.000047\n",
      "training loss: 1.695880 acc:  6.250, mean grad: -0.000008\n",
      "training loss: 1.766104 acc: 11.111, mean grad: -0.000038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|########  | 4/5 [00:06<00:01,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation/Test Loss: 1.7049757242202759, and Accuracy 16.0\n",
      "training loss: 1.670734 acc: 18.750, mean grad: -0.000013\n",
      "training loss: 1.690246 acc: 22.222, mean grad: -0.000075\n",
      "training loss: 1.677693 acc: 25.000, mean grad: -0.000018\n",
      "training loss: 1.550957 acc: 22.222, mean grad: -0.000073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 5/5 [00:07<00:00,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation/Test Loss: 1.6256543397903442, and Accuracy 22.666667938232422\n",
      "* Best accuracy so far *\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.684431 acc: 12.500, mean grad: -0.000029\n",
      "training loss: 1.780886 acc:  0.000, mean grad: 0.000048\n",
      "training loss: 1.686307 acc: 12.500, mean grad: -0.000031\n",
      "training loss: 1.642941 acc: 44.444, mean grad: 0.000037\n",
      "31\n",
      "training loss: 1.647676 acc:  6.250, mean grad: 0.000030\n",
      "training loss: 1.690422 acc: 22.222, mean grad: -0.000002\n",
      "training loss: 1.652482 acc:  6.250, mean grad: 0.000025\n",
      "training loss: 1.598753 acc: 22.222, mean grad: -0.000001\n",
      "32\n",
      "training loss: 1.526454 acc: 31.250, mean grad: 0.000006\n",
      "training loss: 1.912634 acc: 11.111, mean grad: -0.000096\n",
      "training loss: 1.536687 acc: 25.000, mean grad: 0.000007\n",
      "training loss: 1.787290 acc: 22.222, mean grad: -0.000084\n",
      "33\n",
      "training loss: 1.578569 acc: 31.250, mean grad: -0.000013\n",
      "training loss: 1.786916 acc: 11.111, mean grad: -0.000004\n",
      "training loss: 1.580722 acc: 31.250, mean grad: -0.000020\n",
      "training loss: 1.685310 acc: 11.111, mean grad: -0.000012\n",
      "34\n",
      "training loss: 1.746173 acc: 12.500, mean grad: 0.000001\n",
      "training loss: 1.917406 acc: 11.111, mean grad: -0.000030\n",
      "training loss: 1.748996 acc: 12.500, mean grad: -0.000005\n",
      "training loss: 1.792732 acc: 22.222, mean grad: -0.000030\n",
      "35\n",
      "training loss: 1.577528 acc: 12.500, mean grad: 0.000053\n",
      "training loss: 2.156219 acc:  0.000, mean grad: -0.000021\n",
      "training loss: 1.586622 acc: 12.500, mean grad: 0.000039\n",
      "training loss: 1.976469 acc:  0.000, mean grad: -0.000031\n",
      "36\n",
      "training loss: 1.585373 acc: 31.250, mean grad: -0.000031\n",
      "training loss: 1.904530 acc:  0.000, mean grad: 0.000010\n",
      "training loss: 1.594826 acc: 31.250, mean grad: -0.000030\n",
      "training loss: 1.799297 acc: 11.111, mean grad: 0.000009\n",
      "37\n",
      "training loss: 1.704321 acc: 18.750, mean grad: 0.000098\n",
      "training loss: 1.946506 acc:  0.000, mean grad: 0.000015\n",
      "training loss: 1.702794 acc: 18.750, mean grad: 0.000087\n",
      "training loss: 1.808186 acc: 11.111, mean grad: 0.000003\n",
      "38\n",
      "training loss: 1.443004 acc: 50.000, mean grad: -0.000042\n",
      "training loss: 1.794061 acc:  0.000, mean grad: -0.000008\n",
      "training loss: 1.457778 acc: 43.750, mean grad: -0.000039\n",
      "training loss: 1.711974 acc:  0.000, mean grad: -0.000007\n",
      "39\n",
      "training loss: 1.586775 acc: 37.500, mean grad: 0.000028\n",
      "training loss: 1.768315 acc: 11.111, mean grad: -0.000024\n",
      "training loss: 1.583532 acc: 25.000, mean grad: 0.000029\n",
      "training loss: 1.667805 acc: 11.111, mean grad: -0.000022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "training loss: 1.494308 acc: 50.000, mean grad: 0.000016\n",
      "training loss: 1.987978 acc: 11.111, mean grad: -0.000043\n",
      "training loss: 1.503294 acc: 50.000, mean grad: 0.000013\n",
      "training loss: 1.857346 acc: 11.111, mean grad: -0.000041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|##        | 1/5 [00:02<00:09,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation/Test Loss: 1.6918948888778687, and Accuracy 18.666667938232422\n",
      "training loss: 1.656846 acc: 18.750, mean grad: -0.000006\n",
      "training loss: 1.927852 acc:  0.000, mean grad: 0.000033\n",
      "training loss: 1.664261 acc: 18.750, mean grad: -0.000007\n",
      "training loss: 1.794069 acc:  0.000, mean grad: 0.000032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|####      | 2/5 [00:03<00:06,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation/Test Loss: 1.7808585166931152, and Accuracy 14.666666984558105\n",
      "training loss: 1.549928 acc: 37.500, mean grad: 0.000022\n",
      "training loss: 1.906957 acc:  0.000, mean grad: 0.000022\n",
      "training loss: 1.564751 acc: 37.500, mean grad: 0.000021\n",
      "training loss: 1.757457 acc: 11.111, mean grad: 0.000023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|######    | 3/5 [00:05<00:03,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation/Test Loss: 1.668221116065979, and Accuracy 26.666667938232422\n",
      "training loss: 1.504888 acc: 43.750, mean grad: -0.000049\n",
      "training loss: 1.702780 acc:  0.000, mean grad: 0.000023\n",
      "training loss: 1.531997 acc: 31.250, mean grad: -0.000047\n",
      "training loss: 1.577613 acc: 11.111, mean grad: 0.000025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|########  | 4/5 [00:06<00:01,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation/Test Loss: 1.5447510480880737, and Accuracy 29.33333396911621\n",
      "training loss: 1.689559 acc: 18.750, mean grad: 0.000003\n",
      "training loss: 1.956663 acc:  0.000, mean grad: 0.000041\n",
      "training loss: 1.694549 acc: 18.750, mean grad: -0.000001\n",
      "training loss: 1.821139 acc:  0.000, mean grad: 0.000038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 5/5 [00:08<00:00,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation/Test Loss: 1.5948500633239746, and Accuracy 28.0\n",
      "* Best accuracy so far *\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.583406 acc: 18.750, mean grad: 0.000002\n",
      "training loss: 1.925111 acc: 11.111, mean grad: -0.000011\n",
      "training loss: 1.594309 acc: 18.750, mean grad: -0.000001\n",
      "training loss: 1.794685 acc: 11.111, mean grad: -0.000017\n",
      "41\n",
      "training loss: 1.561712 acc: 31.250, mean grad: -0.000009\n",
      "training loss: 1.740330 acc:  0.000, mean grad: -0.000006\n",
      "training loss: 1.578705 acc: 31.250, mean grad: -0.000010\n",
      "training loss: 1.648667 acc:  0.000, mean grad: -0.000014\n",
      "42\n",
      "training loss: 1.810461 acc: 18.750, mean grad: -0.000084\n",
      "training loss: 1.781053 acc:  0.000, mean grad: -0.000021\n",
      "training loss: 1.804751 acc: 18.750, mean grad: -0.000083\n",
      "training loss: 1.698363 acc:  0.000, mean grad: -0.000014\n",
      "43\n",
      "training loss: 1.562874 acc: 18.750, mean grad: -0.000004\n",
      "training loss: 1.784615 acc: 11.111, mean grad: 0.000062\n",
      "training loss: 1.580992 acc: 25.000, mean grad: 0.000003\n",
      "training loss: 1.674384 acc: 11.111, mean grad: 0.000063\n",
      "44\n",
      "training loss: 1.350529 acc: 56.250, mean grad: -0.000018\n",
      "training loss: 1.870974 acc:  0.000, mean grad: 0.000047\n",
      "training loss: 1.369341 acc: 50.000, mean grad: -0.000020\n",
      "training loss: 1.753921 acc:  0.000, mean grad: 0.000058\n",
      "45\n",
      "training loss: 1.784240 acc: 18.750, mean grad: 0.000035\n",
      "training loss: 1.802524 acc:  0.000, mean grad: 0.000006\n",
      "training loss: 1.783004 acc: 18.750, mean grad: 0.000031\n",
      "training loss: 1.685884 acc:  0.000, mean grad: 0.000001\n",
      "46\n",
      "training loss: 1.730170 acc: 18.750, mean grad: -0.000010\n",
      "training loss: 1.933256 acc:  0.000, mean grad: -0.000004\n",
      "training loss: 1.734069 acc: 12.500, mean grad: -0.000008\n",
      "training loss: 1.820089 acc:  0.000, mean grad: -0.000001\n",
      "47\n",
      "training loss: 1.693069 acc: 25.000, mean grad: 0.000029\n",
      "training loss: 1.722448 acc: 11.111, mean grad: -0.000016\n",
      "training loss: 1.690328 acc: 18.750, mean grad: 0.000022\n",
      "training loss: 1.637949 acc: 11.111, mean grad: -0.000024\n",
      "48\n",
      "training loss: 1.639360 acc: 25.000, mean grad: -0.000001\n",
      "training loss: 1.888103 acc:  0.000, mean grad: -0.000017\n",
      "training loss: 1.646910 acc: 18.750, mean grad: -0.000005\n",
      "training loss: 1.762044 acc: 22.222, mean grad: -0.000014\n",
      "49\n",
      "Done\n",
      "BEGIN TESTING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.740956 acc: 18.750, mean grad: 0.000093\n",
      "training loss: 1.960083 acc: 11.111, mean grad: -0.000050\n",
      "training loss: 1.638315 acc: 25.000, mean grad: 0.000081\n",
      "training loss: 1.742578 acc: 11.111, mean grad: -0.000011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1/1 [00:02<00:00,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation/Test Loss: 1.7285420894622803, and Accuracy 10.666666984558105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    args_train={'mode':'train','n_shot':5,'n_eval':15,'n_class':5,'input_size':4,'hidden_size':20,'lr':1e-3,'episode':50,\n",
    "      'episode_val':50,'epoch':8,'batch_size':16,'image_size':84,'grad_clip':0.25,'bn_momentum': 0.95,'bn_eps': 1e-3,\n",
    "       'data': \"miniimagenet\",'data_root': \"./miniImagenet/\", 'resume': None}\n",
    "    \n",
    "    \n",
    "    args_test={'mode':'test','n_shot':5,'n_eval':15,'n_class':5,'input_size':4,'hidden_size':20,'lr':1e-3,'episode':50,\n",
    "      'episode_val':10,'epoch':8,'batch_size':16,'image_size':84,'grad_clip':0.25,'bn_momentum': 0.95,'bn_eps': 1e-3,\n",
    "       'data': \"miniimagenet\",'data_root': \"./miniImagenet/\", 'resume': None}\n",
    "    \n",
    "    \n",
    "    print (\" BEGIN TRAINING: \")\n",
    "    main(args_train)\n",
    "    \n",
    "    \n",
    "    print (\"BEGIN TESTING\")\n",
    "    main(args_test)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
