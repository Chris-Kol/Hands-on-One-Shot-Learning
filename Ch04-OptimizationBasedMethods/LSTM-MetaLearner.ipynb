{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIP: inspired from https://github.com/markdtw/meta-learning-lstm-pytorch\n",
    "# modifiying it to make it simpler\n",
    "#import libraries\n",
    "\n",
    "from __future__ import division, print_function, absolute_import\n",
    "import os\n",
    "import re\n",
    "import pdb\n",
    "import copy\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import PIL.Image as PILI\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Data Loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpisodeDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, root, phase='train', n_shot=5, n_eval=15, transform=None):\n",
    "        \"\"\"Args:\n",
    "            root (str): path to data\n",
    "            phase (str): train, val or test\n",
    "            n_shot (int): how many examples per class for training (k/n_support)\n",
    "            n_eval (int): how many examples per class for evaluation\n",
    "                - n_shot + n_eval = batch_size for data.DataLoader of ClassDataset\n",
    "            transform (torchvision.transforms): data augmentation\n",
    "        \"\"\"\n",
    "        root = os.path.join(root, phase)\n",
    "        self.labels = sorted(os.listdir(root))\n",
    "        images = [glob.glob(os.path.join(root, label, '*')) for label in self.labels]\n",
    "\n",
    "        self.episode_loader = [data.DataLoader(\n",
    "            ClassDataset(images=images[idx], label=idx, transform=transform),\n",
    "            batch_size=n_shot+n_eval, shuffle=True, num_workers=0) for idx, _ in enumerate(self.labels)]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return next(iter(self.episode_loader[idx]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, images, label, transform=None):\n",
    "        \"\"\"Args:\n",
    "            images (list of str): each item is a path to an image of the same label\n",
    "            label (int): the label of all the images\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.label = label\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = PILI.open(self.images[idx]).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, self.label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpisodicSampler(data.Sampler):\n",
    "\n",
    "    def __init__(self, total_classes, n_class, n_episode):\n",
    "        self.total_classes = total_classes\n",
    "        self.n_class = n_class\n",
    "        self.n_episode = n_episode\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(self.n_episode):\n",
    "            yield torch.randperm(self.total_classes)[:self.n_class]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(args):\n",
    "\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "    train_set = EpisodeDataset(args['data_root'], 'train', args['n_shot'], args['n_eval'],\n",
    "        transform=transforms.Compose([\n",
    "            transforms.RandomResizedCrop(args['image_size']),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(\n",
    "                brightness=0.4,\n",
    "                contrast=0.4,\n",
    "                saturation=0.4,\n",
    "                hue=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            normalize]))\n",
    "\n",
    "    val_set = EpisodeDataset(args['data_root'], 'val', args['n_shot'], args['n_eval'],\n",
    "        transform=transforms.Compose([\n",
    "            transforms.Resize(args['image_size'] * 8 // 7),\n",
    "            transforms.CenterCrop(args['image_size']),\n",
    "            transforms.ToTensor(),\n",
    "            normalize]))\n",
    "\n",
    "    test_set = EpisodeDataset(args['data_root'], 'test', args['n_shot'], args['n_eval'],\n",
    "        transform=transforms.Compose([\n",
    "            transforms.Resize(args['image_size'] * 8 // 7),\n",
    "            transforms.CenterCrop(args['image_size']),\n",
    "            transforms.ToTensor(),\n",
    "            normalize]))\n",
    "\n",
    "    train_loader = data.DataLoader(train_set, num_workers=4, pin_memory=True,\n",
    "        batch_sampler=EpisodicSampler(len(train_set), args['n_class'], args['episode']))\n",
    "\n",
    "    val_loader = data.DataLoader(val_set, num_workers=2, pin_memory=True,\n",
    "        batch_sampler=EpisodicSampler(len(val_set), args['n_class'], args['episode_val']))\n",
    "\n",
    "    test_loader = data.DataLoader(test_set, num_workers=2, pin_memory=True,\n",
    "        batch_sampler=EpisodicSampler(len(test_set), args['n_class'], args['episode_val']))\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Learner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner(nn.Module):\n",
    "\n",
    "    def __init__(self, image_size, bn_eps, bn_momentum, n_classes):\n",
    "        super(Learner, self).__init__()\n",
    "        self.model = nn.ModuleDict({'features': nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(3, 32, 3, padding=1)),\n",
    "            ('norm1', nn.BatchNorm2d(32, bn_eps, bn_momentum)),\n",
    "            ('relu1', nn.ReLU(inplace=False)),\n",
    "            ('pool1', nn.MaxPool2d(2)),\n",
    "\n",
    "            ('conv2', nn.Conv2d(32, 32, 3, padding=1)),\n",
    "            ('norm2', nn.BatchNorm2d(32, bn_eps, bn_momentum)),\n",
    "            ('relu2', nn.ReLU(inplace=False)),\n",
    "            ('pool2', nn.MaxPool2d(2)),\n",
    "\n",
    "            ('conv3', nn.Conv2d(32, 32, 3, padding=1)),\n",
    "            ('norm3', nn.BatchNorm2d(32, bn_eps, bn_momentum)),\n",
    "            ('relu3', nn.ReLU(inplace=False)),\n",
    "            ('pool3', nn.MaxPool2d(2)),\n",
    "\n",
    "            ('conv4', nn.Conv2d(32, 32, 3, padding=1)),\n",
    "            ('norm4', nn.BatchNorm2d(32, bn_eps, bn_momentum)),\n",
    "            ('relu4', nn.ReLU(inplace=False)),\n",
    "            ('pool4', nn.MaxPool2d(2))]))\n",
    "        })\n",
    "\n",
    "        clr_in = image_size // 2**4\n",
    "        self.model.update({'cls': nn.Linear(32 * clr_in * clr_in, n_classes)})\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model.features(x)\n",
    "        x = torch.reshape(x, [x.size(0), -1])\n",
    "        outputs = self.model.cls(x)\n",
    "        return outputs\n",
    "\n",
    "    def get_flat_params(self):\n",
    "        return torch.cat([p.view(-1) for p in self.model.parameters()], 0)\n",
    "\n",
    "    def copy_flat_params(self, cI):\n",
    "        idx = 0\n",
    "        for p in self.model.parameters():\n",
    "            plen = p.view(-1).size(0)\n",
    "            p.data.copy_(cI[idx: idx+plen].view_as(p))\n",
    "            idx += plen\n",
    "\n",
    "    def transfer_params(self, learner_w_grad, cI):\n",
    "        # Use load_state_dict only to copy the running mean/var in batchnorm, the values of the parameters\n",
    "        #  are going to be replaced by cI\n",
    "        self.load_state_dict(learner_w_grad.state_dict())\n",
    "        #  replace nn.Parameters with tensors from cI (NOT nn.Parameters anymore).\n",
    "        idx = 0\n",
    "        for m in self.model.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.Linear):\n",
    "                wlen = m._parameters['weight'].view(-1).size(0)\n",
    "                m._parameters['weight'] = cI[idx: idx+wlen].view_as(m._parameters['weight']).clone()\n",
    "                idx += wlen\n",
    "                if m._parameters['bias'] is not None:\n",
    "                    blen = m._parameters['bias'].view(-1).size(0)\n",
    "                    m._parameters['bias'] = cI[idx: idx+blen].view_as(m._parameters['bias']).clone()\n",
    "                    idx += blen\n",
    "\n",
    "    def reset_batch_stats(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                m.reset_running_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Meta Learner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaLSTMCell(nn.Module):\n",
    "    \"\"\"C_t = f_t * C_{t-1} + i_t * \\tilde{C_t}\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, n_learner_params):\n",
    "        super(MetaLSTMCell, self).__init__()\n",
    "        \"\"\"Args:\n",
    "            input_size (int): cell input size, default = 20\n",
    "            hidden_size (int): should be 1\n",
    "            n_learner_params (int): number of learner's parameters\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_learner_params = n_learner_params\n",
    "        self.WF = nn.Parameter(torch.Tensor(input_size + 2, hidden_size))\n",
    "        self.WI = nn.Parameter(torch.Tensor(input_size + 2, hidden_size))\n",
    "        self.cI = nn.Parameter(torch.Tensor(n_learner_params, 1))\n",
    "        self.bI = nn.Parameter(torch.Tensor(1, hidden_size))\n",
    "        self.bF = nn.Parameter(torch.Tensor(1, hidden_size))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for weight in self.parameters():\n",
    "            nn.init.uniform_(weight, -0.01, 0.01)\n",
    "\n",
    "        # want initial forget value to be high and input value to be low so that \n",
    "        #  model starts with gradient descent\n",
    "        nn.init.uniform_(self.bF, 4, 6)\n",
    "        nn.init.uniform_(self.bI, -5, -4)\n",
    "\n",
    "    def init_cI(self, flat_params):\n",
    "        self.cI.data.copy_(flat_params.unsqueeze(1))\n",
    "\n",
    "    def forward(self, inputs, hx=None):\n",
    "        \"\"\"Args:\n",
    "            inputs = [x_all, grad]:\n",
    "                x_all (torch.Tensor of size [n_learner_params, input_size]): outputs from previous LSTM\n",
    "                grad (torch.Tensor of size [n_learner_params]): gradients from learner\n",
    "            hx = [f_prev, i_prev, c_prev]:\n",
    "                f (torch.Tensor of size [n_learner_params, 1]): forget gate\n",
    "                i (torch.Tensor of size [n_learner_params, 1]): input gate\n",
    "                c (torch.Tensor of size [n_learner_params, 1]): flattened learner parameters\n",
    "        \"\"\"\n",
    "        x_all, grad = inputs\n",
    "        batch, _ = x_all.size()\n",
    "\n",
    "        if hx is None:\n",
    "            f_prev = torch.zeros((batch, self.hidden_size)).to(self.WF.device)\n",
    "            i_prev = torch.zeros((batch, self.hidden_size)).to(self.WI.device)\n",
    "            c_prev = self.cI\n",
    "            hx = [f_prev, i_prev, c_prev]\n",
    "\n",
    "        f_prev, i_prev, c_prev = hx\n",
    "        \n",
    "        # f_t = sigmoid(W_f * [grad_t, loss_t, theta_{t-1}, f_{t-1}] + b_f)\n",
    "        f_next = torch.mm(torch.cat((x_all, c_prev, f_prev), 1), self.WF) + self.bF.expand_as(f_prev)\n",
    "        # i_t = sigmoid(W_i * [grad_t, loss_t, theta_{t-1}, i_{t-1}] + b_i)\n",
    "        i_next = torch.mm(torch.cat((x_all, c_prev, i_prev), 1), self.WI) + self.bI.expand_as(i_prev)\n",
    "        # next cell/params\n",
    "        c_next = torch.sigmoid(f_next).mul(c_prev) - torch.sigmoid(i_next).mul(grad)\n",
    "\n",
    "        return c_next, [f_next, i_next, c_next]\n",
    "\n",
    "    def extra_repr(self):\n",
    "        s = '{input_size}, {hidden_size}, {n_learner_params}'\n",
    "        return s.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class MetaLearner(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, n_learner_params):\n",
    "        super(MetaLearner, self).__init__()\n",
    "        \"\"\"Args:\n",
    "            input_size (int): for the first LSTM layer, default = 4\n",
    "            hidden_size (int): for the first LSTM layer, default = 20\n",
    "            n_learner_params (int): number of learner's parameters\n",
    "        \"\"\"\n",
    "        self.lstm = nn.LSTMCell(input_size=input_size, hidden_size=hidden_size)\n",
    "        self.metalstm = MetaLSTMCell(input_size=hidden_size, hidden_size=1, n_learner_params=n_learner_params)\n",
    "\n",
    "    def forward(self, inputs, hs=None):\n",
    "        \"\"\"Args:\n",
    "            inputs = [loss, grad_prep, grad]\n",
    "                loss (torch.Tensor of size [1, 2])\n",
    "                grad_prep (torch.Tensor of size [n_learner_params, 2])\n",
    "                grad (torch.Tensor of size [n_learner_params])\n",
    "            hs = [(lstm_hn, lstm_cn), [metalstm_fn, metalstm_in, metalstm_cn]]\n",
    "        \"\"\"\n",
    "        loss, grad_prep, grad = inputs\n",
    "        loss = loss.expand_as(grad_prep)\n",
    "        inputs = torch.cat((loss, grad_prep), 1)   # [n_learner_params, 4]\n",
    "\n",
    "        if hs is None:\n",
    "            hs = [None, None]\n",
    "\n",
    "        lstmhx, lstmcx = self.lstm(inputs, hs[0])\n",
    "        flat_learner_unsqzd, metalstm_hs = self.metalstm([lstmhx, grad], hs[1])\n",
    "\n",
    "        return flat_learner_unsqzd.squeeze(), [(lstmhx, lstmcx), metalstm_hs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GOATLogger:\n",
    "\n",
    "    def __init__(self, args):\n",
    "        save = \"./\" + '-{}'.format(2019)\n",
    "\n",
    "        self.mode = args['mode']\n",
    "        self.save_root = save\n",
    "        self.log_freq = 100\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            if not os.path.exists(self.save_root):\n",
    "                os.mkdir(self.save_root)\n",
    "            filename = os.path.join(self.save_root, 'console.log')\n",
    "            logging.basicConfig(level=logging.DEBUG,\n",
    "                format='%(asctime)s.%(msecs)03d - %(message)s',\n",
    "                datefmt='%b-%d %H:%M:%S',\n",
    "                filename=filename,\n",
    "                filemode='w')\n",
    "            console = logging.StreamHandler()\n",
    "            console.setLevel(logging.INFO)\n",
    "            console.setFormatter(logging.Formatter('%(message)s'))\n",
    "            logging.getLogger('').addHandler(console)\n",
    "\n",
    "            logging.info(\"Logger created at {}\".format(filename))\n",
    "        else:\n",
    "            logging.basicConfig(level=logging.INFO,\n",
    "                format='%(asctime)s.%(msecs)03d - %(message)s',\n",
    "                datefmt='%b-%d %H:%M:%S')\n",
    "\n",
    "        logging.info(\"Random Seed: {}\".format(2019))\n",
    "        self.reset_stats()\n",
    "\n",
    "    def reset_stats(self):\n",
    "        if self.mode == 'train':\n",
    "            self.stats = {'train': {'loss': [], 'acc': []},\n",
    "                          'eval': {'loss': [], 'acc': []}}\n",
    "        else:\n",
    "            self.stats = {'eval': {'loss': [], 'acc': []}}\n",
    "\n",
    "    def batch_info(self, **kwargs):\n",
    "        if kwargs['phase'] == 'train':\n",
    "            self.stats['train']['loss'].append(kwargs['loss'])\n",
    "            self.stats['train']['acc'].append(kwargs['acc'])\n",
    "\n",
    "            if kwargs['eps'] % self.log_freq == 0 and kwargs['eps'] != 0:\n",
    "                loss_mean = np.mean(self.stats['train']['loss'])\n",
    "                acc_mean = np.mean(self.stats['train']['acc'])\n",
    "                #self.draw_stats()\n",
    "                self.loginfo(\"[{:5d}/{:5d}] loss: {:6.4f} ({:6.4f}), acc: {:6.3f}% ({:6.3f}%)\".format(\\\n",
    "                    kwargs['eps'], kwargs['totaleps'], kwargs['loss'], loss_mean, kwargs['acc'], acc_mean))\n",
    "\n",
    "        elif kwargs['phase'] == 'eval':\n",
    "            self.stats['eval']['loss'].append(kwargs['loss'])\n",
    "            self.stats['eval']['acc'].append(kwargs['acc'])\n",
    "\n",
    "        elif kwargs['phase'] == 'evaldone':\n",
    "            loss_mean = np.mean(self.stats['eval']['loss'])\n",
    "            loss_std = np.std(self.stats['eval']['loss'])\n",
    "            acc_mean = np.mean(self.stats['eval']['acc'])\n",
    "            acc_std = np.std(self.stats['eval']['acc'])\n",
    "            self.loginfo(\"[{:5d}] Eval ({:3d} episode) - loss: {:6.4f} +- {:6.4f}, acc: {:6.3f} +- {:5.3f}%\".format(\\\n",
    "                kwargs['eps'], kwargs['totaleps'], loss_mean, loss_std, acc_mean, acc_std))\n",
    "\n",
    "            self.reset_stats()\n",
    "            return acc_mean\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"phase {} not supported\".format(kwargs['phase']))\n",
    "\n",
    "    def logdebug(self, strout):\n",
    "        logging.debug(strout)\n",
    "    def loginfo(self, strout):\n",
    "        logging.info(strout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res[0].item() if len(res) == 1 else [r.item() for r in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ckpt(episode, metalearner, optim, save):\n",
    "    if not os.path.exists(os.path.join(save, 'ckpts')):\n",
    "        os.mkdir(os.path.join(save, 'ckpts'))\n",
    "\n",
    "    torch.save({\n",
    "        'episode': episode,\n",
    "        'metalearner': metalearner.state_dict(),\n",
    "        'optim': optim.state_dict()\n",
    "    }, os.path.join(save, 'ckpts', 'meta-learner-{}.pth.tar'.format(episode)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resume_ckpt(metalearner, optim, resume, device):\n",
    "    ckpt = torch.load(resume, map_location=device)\n",
    "    last_episode = ckpt['episode']\n",
    "    metalearner.load_state_dict(ckpt['metalearner'])\n",
    "    optim.load_state_dict(ckpt['optim'])\n",
    "    return last_episode, metalearner, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_grad_loss(x):\n",
    "    p = 10\n",
    "    indicator = (x.abs() >= np.exp(-p)).to(torch.float32)\n",
    "\n",
    "    # preproc1\n",
    "    x_proc1 = indicator * torch.log(x.abs() + 1e-8) / p + (1 - indicator) * -1\n",
    "    # preproc2\n",
    "    x_proc2 = indicator * torch.sign(x) + (1 - indicator) * np.exp(p) * x\n",
    "    return torch.stack((x_proc1, x_proc2), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_test(eps, eval_loader, learner_w_grad, learner_wo_grad, metalearner, args, logger):\n",
    "    for subeps, (episode_x, episode_y) in enumerate(tqdm(eval_loader, ascii=True)):\n",
    "        train_input = episode_x[:, :args['n_shot']].reshape(-1, *episode_x.shape[-3:]) # [n_class * n_shot, :]\n",
    "        train_target = torch.LongTensor(np.repeat(range(args['n_class']), args['n_shot'])) # [n_class * n_shot]\n",
    "        test_input = episode_x[:, args['n_shot']:].reshape(-1, *episode_x.shape[-3:]) # [n_class * n_eval, :]\n",
    "        test_target = torch.LongTensor(np.repeat(range(args['n_class']), args['n_eval'])) # [n_class * n_eval]\n",
    "\n",
    "        # Train learner with metalearner\n",
    "        learner_w_grad.reset_batch_stats()\n",
    "        learner_wo_grad.reset_batch_stats()\n",
    "        learner_w_grad.train()\n",
    "        learner_wo_grad.eval()\n",
    "        cI = train_learner(learner_w_grad, metalearner, train_input, train_target, args)\n",
    "\n",
    "        learner_wo_grad.transfer_params(learner_w_grad, cI)\n",
    "        output = learner_wo_grad(test_input)\n",
    "        loss = learner_wo_grad.criterion(output, test_target)\n",
    "        acc = accuracy(output, test_target)\n",
    " \n",
    "        logger.batch_info(loss=loss.item(), acc=acc, phase='eval')\n",
    "        #print (loss=loss.item(), acc=acc, )\n",
    "        \n",
    "\n",
    "    return logger.batch_info(eps=eps, totaleps=args['episode_val'], phase='evaldone')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_learner(learner_w_grad, metalearner, train_input, train_target, args):\n",
    "    cI = metalearner.metalstm.cI.data\n",
    "    hs = [None]\n",
    "    for _ in range(args['epoch']):\n",
    "        for i in range(0, len(train_input), args['batch_size']):\n",
    "            x = train_input[i:i+args['batch_size']]\n",
    "            y = train_target[i:i+args['batch_size']]\n",
    "\n",
    "            # get the loss/grad\n",
    "            learner_w_grad.copy_flat_params(cI)\n",
    "            output = learner_w_grad(x)\n",
    "            loss = learner_w_grad.criterion(output, y)\n",
    "            acc = accuracy(output, y)\n",
    "            learner_w_grad.zero_grad()\n",
    "            loss.backward()\n",
    "            grad = torch.cat([p.grad.data.view(-1) / args['batch_size'] for p in learner_w_grad.parameters()], 0)\n",
    "\n",
    "            # preprocess grad & loss and metalearner forward\n",
    "            grad_prep = preprocess_grad_loss(grad)  # [n_learner_params, 2]\n",
    "            loss_prep = preprocess_grad_loss(loss.data.unsqueeze(0)) # [1, 2]\n",
    "            metalearner_input = [loss_prep, grad_prep, grad.unsqueeze(1)]\n",
    "            cI, h = metalearner(metalearner_input, hs[-1])\n",
    "            hs.append(h)\n",
    "            print(\"training loss: {:8.6f} acc: {:6.3f}, mean grad: {:8.6f}\".format(loss, acc, torch.mean(grad)))\n",
    "\n",
    "    return cI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    seed = 2019\n",
    "    if seed is None:\n",
    "        seed = random.randint(0, 1e3)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    dev = torch.device('cpu')\n",
    "    logger = GOATLogger(args)\n",
    "\n",
    "    # Get data\n",
    "    train_loader, val_loader, test_loader = prepare_data(args)\n",
    "    \n",
    "    # Set up learner, meta-learner\n",
    "    learner_w_grad = Learner(args['image_size'], args['bn_eps'], args['bn_momentum'], args['n_class'])\n",
    "    learner_wo_grad = copy.deepcopy(learner_w_grad)\n",
    "    metalearner = MetaLearner(args['input_size'], args['hidden_size'], learner_w_grad.get_flat_params().size(0))\n",
    "    metalearner.metalstm.init_cI(learner_w_grad.get_flat_params())\n",
    "\n",
    "    # Set up loss, optimizer, learning rate scheduler\n",
    "    optim = torch.optim.Adam(metalearner.parameters(), args['lr'])\n",
    "\n",
    "    if args['resume']:\n",
    "        logger.loginfo(\"Initialized from: {}\".format(args['resume']))\n",
    "        last_eps, metalearner, optim = resume_ckpt(metalearner, optim, args['resume'], dev)\n",
    "\n",
    "    if args['mode'] == 'test':\n",
    "        _ = meta_test(last_eps, test_loader, learner_w_grad, learner_wo_grad, metalearner, args, logger)\n",
    "        return\n",
    "\n",
    "    best_acc = 0.0\n",
    "    logger.loginfo(\"Start training\")\n",
    "    # Meta-training\n",
    "    for eps, (episode_x, episode_y) in enumerate(train_loader):\n",
    "        # episode_x.shape = [n_class, n_shot + n_eval, c, h, w]\n",
    "        # episode_y.shape = [n_class, n_shot + n_eval] --> NEVER USED\n",
    "        train_input = episode_x[:, :args['n_shot']].reshape(-1, *episode_x.shape[-3:]) # [n_class * n_shot, :]\n",
    "        train_target = torch.LongTensor(np.repeat(range(args['n_shot']), args['n_shot'])) # [n_class * n_shot]\n",
    "        test_input = episode_x[:, args['n_shot']:].reshape(-1, *episode_x.shape[-3:]) # [n_class * n_eval, :]\n",
    "        test_target = torch.LongTensor(np.repeat(range(args['n_shot']), args['n_eval'])) # [n_class * n_eval]\n",
    "\n",
    "        # Train learner with metalearner\n",
    "        learner_w_grad.reset_batch_stats()\n",
    "        learner_wo_grad.reset_batch_stats()\n",
    "        learner_w_grad.train()\n",
    "        learner_wo_grad.train()\n",
    "        cI = train_learner(learner_w_grad, metalearner, train_input, train_target, args)\n",
    "\n",
    "        # Train meta-learner with validation loss\n",
    "        learner_wo_grad.transfer_params(learner_w_grad, cI)\n",
    "        output = learner_wo_grad(test_input)\n",
    "        loss = learner_wo_grad.criterion(output, test_target)\n",
    "        acc = accuracy(output, test_target)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(metalearner.parameters(), args['grad_clip'])\n",
    "        optim.step()\n",
    "\n",
    "        logger.batch_info(eps=eps, totaleps=args['episode'], loss=loss.item(), acc=acc, phase='train')\n",
    "\n",
    "        # Meta-validation\n",
    "        if eps % 1000 == 0 and eps != 0:\n",
    "            save_ckpt(eps, metalearner, optim, \"./\")\n",
    "            acc = meta_test(eps, val_loader, learner_w_grad, learner_wo_grad, metalearner, args, logger)\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                logger.loginfo(\"* Best accuracy so far *\\n\")\n",
    "\n",
    "    logger.loginfo(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Logger created at ./-2019/console.log\n",
      "Random Seed: 2019\n",
      "Start training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " BEGIN TRAINING: \n",
      "training loss: 1.818345 acc: 25.000, mean grad: 0.000053\n",
      "training loss: 1.644813 acc:  0.000, mean grad: 0.000002\n",
      "training loss: 1.790295 acc: 25.000, mean grad: 0.000063\n",
      "training loss: 1.576223 acc:  0.000, mean grad: 0.000009\n",
      "training loss: 1.766894 acc: 25.000, mean grad: 0.000054\n",
      "training loss: 1.518992 acc: 22.222, mean grad: 0.000009\n",
      "training loss: 1.745665 acc: 25.000, mean grad: 0.000053\n",
      "training loss: 1.471501 acc: 22.222, mean grad: 0.000014\n",
      "training loss: 1.726845 acc: 25.000, mean grad: 0.000048\n",
      "training loss: 1.431588 acc: 33.333, mean grad: 0.000010\n",
      "training loss: 1.710378 acc: 25.000, mean grad: 0.000043\n",
      "training loss: 1.397009 acc: 44.444, mean grad: 0.000014\n",
      "training loss: 1.695461 acc: 25.000, mean grad: 0.000042\n",
      "training loss: 1.366491 acc: 55.556, mean grad: 0.000019\n",
      "training loss: 1.681154 acc: 25.000, mean grad: 0.000040\n",
      "training loss: 1.339876 acc: 55.556, mean grad: 0.000020\n",
      "training loss: 1.605418 acc: 31.250, mean grad: -0.000042\n",
      "training loss: 1.644333 acc:  0.000, mean grad: 0.000007\n",
      "training loss: 1.594200 acc: 31.250, mean grad: -0.000035\n",
      "training loss: 1.586638 acc: 22.222, mean grad: 0.000026\n",
      "training loss: 1.584412 acc: 31.250, mean grad: -0.000034\n",
      "training loss: 1.535838 acc: 33.333, mean grad: 0.000013\n",
      "training loss: 1.575086 acc: 31.250, mean grad: -0.000042\n",
      "training loss: 1.491459 acc: 33.333, mean grad: 0.000019\n",
      "training loss: 1.566365 acc: 37.500, mean grad: -0.000039\n",
      "training loss: 1.452320 acc: 33.333, mean grad: 0.000019\n",
      "training loss: 1.558814 acc: 37.500, mean grad: -0.000034\n",
      "training loss: 1.418601 acc: 44.444, mean grad: 0.000026\n",
      "training loss: 1.552777 acc: 37.500, mean grad: -0.000027\n",
      "training loss: 1.388608 acc: 55.556, mean grad: 0.000031\n",
      "training loss: 1.547005 acc: 37.500, mean grad: -0.000025\n",
      "training loss: 1.362208 acc: 55.556, mean grad: 0.000045\n",
      "training loss: 1.447758 acc: 56.250, mean grad: -0.000058\n",
      "training loss: 1.825889 acc:  0.000, mean grad: 0.000068\n",
      "training loss: 1.440203 acc: 56.250, mean grad: -0.000063\n",
      "training loss: 1.774108 acc:  0.000, mean grad: 0.000056\n",
      "training loss: 1.433260 acc: 62.500, mean grad: -0.000065\n",
      "training loss: 1.726325 acc:  0.000, mean grad: 0.000059\n",
      "training loss: 1.426950 acc: 62.500, mean grad: -0.000070\n",
      "training loss: 1.682600 acc:  0.000, mean grad: 0.000050\n",
      "training loss: 1.421696 acc: 62.500, mean grad: -0.000075\n",
      "training loss: 1.643041 acc:  0.000, mean grad: 0.000051\n",
      "training loss: 1.416913 acc: 62.500, mean grad: -0.000070\n",
      "training loss: 1.607651 acc: 11.111, mean grad: 0.000041\n",
      "training loss: 1.412650 acc: 62.500, mean grad: -0.000066\n",
      "training loss: 1.575172 acc: 33.333, mean grad: 0.000029\n",
      "training loss: 1.409155 acc: 62.500, mean grad: -0.000057\n",
      "training loss: 1.546429 acc: 44.444, mean grad: 0.000025\n",
      "training loss: 1.553847 acc: 37.500, mean grad: -0.000059\n",
      "training loss: 1.838674 acc:  0.000, mean grad: 0.000002\n",
      "training loss: 1.548826 acc: 37.500, mean grad: -0.000058\n",
      "training loss: 1.777514 acc:  0.000, mean grad: 0.000011\n",
      "training loss: 1.544140 acc: 43.750, mean grad: -0.000057\n",
      "training loss: 1.722301 acc:  0.000, mean grad: 0.000002\n",
      "training loss: 1.539468 acc: 37.500, mean grad: -0.000050\n",
      "training loss: 1.673499 acc:  0.000, mean grad: -0.000006\n",
      "training loss: 1.535062 acc: 37.500, mean grad: -0.000043\n",
      "training loss: 1.629385 acc:  0.000, mean grad: -0.000017\n",
      "training loss: 1.531046 acc: 37.500, mean grad: -0.000045\n",
      "training loss: 1.589865 acc: 11.111, mean grad: -0.000006\n",
      "training loss: 1.526810 acc: 37.500, mean grad: -0.000045\n",
      "training loss: 1.554989 acc: 11.111, mean grad: -0.000012\n",
      "training loss: 1.522804 acc: 37.500, mean grad: -0.000042\n",
      "training loss: 1.523769 acc: 22.222, mean grad: 0.000004\n",
      "training loss: 1.490549 acc: 31.250, mean grad: -0.000024\n",
      "training loss: 2.014267 acc:  0.000, mean grad: 0.000057\n",
      "training loss: 1.488162 acc: 31.250, mean grad: -0.000032\n",
      "training loss: 1.933064 acc:  0.000, mean grad: 0.000057\n",
      "training loss: 1.485311 acc: 31.250, mean grad: -0.000033\n",
      "training loss: 1.855571 acc:  0.000, mean grad: 0.000050\n",
      "training loss: 1.482741 acc: 31.250, mean grad: -0.000015\n",
      "training loss: 1.787577 acc:  0.000, mean grad: 0.000038\n",
      "training loss: 1.482307 acc: 43.750, mean grad: 0.000000\n",
      "training loss: 1.729331 acc:  0.000, mean grad: 0.000041\n",
      "training loss: 1.482450 acc: 50.000, mean grad: -0.000006\n",
      "training loss: 1.678461 acc:  0.000, mean grad: 0.000046\n",
      "training loss: 1.482705 acc: 50.000, mean grad: 0.000002\n",
      "training loss: 1.632897 acc: 11.111, mean grad: 0.000045\n",
      "training loss: 1.482853 acc: 50.000, mean grad: 0.000007\n",
      "training loss: 1.592340 acc: 11.111, mean grad: 0.000028\n",
      "training loss: 1.465943 acc: 31.250, mean grad: 0.000063\n",
      "training loss: 1.982126 acc:  0.000, mean grad: -0.000044\n",
      "training loss: 1.461892 acc: 31.250, mean grad: 0.000057\n",
      "training loss: 1.906377 acc:  0.000, mean grad: -0.000044\n",
      "training loss: 1.458636 acc: 31.250, mean grad: 0.000054\n",
      "training loss: 1.838517 acc:  0.000, mean grad: -0.000039\n",
      "training loss: 1.456563 acc: 31.250, mean grad: 0.000046\n",
      "training loss: 1.777997 acc:  0.000, mean grad: -0.000044\n",
      "training loss: 1.454978 acc: 31.250, mean grad: 0.000039\n",
      "training loss: 1.723442 acc:  0.000, mean grad: -0.000047\n",
      "training loss: 1.453694 acc: 31.250, mean grad: 0.000025\n",
      "training loss: 1.674782 acc:  0.000, mean grad: -0.000048\n",
      "training loss: 1.452558 acc: 37.500, mean grad: 0.000018\n",
      "training loss: 1.631180 acc: 11.111, mean grad: -0.000048\n",
      "training loss: 1.451671 acc: 37.500, mean grad: 0.000020\n",
      "training loss: 1.592547 acc: 11.111, mean grad: -0.000054\n",
      "training loss: 1.448900 acc: 37.500, mean grad: 0.000016\n",
      "training loss: 2.012214 acc:  0.000, mean grad: -0.000022\n",
      "training loss: 1.447720 acc: 50.000, mean grad: 0.000011\n",
      "training loss: 1.939285 acc:  0.000, mean grad: -0.000032\n",
      "training loss: 1.445861 acc: 50.000, mean grad: 0.000012\n",
      "training loss: 1.872186 acc:  0.000, mean grad: -0.000027\n",
      "training loss: 1.444390 acc: 50.000, mean grad: 0.000021\n",
      "training loss: 1.810889 acc:  0.000, mean grad: -0.000016\n",
      "training loss: 1.442950 acc: 50.000, mean grad: 0.000018\n",
      "training loss: 1.756321 acc:  0.000, mean grad: -0.000026\n",
      "training loss: 1.441687 acc: 50.000, mean grad: 0.000014\n",
      "training loss: 1.707440 acc:  0.000, mean grad: -0.000021\n",
      "training loss: 1.440539 acc: 56.250, mean grad: 0.000012\n",
      "training loss: 1.663164 acc:  0.000, mean grad: -0.000016\n",
      "training loss: 1.439736 acc: 62.500, mean grad: 0.000020\n",
      "training loss: 1.622821 acc: 11.111, mean grad: -0.000012\n",
      "training loss: 1.490388 acc: 43.750, mean grad: 0.000008\n",
      "training loss: 1.992398 acc:  0.000, mean grad: -0.000021\n",
      "training loss: 1.488509 acc: 43.750, mean grad: 0.000006\n",
      "training loss: 1.910498 acc:  0.000, mean grad: -0.000027\n",
      "training loss: 1.486223 acc: 43.750, mean grad: 0.000006\n",
      "training loss: 1.837552 acc:  0.000, mean grad: -0.000025\n",
      "training loss: 1.484511 acc: 43.750, mean grad: 0.000002\n",
      "training loss: 1.772111 acc:  0.000, mean grad: -0.000020\n",
      "training loss: 1.483454 acc: 43.750, mean grad: -0.000000\n",
      "training loss: 1.713002 acc:  0.000, mean grad: -0.000018\n",
      "training loss: 1.482694 acc: 43.750, mean grad: -0.000001\n",
      "training loss: 1.659794 acc:  0.000, mean grad: -0.000016\n",
      "training loss: 1.481750 acc: 43.750, mean grad: -0.000008\n",
      "training loss: 1.611747 acc:  0.000, mean grad: -0.000003\n",
      "training loss: 1.480812 acc: 37.500, mean grad: -0.000008\n",
      "training loss: 1.568494 acc: 11.111, mean grad: -0.000007\n",
      "training loss: 1.505070 acc: 18.750, mean grad: -0.000066\n",
      "training loss: 1.914881 acc:  0.000, mean grad: 0.000035\n",
      "training loss: 1.499767 acc: 18.750, mean grad: -0.000057\n",
      "training loss: 1.842358 acc:  0.000, mean grad: 0.000031\n",
      "training loss: 1.494528 acc: 31.250, mean grad: -0.000054\n",
      "training loss: 1.777383 acc: 11.111, mean grad: 0.000026\n",
      "training loss: 1.489684 acc: 37.500, mean grad: -0.000054\n",
      "training loss: 1.718972 acc: 11.111, mean grad: 0.000022\n",
      "training loss: 1.485773 acc: 37.500, mean grad: -0.000057\n",
      "training loss: 1.666714 acc: 22.222, mean grad: 0.000027\n",
      "training loss: 1.482544 acc: 37.500, mean grad: -0.000057\n",
      "training loss: 1.619882 acc: 33.333, mean grad: 0.000028\n",
      "training loss: 1.479462 acc: 43.750, mean grad: -0.000060\n",
      "training loss: 1.578009 acc: 55.556, mean grad: 0.000034\n",
      "training loss: 1.476667 acc: 43.750, mean grad: -0.000068\n",
      "training loss: 1.540117 acc: 55.556, mean grad: 0.000041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.548511 acc: 25.000, mean grad: 0.000006\n",
      "training loss: 1.924238 acc:  0.000, mean grad: -0.000006\n",
      "training loss: 1.540775 acc: 31.250, mean grad: 0.000001\n",
      "training loss: 1.868076 acc:  0.000, mean grad: -0.000001\n",
      "training loss: 1.533399 acc: 31.250, mean grad: 0.000006\n",
      "training loss: 1.816997 acc: 11.111, mean grad: -0.000008\n",
      "training loss: 1.526887 acc: 31.250, mean grad: 0.000005\n",
      "training loss: 1.771737 acc: 22.222, mean grad: -0.000013\n",
      "training loss: 1.521333 acc: 31.250, mean grad: 0.000010\n",
      "training loss: 1.730765 acc: 22.222, mean grad: -0.000021\n",
      "training loss: 1.516629 acc: 31.250, mean grad: 0.000009\n",
      "training loss: 1.693472 acc: 22.222, mean grad: -0.000027\n",
      "training loss: 1.512358 acc: 31.250, mean grad: 0.000007\n",
      "training loss: 1.659215 acc: 22.222, mean grad: -0.000028\n",
      "training loss: 1.508586 acc: 31.250, mean grad: 0.000003\n",
      "training loss: 1.628090 acc: 22.222, mean grad: -0.000023\n",
      "training loss: 1.536752 acc: 31.250, mean grad: -0.000009\n",
      "training loss: 1.893290 acc:  0.000, mean grad: 0.000085\n",
      "training loss: 1.534404 acc: 31.250, mean grad: -0.000006\n",
      "training loss: 1.832444 acc:  0.000, mean grad: 0.000087\n",
      "training loss: 1.532795 acc: 31.250, mean grad: -0.000014\n",
      "training loss: 1.779048 acc:  0.000, mean grad: 0.000074\n",
      "training loss: 1.531272 acc: 31.250, mean grad: -0.000018\n",
      "training loss: 1.731025 acc:  0.000, mean grad: 0.000065\n",
      "training loss: 1.530142 acc: 25.000, mean grad: -0.000020\n",
      "training loss: 1.687773 acc:  0.000, mean grad: 0.000067\n",
      "training loss: 1.529454 acc: 25.000, mean grad: -0.000027\n",
      "training loss: 1.648531 acc: 11.111, mean grad: 0.000063\n",
      "training loss: 1.529007 acc: 25.000, mean grad: -0.000027\n",
      "training loss: 1.613353 acc: 22.222, mean grad: 0.000057\n",
      "training loss: 1.528660 acc: 18.750, mean grad: -0.000028\n",
      "training loss: 1.581509 acc: 22.222, mean grad: 0.000053\n",
      "training loss: 1.558507 acc: 31.250, mean grad: 0.000012\n",
      "training loss: 1.768182 acc:  0.000, mean grad: 0.000101\n",
      "training loss: 1.556844 acc: 31.250, mean grad: 0.000008\n",
      "training loss: 1.718436 acc:  0.000, mean grad: 0.000100\n",
      "training loss: 1.554837 acc: 37.500, mean grad: 0.000010\n",
      "training loss: 1.673462 acc:  0.000, mean grad: 0.000095\n",
      "training loss: 1.552941 acc: 31.250, mean grad: 0.000006\n",
      "training loss: 1.633187 acc: 11.111, mean grad: 0.000094\n",
      "training loss: 1.551593 acc: 31.250, mean grad: 0.000001\n",
      "training loss: 1.597022 acc: 11.111, mean grad: 0.000095\n",
      "training loss: 1.550593 acc: 31.250, mean grad: 0.000006\n",
      "training loss: 1.564314 acc: 22.222, mean grad: 0.000095\n",
      "training loss: 1.549895 acc: 31.250, mean grad: 0.000004\n",
      "training loss: 1.534532 acc: 33.333, mean grad: 0.000102\n",
      "training loss: 1.549239 acc: 31.250, mean grad: 0.000005\n",
      "training loss: 1.507233 acc: 44.444, mean grad: 0.000097\n",
      "training loss: 1.577071 acc: 25.000, mean grad: 0.000024\n",
      "training loss: 1.794400 acc:  0.000, mean grad: 0.000037\n",
      "training loss: 1.576160 acc: 18.750, mean grad: 0.000023\n",
      "training loss: 1.745474 acc:  0.000, mean grad: 0.000043\n",
      "training loss: 1.574521 acc: 31.250, mean grad: 0.000023\n",
      "training loss: 1.701292 acc:  0.000, mean grad: 0.000041\n",
      "training loss: 1.573176 acc: 37.500, mean grad: 0.000020\n",
      "training loss: 1.661510 acc: 11.111, mean grad: 0.000033\n",
      "training loss: 1.571708 acc: 37.500, mean grad: 0.000019\n",
      "training loss: 1.625583 acc: 22.222, mean grad: 0.000033\n",
      "training loss: 1.570429 acc: 43.750, mean grad: 0.000011\n",
      "training loss: 1.593194 acc: 22.222, mean grad: 0.000026\n",
      "training loss: 1.569014 acc: 43.750, mean grad: 0.000012\n",
      "training loss: 1.563674 acc: 22.222, mean grad: 0.000019\n",
      "training loss: 1.567592 acc: 43.750, mean grad: 0.000004\n",
      "training loss: 1.536983 acc: 33.333, mean grad: 0.000020\n",
      "training loss: 1.548556 acc: 37.500, mean grad: -0.000049\n",
      "training loss: 1.590037 acc: 33.333, mean grad: -0.000051\n",
      "training loss: 1.549973 acc: 31.250, mean grad: -0.000044\n",
      "training loss: 1.549138 acc: 33.333, mean grad: -0.000048\n",
      "training loss: 1.550457 acc: 31.250, mean grad: -0.000041\n",
      "training loss: 1.512143 acc: 44.444, mean grad: -0.000044\n",
      "training loss: 1.550977 acc: 25.000, mean grad: -0.000042\n",
      "training loss: 1.478899 acc: 44.444, mean grad: -0.000043\n",
      "training loss: 1.551436 acc: 25.000, mean grad: -0.000043\n",
      "training loss: 1.448926 acc: 66.667, mean grad: -0.000048\n",
      "training loss: 1.551509 acc: 25.000, mean grad: -0.000036\n",
      "training loss: 1.422111 acc: 66.667, mean grad: -0.000050\n",
      "training loss: 1.551463 acc: 25.000, mean grad: -0.000041\n",
      "training loss: 1.398078 acc: 66.667, mean grad: -0.000041\n",
      "training loss: 1.551236 acc: 18.750, mean grad: -0.000045\n",
      "training loss: 1.376369 acc: 66.667, mean grad: -0.000034\n",
      "training loss: 1.613126 acc: 18.750, mean grad: 0.000028\n",
      "training loss: 1.652345 acc:  0.000, mean grad: 0.000027\n",
      "training loss: 1.613197 acc: 12.500, mean grad: 0.000027\n",
      "training loss: 1.616613 acc:  0.000, mean grad: 0.000024\n",
      "training loss: 1.612247 acc: 12.500, mean grad: 0.000029\n",
      "training loss: 1.584092 acc:  0.000, mean grad: 0.000024\n",
      "training loss: 1.611373 acc: 12.500, mean grad: 0.000031\n",
      "training loss: 1.554721 acc: 11.111, mean grad: 0.000020\n",
      "training loss: 1.610332 acc: 12.500, mean grad: 0.000028\n",
      "training loss: 1.528126 acc: 55.556, mean grad: 0.000018\n",
      "training loss: 1.609277 acc: 12.500, mean grad: 0.000027\n",
      "training loss: 1.504016 acc: 55.556, mean grad: 0.000017\n",
      "training loss: 1.608367 acc: 12.500, mean grad: 0.000023\n",
      "training loss: 1.481931 acc: 66.667, mean grad: 0.000017\n",
      "training loss: 1.607379 acc: 12.500, mean grad: 0.000027\n",
      "training loss: 1.461635 acc: 66.667, mean grad: 0.000023\n",
      "training loss: 1.593702 acc: 25.000, mean grad: 0.000030\n",
      "training loss: 1.666729 acc: 11.111, mean grad: 0.000070\n",
      "training loss: 1.593698 acc: 25.000, mean grad: 0.000027\n",
      "training loss: 1.621729 acc: 22.222, mean grad: 0.000069\n",
      "training loss: 1.592557 acc: 25.000, mean grad: 0.000025\n",
      "training loss: 1.581248 acc: 33.333, mean grad: 0.000056\n",
      "training loss: 1.591492 acc: 25.000, mean grad: 0.000022\n",
      "training loss: 1.545082 acc: 33.333, mean grad: 0.000060\n",
      "training loss: 1.590626 acc: 25.000, mean grad: 0.000019\n",
      "training loss: 1.511825 acc: 33.333, mean grad: 0.000059\n",
      "training loss: 1.589795 acc: 25.000, mean grad: 0.000013\n",
      "training loss: 1.481452 acc: 55.556, mean grad: 0.000052\n",
      "training loss: 1.588869 acc: 25.000, mean grad: 0.000007\n",
      "training loss: 1.453944 acc: 55.556, mean grad: 0.000045\n",
      "training loss: 1.587914 acc: 25.000, mean grad: 0.000008\n",
      "training loss: 1.429210 acc: 66.667, mean grad: 0.000040\n",
      "training loss: 1.606471 acc: 18.750, mean grad: -0.000064\n",
      "training loss: 1.543514 acc: 44.444, mean grad: 0.000003\n",
      "training loss: 1.605206 acc: 18.750, mean grad: -0.000062\n",
      "training loss: 1.500746 acc: 44.444, mean grad: -0.000006\n",
      "training loss: 1.602541 acc: 18.750, mean grad: -0.000058\n",
      "training loss: 1.462022 acc: 55.556, mean grad: -0.000014\n",
      "training loss: 1.599819 acc: 18.750, mean grad: -0.000059\n",
      "training loss: 1.427043 acc: 55.556, mean grad: -0.000017\n",
      "training loss: 1.597323 acc: 18.750, mean grad: -0.000059\n",
      "training loss: 1.395176 acc: 55.556, mean grad: -0.000013\n",
      "training loss: 1.594835 acc: 18.750, mean grad: -0.000056\n",
      "training loss: 1.366437 acc: 55.556, mean grad: -0.000023\n",
      "training loss: 1.592544 acc: 18.750, mean grad: -0.000058\n",
      "training loss: 1.340843 acc: 55.556, mean grad: -0.000027\n",
      "training loss: 1.590200 acc: 18.750, mean grad: -0.000057\n",
      "training loss: 1.317808 acc: 55.556, mean grad: -0.000027\n",
      "training loss: 1.699316 acc: 12.500, mean grad: 0.000003\n",
      "training loss: 1.695073 acc: 11.111, mean grad: 0.000034\n",
      "training loss: 1.692964 acc: 12.500, mean grad: -0.000004\n",
      "training loss: 1.650494 acc: 33.333, mean grad: 0.000033\n",
      "training loss: 1.686017 acc: 12.500, mean grad: -0.000001\n",
      "training loss: 1.609540 acc: 44.444, mean grad: 0.000025\n",
      "training loss: 1.680086 acc: 12.500, mean grad: -0.000004\n",
      "training loss: 1.572508 acc: 44.444, mean grad: 0.000026\n",
      "training loss: 1.675018 acc: 12.500, mean grad: -0.000005\n",
      "training loss: 1.538776 acc: 44.444, mean grad: 0.000036\n",
      "training loss: 1.670331 acc: 12.500, mean grad: -0.000013\n",
      "training loss: 1.508731 acc: 55.556, mean grad: 0.000037\n",
      "training loss: 1.665910 acc: 12.500, mean grad: -0.000011\n",
      "training loss: 1.481712 acc: 55.556, mean grad: 0.000040\n",
      "training loss: 1.661981 acc: 12.500, mean grad: -0.000012\n",
      "training loss: 1.457249 acc: 55.556, mean grad: 0.000039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.677855 acc: 25.000, mean grad: -0.000033\n",
      "training loss: 1.615753 acc: 11.111, mean grad: -0.000014\n",
      "training loss: 1.667161 acc: 25.000, mean grad: -0.000042\n",
      "training loss: 1.576966 acc: 22.222, mean grad: -0.000014\n",
      "training loss: 1.654901 acc: 25.000, mean grad: -0.000042\n",
      "training loss: 1.541796 acc: 22.222, mean grad: -0.000017\n",
      "training loss: 1.644382 acc: 12.500, mean grad: -0.000042\n",
      "training loss: 1.510861 acc: 22.222, mean grad: -0.000020\n",
      "training loss: 1.634868 acc: 12.500, mean grad: -0.000038\n",
      "training loss: 1.482659 acc: 33.333, mean grad: -0.000018\n",
      "training loss: 1.626018 acc: 12.500, mean grad: -0.000036\n",
      "training loss: 1.457137 acc: 33.333, mean grad: -0.000016\n",
      "training loss: 1.617258 acc: 18.750, mean grad: -0.000039\n",
      "training loss: 1.433913 acc: 33.333, mean grad: -0.000015\n",
      "training loss: 1.609069 acc: 18.750, mean grad: -0.000038\n",
      "training loss: 1.413026 acc: 33.333, mean grad: -0.000013\n",
      "training loss: 1.666476 acc: 25.000, mean grad: 0.000043\n",
      "training loss: 1.711140 acc:  0.000, mean grad: -0.000021\n",
      "training loss: 1.663726 acc: 18.750, mean grad: 0.000038\n",
      "training loss: 1.673155 acc: 11.111, mean grad: -0.000023\n",
      "training loss: 1.659495 acc: 18.750, mean grad: 0.000036\n",
      "training loss: 1.638255 acc: 11.111, mean grad: -0.000023\n",
      "training loss: 1.655243 acc: 25.000, mean grad: 0.000034\n",
      "training loss: 1.606318 acc: 11.111, mean grad: -0.000022\n",
      "training loss: 1.651229 acc: 25.000, mean grad: 0.000031\n",
      "training loss: 1.577191 acc: 11.111, mean grad: -0.000019\n",
      "training loss: 1.647549 acc: 25.000, mean grad: 0.000031\n",
      "training loss: 1.550692 acc: 11.111, mean grad: -0.000017\n",
      "training loss: 1.644083 acc: 25.000, mean grad: 0.000031\n",
      "training loss: 1.526388 acc: 22.222, mean grad: -0.000013\n",
      "training loss: 1.640693 acc: 25.000, mean grad: 0.000030\n",
      "training loss: 1.503852 acc: 44.444, mean grad: -0.000019\n",
      "training loss: 1.672011 acc: 18.750, mean grad: -0.000005\n",
      "training loss: 1.650650 acc: 22.222, mean grad: -0.000011\n",
      "training loss: 1.666843 acc: 18.750, mean grad: -0.000008\n",
      "training loss: 1.620780 acc: 22.222, mean grad: -0.000009\n",
      "training loss: 1.660354 acc: 18.750, mean grad: -0.000007\n",
      "training loss: 1.593522 acc: 22.222, mean grad: -0.000015\n",
      "training loss: 1.654323 acc: 18.750, mean grad: -0.000010\n",
      "training loss: 1.568747 acc: 22.222, mean grad: -0.000012\n",
      "training loss: 1.648526 acc: 12.500, mean grad: -0.000012\n",
      "training loss: 1.546308 acc: 22.222, mean grad: -0.000007\n",
      "training loss: 1.643034 acc: 12.500, mean grad: -0.000012\n",
      "training loss: 1.525581 acc: 22.222, mean grad: -0.000009\n",
      "training loss: 1.637572 acc: 12.500, mean grad: -0.000014\n",
      "training loss: 1.506857 acc: 33.333, mean grad: -0.000006\n",
      "training loss: 1.632500 acc: 12.500, mean grad: -0.000012\n",
      "training loss: 1.489554 acc: 33.333, mean grad: -0.000015\n",
      "training loss: 1.692776 acc: 12.500, mean grad: 0.000004\n",
      "training loss: 1.588537 acc: 22.222, mean grad: 0.000016\n",
      "training loss: 1.686356 acc: 12.500, mean grad: 0.000000\n",
      "training loss: 1.559509 acc: 33.333, mean grad: 0.000012\n",
      "training loss: 1.678148 acc: 12.500, mean grad: -0.000004\n",
      "training loss: 1.532772 acc: 33.333, mean grad: 0.000009\n",
      "training loss: 1.670545 acc: 12.500, mean grad: -0.000004\n",
      "training loss: 1.508279 acc: 33.333, mean grad: 0.000009\n",
      "training loss: 1.663251 acc: 12.500, mean grad: -0.000005\n",
      "training loss: 1.485927 acc: 33.333, mean grad: 0.000006\n",
      "training loss: 1.656283 acc:  6.250, mean grad: -0.000005\n",
      "training loss: 1.465489 acc: 33.333, mean grad: 0.000002\n",
      "training loss: 1.649795 acc:  6.250, mean grad: -0.000004\n",
      "training loss: 1.446826 acc: 44.444, mean grad: -0.000002\n",
      "training loss: 1.643736 acc: 12.500, mean grad: -0.000005\n",
      "training loss: 1.429662 acc: 55.556, mean grad: -0.000002\n",
      "training loss: 1.712585 acc: 18.750, mean grad: 0.000006\n",
      "training loss: 1.694305 acc: 22.222, mean grad: 0.000028\n",
      "training loss: 1.701944 acc: 18.750, mean grad: 0.000007\n",
      "training loss: 1.662070 acc: 22.222, mean grad: 0.000025\n",
      "training loss: 1.690046 acc: 25.000, mean grad: 0.000004\n",
      "training loss: 1.632783 acc: 22.222, mean grad: 0.000029\n",
      "training loss: 1.678998 acc: 25.000, mean grad: 0.000005\n",
      "training loss: 1.605959 acc: 22.222, mean grad: 0.000023\n",
      "training loss: 1.668631 acc: 25.000, mean grad: 0.000003\n",
      "training loss: 1.581555 acc: 33.333, mean grad: 0.000022\n",
      "training loss: 1.659057 acc: 18.750, mean grad: 0.000004\n",
      "training loss: 1.559385 acc: 33.333, mean grad: 0.000022\n",
      "training loss: 1.650212 acc: 18.750, mean grad: 0.000007\n",
      "training loss: 1.539244 acc: 33.333, mean grad: 0.000020\n",
      "training loss: 1.641999 acc: 18.750, mean grad: 0.000006\n",
      "training loss: 1.520981 acc: 33.333, mean grad: 0.000024\n",
      "training loss: 1.680218 acc: 12.500, mean grad: 0.000041\n",
      "training loss: 1.799000 acc:  0.000, mean grad: 0.000002\n",
      "training loss: 1.677910 acc:  6.250, mean grad: 0.000041\n",
      "training loss: 1.748006 acc: 11.111, mean grad: -0.000007\n",
      "training loss: 1.674263 acc:  6.250, mean grad: 0.000037\n",
      "training loss: 1.702604 acc: 22.222, mean grad: -0.000009\n",
      "training loss: 1.670772 acc:  6.250, mean grad: 0.000034\n",
      "training loss: 1.661865 acc: 22.222, mean grad: -0.000013\n",
      "training loss: 1.667246 acc: 12.500, mean grad: 0.000035\n",
      "training loss: 1.625470 acc: 33.333, mean grad: -0.000016\n",
      "training loss: 1.663638 acc: 12.500, mean grad: 0.000035\n",
      "training loss: 1.592961 acc: 33.333, mean grad: -0.000017\n",
      "training loss: 1.660263 acc: 12.500, mean grad: 0.000032\n",
      "training loss: 1.563632 acc: 44.444, mean grad: -0.000022\n",
      "training loss: 1.656873 acc: 12.500, mean grad: 0.000034\n",
      "training loss: 1.537178 acc: 44.444, mean grad: -0.000020\n",
      "training loss: 1.763007 acc: 18.750, mean grad: 0.000065\n",
      "training loss: 1.693746 acc: 22.222, mean grad: -0.000020\n",
      "training loss: 1.749440 acc: 18.750, mean grad: 0.000060\n",
      "training loss: 1.664304 acc: 33.333, mean grad: -0.000021\n",
      "training loss: 1.735140 acc: 18.750, mean grad: 0.000058\n",
      "training loss: 1.637718 acc: 33.333, mean grad: -0.000020\n",
      "training loss: 1.722361 acc: 18.750, mean grad: 0.000058\n",
      "training loss: 1.613737 acc: 33.333, mean grad: -0.000023\n",
      "training loss: 1.710512 acc: 18.750, mean grad: 0.000051\n",
      "training loss: 1.592050 acc: 33.333, mean grad: -0.000021\n",
      "training loss: 1.699931 acc: 18.750, mean grad: 0.000047\n",
      "training loss: 1.572155 acc: 33.333, mean grad: -0.000025\n",
      "training loss: 1.690311 acc: 18.750, mean grad: 0.000046\n",
      "training loss: 1.554090 acc: 33.333, mean grad: -0.000028\n",
      "training loss: 1.681489 acc: 18.750, mean grad: 0.000045\n",
      "training loss: 1.537639 acc: 33.333, mean grad: -0.000026\n",
      "training loss: 1.636485 acc:  6.250, mean grad: 0.000015\n",
      "training loss: 1.730586 acc:  0.000, mean grad: -0.000002\n",
      "training loss: 1.636270 acc:  6.250, mean grad: 0.000009\n",
      "training loss: 1.690395 acc: 22.222, mean grad: -0.000003\n",
      "training loss: 1.634939 acc:  6.250, mean grad: 0.000011\n",
      "training loss: 1.653900 acc: 22.222, mean grad: -0.000003\n",
      "training loss: 1.633768 acc: 12.500, mean grad: 0.000015\n",
      "training loss: 1.620903 acc: 22.222, mean grad: 0.000003\n",
      "training loss: 1.632483 acc:  6.250, mean grad: 0.000015\n",
      "training loss: 1.591162 acc: 33.333, mean grad: 0.000005\n",
      "training loss: 1.631231 acc:  6.250, mean grad: 0.000008\n",
      "training loss: 1.564299 acc: 66.667, mean grad: 0.000002\n",
      "training loss: 1.630004 acc:  6.250, mean grad: 0.000008\n",
      "training loss: 1.540132 acc: 66.667, mean grad: 0.000005\n",
      "training loss: 1.628861 acc:  6.250, mean grad: 0.000004\n",
      "training loss: 1.518154 acc: 66.667, mean grad: 0.000008\n",
      "training loss: 1.561307 acc: 18.750, mean grad: -0.000003\n",
      "training loss: 1.739964 acc:  0.000, mean grad: -0.000058\n",
      "training loss: 1.558806 acc: 18.750, mean grad: -0.000006\n",
      "training loss: 1.705054 acc: 11.111, mean grad: -0.000054\n",
      "training loss: 1.555186 acc: 31.250, mean grad: -0.000003\n",
      "training loss: 1.673476 acc: 22.222, mean grad: -0.000054\n",
      "training loss: 1.551842 acc: 37.500, mean grad: -0.000007\n",
      "training loss: 1.644910 acc: 22.222, mean grad: -0.000048\n",
      "training loss: 1.548779 acc: 37.500, mean grad: -0.000006\n",
      "training loss: 1.619212 acc: 22.222, mean grad: -0.000045\n",
      "training loss: 1.545847 acc: 37.500, mean grad: -0.000010\n",
      "training loss: 1.595942 acc: 33.333, mean grad: -0.000044\n",
      "training loss: 1.543154 acc: 37.500, mean grad: -0.000012\n",
      "training loss: 1.574728 acc: 33.333, mean grad: -0.000039\n",
      "training loss: 1.540641 acc: 37.500, mean grad: -0.000012\n",
      "training loss: 1.555609 acc: 44.444, mean grad: -0.000038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.545787 acc: 31.250, mean grad: -0.000031\n",
      "training loss: 1.965320 acc:  0.000, mean grad: 0.000019\n",
      "training loss: 1.547970 acc: 31.250, mean grad: -0.000031\n",
      "training loss: 1.913669 acc: 11.111, mean grad: 0.000008\n",
      "training loss: 1.548909 acc: 31.250, mean grad: -0.000032\n",
      "training loss: 1.866336 acc: 11.111, mean grad: 0.000007\n",
      "training loss: 1.549880 acc: 31.250, mean grad: -0.000023\n",
      "training loss: 1.823274 acc: 11.111, mean grad: 0.000014\n",
      "training loss: 1.550919 acc: 31.250, mean grad: -0.000025\n",
      "training loss: 1.783915 acc: 11.111, mean grad: 0.000012\n",
      "training loss: 1.552038 acc: 31.250, mean grad: -0.000016\n",
      "training loss: 1.747998 acc: 11.111, mean grad: 0.000011\n",
      "training loss: 1.553068 acc: 31.250, mean grad: -0.000014\n",
      "training loss: 1.715202 acc: 11.111, mean grad: 0.000009\n",
      "training loss: 1.554101 acc: 31.250, mean grad: -0.000013\n",
      "training loss: 1.685208 acc: 11.111, mean grad: 0.000006\n",
      "training loss: 1.515662 acc: 31.250, mean grad: 0.000003\n",
      "training loss: 1.842015 acc:  0.000, mean grad: -0.000017\n",
      "training loss: 1.516546 acc: 31.250, mean grad: -0.000003\n",
      "training loss: 1.789749 acc: 11.111, mean grad: -0.000019\n",
      "training loss: 1.515902 acc: 31.250, mean grad: -0.000008\n",
      "training loss: 1.742495 acc: 11.111, mean grad: -0.000016\n",
      "training loss: 1.515219 acc: 25.000, mean grad: -0.000006\n",
      "training loss: 1.699709 acc: 11.111, mean grad: -0.000019\n",
      "training loss: 1.514614 acc: 37.500, mean grad: -0.000006\n",
      "training loss: 1.661138 acc: 11.111, mean grad: -0.000016\n",
      "training loss: 1.514287 acc: 37.500, mean grad: -0.000002\n",
      "training loss: 1.626546 acc: 22.222, mean grad: -0.000014\n",
      "training loss: 1.514056 acc: 37.500, mean grad: -0.000000\n",
      "training loss: 1.595402 acc: 33.333, mean grad: -0.000015\n",
      "training loss: 1.513725 acc: 37.500, mean grad: -0.000004\n",
      "training loss: 1.567573 acc: 33.333, mean grad: -0.000011\n",
      "training loss: 1.499928 acc: 25.000, mean grad: -0.000012\n",
      "training loss: 1.933629 acc:  0.000, mean grad: 0.000021\n",
      "training loss: 1.504196 acc: 25.000, mean grad: -0.000009\n",
      "training loss: 1.869778 acc:  0.000, mean grad: 0.000014\n",
      "training loss: 1.507110 acc: 25.000, mean grad: -0.000011\n",
      "training loss: 1.811875 acc: 11.111, mean grad: 0.000014\n",
      "training loss: 1.509979 acc: 18.750, mean grad: -0.000016\n",
      "training loss: 1.759928 acc: 11.111, mean grad: 0.000018\n",
      "training loss: 1.512920 acc: 25.000, mean grad: -0.000016\n",
      "training loss: 1.713012 acc: 11.111, mean grad: 0.000013\n",
      "training loss: 1.515814 acc: 25.000, mean grad: -0.000015\n",
      "training loss: 1.670516 acc: 11.111, mean grad: 0.000015\n",
      "training loss: 1.518565 acc: 25.000, mean grad: -0.000014\n",
      "training loss: 1.632327 acc: 11.111, mean grad: 0.000012\n",
      "training loss: 1.521430 acc: 25.000, mean grad: -0.000013\n",
      "training loss: 1.598011 acc: 22.222, mean grad: 0.000012\n",
      "training loss: 1.536612 acc: 25.000, mean grad: -0.000008\n",
      "training loss: 1.965062 acc:  0.000, mean grad: -0.000028\n",
      "training loss: 1.533683 acc: 31.250, mean grad: -0.000003\n",
      "training loss: 1.903570 acc:  0.000, mean grad: -0.000022\n",
      "training loss: 1.530027 acc: 25.000, mean grad: 0.000000\n",
      "training loss: 1.847155 acc:  0.000, mean grad: -0.000025\n",
      "training loss: 1.526767 acc: 25.000, mean grad: -0.000004\n",
      "training loss: 1.795899 acc:  0.000, mean grad: -0.000027\n",
      "training loss: 1.523742 acc: 25.000, mean grad: -0.000003\n",
      "training loss: 1.749281 acc:  0.000, mean grad: -0.000024\n",
      "training loss: 1.521311 acc: 25.000, mean grad: -0.000008\n",
      "training loss: 1.706902 acc: 11.111, mean grad: -0.000028\n",
      "training loss: 1.519333 acc: 25.000, mean grad: -0.000006\n",
      "training loss: 1.668469 acc: 11.111, mean grad: -0.000030\n",
      "training loss: 1.517624 acc: 25.000, mean grad: -0.000012\n",
      "training loss: 1.633348 acc: 11.111, mean grad: -0.000025\n",
      "training loss: 1.512929 acc: 18.750, mean grad: -0.000014\n",
      "training loss: 1.919213 acc:  0.000, mean grad: 0.000006\n",
      "training loss: 1.515953 acc: 18.750, mean grad: -0.000010\n",
      "training loss: 1.859790 acc:  0.000, mean grad: 0.000009\n",
      "training loss: 1.517413 acc: 18.750, mean grad: -0.000007\n",
      "training loss: 1.805781 acc:  0.000, mean grad: 0.000013\n",
      "training loss: 1.518928 acc: 25.000, mean grad: -0.000006\n",
      "training loss: 1.757365 acc:  0.000, mean grad: 0.000016\n",
      "training loss: 1.520793 acc: 25.000, mean grad: -0.000005\n",
      "training loss: 1.714386 acc: 11.111, mean grad: 0.000013\n",
      "training loss: 1.522577 acc: 31.250, mean grad: -0.000004\n",
      "training loss: 1.676006 acc: 11.111, mean grad: 0.000012\n",
      "training loss: 1.524332 acc: 31.250, mean grad: -0.000006\n",
      "training loss: 1.641512 acc: 11.111, mean grad: 0.000014\n",
      "training loss: 1.525960 acc: 37.500, mean grad: -0.000007\n",
      "training loss: 1.610475 acc: 22.222, mean grad: 0.000011\n",
      "training loss: 1.521120 acc: 25.000, mean grad: 0.000034\n",
      "training loss: 1.878968 acc:  0.000, mean grad: -0.000004\n",
      "training loss: 1.519474 acc: 25.000, mean grad: 0.000032\n",
      "training loss: 1.830238 acc:  0.000, mean grad: -0.000011\n",
      "training loss: 1.516527 acc: 31.250, mean grad: 0.000033\n",
      "training loss: 1.785404 acc:  0.000, mean grad: -0.000007\n",
      "training loss: 1.514090 acc: 31.250, mean grad: 0.000030\n",
      "training loss: 1.744861 acc:  0.000, mean grad: -0.000004\n",
      "training loss: 1.511949 acc: 37.500, mean grad: 0.000030\n",
      "training loss: 1.708053 acc:  0.000, mean grad: -0.000006\n",
      "training loss: 1.510308 acc: 31.250, mean grad: 0.000030\n",
      "training loss: 1.674734 acc:  0.000, mean grad: -0.000009\n",
      "training loss: 1.508967 acc: 31.250, mean grad: 0.000031\n",
      "training loss: 1.644584 acc:  0.000, mean grad: -0.000010\n",
      "training loss: 1.507750 acc: 37.500, mean grad: 0.000027\n",
      "training loss: 1.617176 acc: 22.222, mean grad: -0.000013\n",
      "training loss: 1.500737 acc: 43.750, mean grad: 0.000029\n",
      "training loss: 1.937543 acc:  0.000, mean grad: -0.000039\n",
      "training loss: 1.502834 acc: 43.750, mean grad: 0.000023\n",
      "training loss: 1.881774 acc: 11.111, mean grad: -0.000034\n",
      "training loss: 1.503132 acc: 43.750, mean grad: 0.000023\n",
      "training loss: 1.830953 acc: 11.111, mean grad: -0.000029\n",
      "training loss: 1.503628 acc: 43.750, mean grad: 0.000021\n",
      "training loss: 1.785212 acc: 22.222, mean grad: -0.000027\n",
      "training loss: 1.504099 acc: 43.750, mean grad: 0.000023\n",
      "training loss: 1.744058 acc: 22.222, mean grad: -0.000027\n",
      "training loss: 1.504617 acc: 43.750, mean grad: 0.000019\n",
      "training loss: 1.706800 acc: 22.222, mean grad: -0.000019\n",
      "training loss: 1.505009 acc: 43.750, mean grad: 0.000016\n",
      "training loss: 1.672863 acc: 22.222, mean grad: -0.000021\n",
      "training loss: 1.505696 acc: 43.750, mean grad: 0.000015\n",
      "training loss: 1.642405 acc: 22.222, mean grad: -0.000021\n",
      "training loss: 1.504186 acc: 37.500, mean grad: -0.000011\n",
      "training loss: 1.916978 acc:  0.000, mean grad: 0.000008\n",
      "training loss: 1.508322 acc: 37.500, mean grad: -0.000013\n",
      "training loss: 1.875118 acc:  0.000, mean grad: 0.000010\n",
      "training loss: 1.510628 acc: 37.500, mean grad: -0.000011\n",
      "training loss: 1.836743 acc:  0.000, mean grad: 0.000011\n",
      "training loss: 1.512606 acc: 37.500, mean grad: -0.000009\n",
      "training loss: 1.801749 acc:  0.000, mean grad: 0.000008\n",
      "training loss: 1.514224 acc: 37.500, mean grad: -0.000009\n",
      "training loss: 1.770193 acc:  0.000, mean grad: 0.000004\n",
      "training loss: 1.515544 acc: 37.500, mean grad: -0.000008\n",
      "training loss: 1.741466 acc: 11.111, mean grad: 0.000001\n",
      "training loss: 1.516857 acc: 31.250, mean grad: -0.000008\n",
      "training loss: 1.715317 acc: 11.111, mean grad: -0.000001\n",
      "training loss: 1.518006 acc: 31.250, mean grad: -0.000007\n",
      "training loss: 1.691560 acc: 11.111, mean grad: -0.000003\n",
      "training loss: 1.507963 acc: 37.500, mean grad: 0.000015\n",
      "training loss: 1.740950 acc: 11.111, mean grad: -0.000006\n",
      "training loss: 1.510563 acc: 37.500, mean grad: 0.000010\n",
      "training loss: 1.693544 acc: 11.111, mean grad: -0.000005\n",
      "training loss: 1.512138 acc: 37.500, mean grad: 0.000010\n",
      "training loss: 1.650316 acc: 11.111, mean grad: -0.000001\n",
      "training loss: 1.513745 acc: 37.500, mean grad: 0.000007\n",
      "training loss: 1.611439 acc: 22.222, mean grad: -0.000002\n",
      "training loss: 1.515403 acc: 37.500, mean grad: 0.000006\n",
      "training loss: 1.576541 acc: 22.222, mean grad: -0.000002\n",
      "training loss: 1.517011 acc: 37.500, mean grad: 0.000003\n",
      "training loss: 1.545626 acc: 33.333, mean grad: -0.000003\n",
      "training loss: 1.518574 acc: 31.250, mean grad: 0.000002\n",
      "training loss: 1.518021 acc: 33.333, mean grad: -0.000007\n",
      "training loss: 1.520126 acc: 31.250, mean grad: -0.000003\n",
      "training loss: 1.493237 acc: 33.333, mean grad: -0.000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.553959 acc: 12.500, mean grad: 0.000002\n",
      "training loss: 1.912628 acc:  0.000, mean grad: -0.000007\n",
      "training loss: 1.548029 acc: 12.500, mean grad: 0.000003\n",
      "training loss: 1.865874 acc:  0.000, mean grad: -0.000012\n",
      "training loss: 1.540588 acc: 12.500, mean grad: 0.000006\n",
      "training loss: 1.822937 acc:  0.000, mean grad: -0.000011\n",
      "training loss: 1.534034 acc: 12.500, mean grad: 0.000012\n",
      "training loss: 1.783912 acc:  0.000, mean grad: -0.000015\n",
      "training loss: 1.528636 acc: 12.500, mean grad: 0.000018\n",
      "training loss: 1.748560 acc:  0.000, mean grad: -0.000017\n",
      "training loss: 1.524016 acc: 18.750, mean grad: 0.000016\n",
      "training loss: 1.716529 acc: 11.111, mean grad: -0.000017\n",
      "training loss: 1.520083 acc: 18.750, mean grad: 0.000015\n",
      "training loss: 1.687382 acc: 11.111, mean grad: -0.000020\n",
      "training loss: 1.516654 acc: 18.750, mean grad: 0.000018\n",
      "training loss: 1.660815 acc: 22.222, mean grad: -0.000021\n",
      "training loss: 1.569333 acc:  6.250, mean grad: 0.000033\n",
      "training loss: 1.875655 acc: 11.111, mean grad: -0.000017\n",
      "training loss: 1.569524 acc:  6.250, mean grad: 0.000032\n",
      "training loss: 1.821357 acc: 11.111, mean grad: -0.000018\n",
      "training loss: 1.567621 acc:  6.250, mean grad: 0.000033\n",
      "training loss: 1.770588 acc: 11.111, mean grad: -0.000022\n",
      "training loss: 1.565915 acc: 12.500, mean grad: 0.000028\n",
      "training loss: 1.724233 acc: 11.111, mean grad: -0.000019\n",
      "training loss: 1.564284 acc: 12.500, mean grad: 0.000026\n",
      "training loss: 1.682558 acc: 11.111, mean grad: -0.000016\n",
      "training loss: 1.562782 acc: 18.750, mean grad: 0.000024\n",
      "training loss: 1.644994 acc: 11.111, mean grad: -0.000010\n",
      "training loss: 1.561423 acc: 18.750, mean grad: 0.000019\n",
      "training loss: 1.611180 acc: 11.111, mean grad: -0.000011\n",
      "training loss: 1.560101 acc: 12.500, mean grad: 0.000021\n",
      "training loss: 1.580866 acc: 11.111, mean grad: -0.000010\n",
      "training loss: 1.421436 acc: 50.000, mean grad: -0.000001\n",
      "training loss: 1.807211 acc:  0.000, mean grad: 0.000007\n",
      "training loss: 1.427750 acc: 50.000, mean grad: 0.000002\n",
      "training loss: 1.759197 acc: 11.111, mean grad: 0.000005\n",
      "training loss: 1.432461 acc: 50.000, mean grad: 0.000004\n",
      "training loss: 1.715500 acc: 11.111, mean grad: 0.000006\n",
      "training loss: 1.436867 acc: 50.000, mean grad: 0.000003\n",
      "training loss: 1.676024 acc: 11.111, mean grad: 0.000000\n",
      "training loss: 1.441127 acc: 43.750, mean grad: 0.000001\n",
      "training loss: 1.640844 acc: 33.333, mean grad: -0.000001\n",
      "training loss: 1.445358 acc: 50.000, mean grad: 0.000002\n",
      "training loss: 1.609516 acc: 44.444, mean grad: -0.000005\n",
      "training loss: 1.449384 acc: 50.000, mean grad: 0.000001\n",
      "training loss: 1.581499 acc: 44.444, mean grad: -0.000007\n",
      "training loss: 1.453201 acc: 43.750, mean grad: 0.000002\n",
      "training loss: 1.556390 acc: 44.444, mean grad: -0.000006\n",
      "training loss: 1.614750 acc: 18.750, mean grad: -0.000030\n",
      "training loss: 1.771805 acc: 11.111, mean grad: -0.000034\n",
      "training loss: 1.613619 acc: 18.750, mean grad: -0.000033\n",
      "training loss: 1.733487 acc: 11.111, mean grad: -0.000027\n",
      "training loss: 1.610606 acc: 18.750, mean grad: -0.000033\n",
      "training loss: 1.698331 acc: 11.111, mean grad: -0.000025\n",
      "training loss: 1.607658 acc: 18.750, mean grad: -0.000033\n",
      "training loss: 1.666416 acc: 11.111, mean grad: -0.000023\n",
      "training loss: 1.604940 acc: 18.750, mean grad: -0.000032\n",
      "training loss: 1.637837 acc: 11.111, mean grad: -0.000024\n",
      "training loss: 1.602381 acc: 18.750, mean grad: -0.000034\n",
      "training loss: 1.612135 acc: 22.222, mean grad: -0.000023\n",
      "training loss: 1.599925 acc: 18.750, mean grad: -0.000037\n",
      "training loss: 1.588980 acc: 22.222, mean grad: -0.000025\n",
      "training loss: 1.597445 acc: 18.750, mean grad: -0.000036\n",
      "training loss: 1.568063 acc: 22.222, mean grad: -0.000025\n",
      "training loss: 1.629203 acc: 25.000, mean grad: -0.000003\n",
      "training loss: 2.008786 acc:  0.000, mean grad: 0.000011\n",
      "training loss: 1.626374 acc: 25.000, mean grad: -0.000005\n",
      "training loss: 1.937895 acc:  0.000, mean grad: 0.000015\n",
      "training loss: 1.621691 acc: 25.000, mean grad: -0.000007\n",
      "training loss: 1.872208 acc:  0.000, mean grad: 0.000015\n",
      "training loss: 1.617501 acc: 25.000, mean grad: -0.000007\n",
      "training loss: 1.811908 acc:  0.000, mean grad: 0.000014\n",
      "training loss: 1.613818 acc: 25.000, mean grad: -0.000008\n",
      "training loss: 1.756985 acc:  0.000, mean grad: 0.000014\n",
      "training loss: 1.610486 acc: 25.000, mean grad: -0.000015\n",
      "training loss: 1.707207 acc: 22.222, mean grad: 0.000014\n",
      "training loss: 1.607423 acc: 25.000, mean grad: -0.000016\n",
      "training loss: 1.662200 acc: 22.222, mean grad: 0.000017\n",
      "training loss: 1.604602 acc: 25.000, mean grad: -0.000020\n",
      "training loss: 1.620975 acc: 33.333, mean grad: 0.000016\n",
      "training loss: 1.545450 acc: 31.250, mean grad: -0.000013\n",
      "training loss: 1.761482 acc:  0.000, mean grad: 0.000010\n",
      "training loss: 1.550835 acc: 25.000, mean grad: -0.000015\n",
      "training loss: 1.722893 acc: 11.111, mean grad: 0.000009\n",
      "training loss: 1.554529 acc: 25.000, mean grad: -0.000019\n",
      "training loss: 1.687722 acc: 22.222, mean grad: 0.000008\n",
      "training loss: 1.557899 acc: 25.000, mean grad: -0.000019\n",
      "training loss: 1.656121 acc: 22.222, mean grad: 0.000008\n",
      "training loss: 1.561077 acc: 25.000, mean grad: -0.000018\n",
      "training loss: 1.627844 acc: 22.222, mean grad: 0.000004\n",
      "training loss: 1.563958 acc: 25.000, mean grad: -0.000018\n",
      "training loss: 1.602963 acc: 22.222, mean grad: 0.000005\n",
      "training loss: 1.566727 acc: 18.750, mean grad: -0.000019\n",
      "training loss: 1.580634 acc: 33.333, mean grad: 0.000001\n",
      "training loss: 1.569278 acc: 18.750, mean grad: -0.000022\n",
      "training loss: 1.560636 acc: 33.333, mean grad: 0.000000\n",
      "training loss: 1.459894 acc: 50.000, mean grad: -0.000015\n",
      "training loss: 1.738380 acc: 11.111, mean grad: -0.000022\n",
      "training loss: 1.467629 acc: 56.250, mean grad: -0.000014\n",
      "training loss: 1.703701 acc: 22.222, mean grad: -0.000022\n",
      "training loss: 1.473737 acc: 56.250, mean grad: -0.000011\n",
      "training loss: 1.671746 acc: 22.222, mean grad: -0.000024\n",
      "training loss: 1.479331 acc: 50.000, mean grad: -0.000010\n",
      "training loss: 1.642683 acc: 22.222, mean grad: -0.000026\n",
      "training loss: 1.484501 acc: 43.750, mean grad: -0.000005\n",
      "training loss: 1.616421 acc: 44.444, mean grad: -0.000025\n",
      "training loss: 1.489521 acc: 43.750, mean grad: -0.000005\n",
      "training loss: 1.592871 acc: 44.444, mean grad: -0.000027\n",
      "training loss: 1.494133 acc: 37.500, mean grad: -0.000003\n",
      "training loss: 1.571821 acc: 44.444, mean grad: -0.000020\n",
      "training loss: 1.498502 acc: 43.750, mean grad: -0.000001\n",
      "training loss: 1.552992 acc: 44.444, mean grad: -0.000020\n",
      "training loss: 1.564896 acc: 31.250, mean grad: 0.000026\n",
      "training loss: 1.952014 acc:  0.000, mean grad: 0.000040\n",
      "training loss: 1.563087 acc: 31.250, mean grad: 0.000027\n",
      "training loss: 1.902539 acc:  0.000, mean grad: 0.000039\n",
      "training loss: 1.558435 acc: 31.250, mean grad: 0.000026\n",
      "training loss: 1.856287 acc:  0.000, mean grad: 0.000039\n",
      "training loss: 1.554314 acc: 31.250, mean grad: 0.000024\n",
      "training loss: 1.813681 acc:  0.000, mean grad: 0.000039\n",
      "training loss: 1.550273 acc: 31.250, mean grad: 0.000019\n",
      "training loss: 1.774691 acc:  0.000, mean grad: 0.000036\n",
      "training loss: 1.546520 acc: 31.250, mean grad: 0.000019\n",
      "training loss: 1.739106 acc: 11.111, mean grad: 0.000035\n",
      "training loss: 1.542913 acc: 31.250, mean grad: 0.000017\n",
      "training loss: 1.706656 acc: 33.333, mean grad: 0.000035\n",
      "training loss: 1.539733 acc: 37.500, mean grad: 0.000021\n",
      "training loss: 1.677028 acc: 33.333, mean grad: 0.000027\n",
      "training loss: 1.590975 acc: 18.750, mean grad: 0.000017\n",
      "training loss: 1.678877 acc:  0.000, mean grad: 0.000029\n",
      "training loss: 1.591795 acc: 18.750, mean grad: 0.000014\n",
      "training loss: 1.643009 acc:  0.000, mean grad: 0.000025\n",
      "training loss: 1.590738 acc: 18.750, mean grad: 0.000011\n",
      "training loss: 1.609684 acc:  0.000, mean grad: 0.000025\n",
      "training loss: 1.589645 acc: 25.000, mean grad: 0.000009\n",
      "training loss: 1.579284 acc: 11.111, mean grad: 0.000025\n",
      "training loss: 1.588650 acc: 25.000, mean grad: 0.000005\n",
      "training loss: 1.551816 acc: 22.222, mean grad: 0.000022\n",
      "training loss: 1.587229 acc: 25.000, mean grad: 0.000001\n",
      "training loss: 1.526883 acc: 33.333, mean grad: 0.000019\n",
      "training loss: 1.585728 acc: 25.000, mean grad: -0.000000\n",
      "training loss: 1.504432 acc: 44.444, mean grad: 0.000020\n",
      "training loss: 1.584440 acc: 25.000, mean grad: 0.000003\n",
      "training loss: 1.484308 acc: 55.556, mean grad: 0.000022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.533787 acc: 37.500, mean grad: 0.000018\n",
      "training loss: 1.721704 acc:  0.000, mean grad: 0.000010\n",
      "training loss: 1.535343 acc: 37.500, mean grad: 0.000017\n",
      "training loss: 1.693513 acc:  0.000, mean grad: 0.000007\n",
      "training loss: 1.534893 acc: 37.500, mean grad: 0.000015\n",
      "training loss: 1.667233 acc: 11.111, mean grad: 0.000004\n",
      "training loss: 1.534522 acc: 37.500, mean grad: 0.000013\n",
      "training loss: 1.643312 acc: 11.111, mean grad: -0.000001\n",
      "training loss: 1.534050 acc: 37.500, mean grad: 0.000012\n",
      "training loss: 1.621789 acc: 22.222, mean grad: -0.000002\n",
      "training loss: 1.533635 acc: 37.500, mean grad: 0.000011\n",
      "training loss: 1.602411 acc: 22.222, mean grad: -0.000003\n",
      "training loss: 1.533277 acc: 37.500, mean grad: 0.000008\n",
      "training loss: 1.584947 acc: 33.333, mean grad: -0.000003\n",
      "training loss: 1.533029 acc: 37.500, mean grad: 0.000006\n",
      "training loss: 1.569191 acc: 33.333, mean grad: -0.000004\n",
      "training loss: 1.600837 acc: 18.750, mean grad: -0.000010\n",
      "training loss: 1.776595 acc: 11.111, mean grad: 0.000001\n",
      "training loss: 1.603081 acc: 18.750, mean grad: -0.000011\n",
      "training loss: 1.743108 acc: 11.111, mean grad: -0.000001\n",
      "training loss: 1.602945 acc: 18.750, mean grad: -0.000012\n",
      "training loss: 1.711771 acc: 22.222, mean grad: -0.000003\n",
      "training loss: 1.602452 acc: 18.750, mean grad: -0.000013\n",
      "training loss: 1.682988 acc: 22.222, mean grad: -0.000005\n",
      "training loss: 1.601787 acc: 18.750, mean grad: -0.000015\n",
      "training loss: 1.656885 acc: 22.222, mean grad: -0.000007\n",
      "training loss: 1.601177 acc: 12.500, mean grad: -0.000015\n",
      "training loss: 1.633523 acc: 22.222, mean grad: -0.000009\n",
      "training loss: 1.600624 acc: 18.750, mean grad: -0.000015\n",
      "training loss: 1.612555 acc: 22.222, mean grad: -0.000010\n",
      "training loss: 1.599998 acc: 18.750, mean grad: -0.000016\n",
      "training loss: 1.593649 acc: 22.222, mean grad: -0.000012\n",
      "training loss: 1.479363 acc: 31.250, mean grad: 0.000009\n",
      "training loss: 1.742090 acc:  0.000, mean grad: -0.000016\n",
      "training loss: 1.485007 acc: 43.750, mean grad: 0.000010\n",
      "training loss: 1.708992 acc: 11.111, mean grad: -0.000015\n",
      "training loss: 1.488689 acc: 43.750, mean grad: 0.000009\n",
      "training loss: 1.677856 acc: 11.111, mean grad: -0.000014\n",
      "training loss: 1.492213 acc: 43.750, mean grad: 0.000008\n",
      "training loss: 1.649227 acc: 11.111, mean grad: -0.000017\n",
      "training loss: 1.495680 acc: 43.750, mean grad: 0.000010\n",
      "training loss: 1.623206 acc: 22.222, mean grad: -0.000018\n",
      "training loss: 1.499076 acc: 43.750, mean grad: 0.000010\n",
      "training loss: 1.599724 acc: 22.222, mean grad: -0.000017\n",
      "training loss: 1.502398 acc: 43.750, mean grad: 0.000009\n",
      "training loss: 1.578743 acc: 33.333, mean grad: -0.000018\n",
      "training loss: 1.505608 acc: 31.250, mean grad: 0.000008\n",
      "training loss: 1.559920 acc: 33.333, mean grad: -0.000021\n",
      "training loss: 1.557070 acc: 31.250, mean grad: 0.000016\n",
      "training loss: 1.786880 acc:  0.000, mean grad: -0.000033\n",
      "training loss: 1.557770 acc: 31.250, mean grad: 0.000015\n",
      "training loss: 1.750039 acc:  0.000, mean grad: -0.000034\n",
      "training loss: 1.555659 acc: 37.500, mean grad: 0.000014\n",
      "training loss: 1.715231 acc:  0.000, mean grad: -0.000033\n",
      "training loss: 1.553620 acc: 37.500, mean grad: 0.000012\n",
      "training loss: 1.683082 acc: 11.111, mean grad: -0.000034\n",
      "training loss: 1.551732 acc: 37.500, mean grad: 0.000013\n",
      "training loss: 1.653916 acc: 11.111, mean grad: -0.000032\n",
      "training loss: 1.550031 acc: 37.500, mean grad: 0.000013\n",
      "training loss: 1.627588 acc: 11.111, mean grad: -0.000030\n",
      "training loss: 1.548518 acc: 31.250, mean grad: 0.000013\n",
      "training loss: 1.603888 acc: 22.222, mean grad: -0.000028\n",
      "training loss: 1.547199 acc: 37.500, mean grad: 0.000014\n",
      "training loss: 1.582524 acc: 33.333, mean grad: -0.000027\n",
      "training loss: 1.544432 acc: 25.000, mean grad: 0.000029\n",
      "training loss: 1.798197 acc: 11.111, mean grad: 0.000007\n",
      "training loss: 1.548822 acc: 25.000, mean grad: 0.000029\n",
      "training loss: 1.756140 acc: 11.111, mean grad: 0.000001\n",
      "training loss: 1.550741 acc: 25.000, mean grad: 0.000028\n",
      "training loss: 1.716597 acc: 11.111, mean grad: 0.000001\n",
      "training loss: 1.552427 acc: 25.000, mean grad: 0.000030\n",
      "training loss: 1.680222 acc: 11.111, mean grad: 0.000003\n",
      "training loss: 1.553528 acc: 25.000, mean grad: 0.000028\n",
      "training loss: 1.647140 acc: 11.111, mean grad: -0.000000\n",
      "training loss: 1.554526 acc: 25.000, mean grad: 0.000026\n",
      "training loss: 1.617324 acc: 11.111, mean grad: -0.000000\n",
      "training loss: 1.555338 acc: 25.000, mean grad: 0.000027\n",
      "training loss: 1.590481 acc: 11.111, mean grad: -0.000001\n",
      "training loss: 1.555915 acc: 31.250, mean grad: 0.000027\n",
      "training loss: 1.566327 acc: 11.111, mean grad: -0.000003\n",
      "training loss: 1.587709 acc: 25.000, mean grad: -0.000017\n",
      "training loss: 1.770091 acc:  0.000, mean grad: -0.000016\n",
      "training loss: 1.591596 acc: 25.000, mean grad: -0.000020\n",
      "training loss: 1.735720 acc:  0.000, mean grad: -0.000016\n",
      "training loss: 1.593074 acc: 31.250, mean grad: -0.000022\n",
      "training loss: 1.703463 acc:  0.000, mean grad: -0.000019\n",
      "training loss: 1.594169 acc: 31.250, mean grad: -0.000022\n",
      "training loss: 1.673832 acc:  0.000, mean grad: -0.000018\n",
      "training loss: 1.595041 acc: 31.250, mean grad: -0.000021\n",
      "training loss: 1.647079 acc:  0.000, mean grad: -0.000022\n",
      "training loss: 1.595807 acc: 18.750, mean grad: -0.000020\n",
      "training loss: 1.623248 acc: 11.111, mean grad: -0.000023\n",
      "training loss: 1.596463 acc: 25.000, mean grad: -0.000019\n",
      "training loss: 1.601958 acc: 11.111, mean grad: -0.000022\n",
      "training loss: 1.596992 acc: 18.750, mean grad: -0.000020\n",
      "training loss: 1.583031 acc: 22.222, mean grad: -0.000019\n",
      "training loss: 1.551347 acc: 25.000, mean grad: 0.000010\n",
      "training loss: 1.854521 acc:  0.000, mean grad: -0.000008\n",
      "training loss: 1.554635 acc: 31.250, mean grad: 0.000011\n",
      "training loss: 1.807565 acc:  0.000, mean grad: -0.000009\n",
      "training loss: 1.555648 acc: 37.500, mean grad: 0.000015\n",
      "training loss: 1.763511 acc: 11.111, mean grad: -0.000010\n",
      "training loss: 1.556715 acc: 37.500, mean grad: 0.000014\n",
      "training loss: 1.723155 acc: 11.111, mean grad: -0.000015\n",
      "training loss: 1.557911 acc: 25.000, mean grad: 0.000014\n",
      "training loss: 1.686834 acc: 11.111, mean grad: -0.000009\n",
      "training loss: 1.559232 acc: 18.750, mean grad: 0.000013\n",
      "training loss: 1.654393 acc: 11.111, mean grad: -0.000010\n",
      "training loss: 1.560581 acc: 25.000, mean grad: 0.000012\n",
      "training loss: 1.625409 acc: 11.111, mean grad: -0.000010\n",
      "training loss: 1.561908 acc: 25.000, mean grad: 0.000013\n",
      "training loss: 1.599579 acc: 22.222, mean grad: -0.000008\n",
      "training loss: 1.525226 acc: 25.000, mean grad: 0.000021\n",
      "training loss: 1.886790 acc:  0.000, mean grad: 0.000001\n",
      "training loss: 1.531996 acc: 25.000, mean grad: 0.000023\n",
      "training loss: 1.842499 acc:  0.000, mean grad: 0.000002\n",
      "training loss: 1.536630 acc: 25.000, mean grad: 0.000026\n",
      "training loss: 1.799825 acc:  0.000, mean grad: 0.000004\n",
      "training loss: 1.540787 acc: 31.250, mean grad: 0.000027\n",
      "training loss: 1.760273 acc:  0.000, mean grad: 0.000003\n",
      "training loss: 1.544512 acc: 31.250, mean grad: 0.000025\n",
      "training loss: 1.724543 acc:  0.000, mean grad: -0.000000\n",
      "training loss: 1.547915 acc: 31.250, mean grad: 0.000025\n",
      "training loss: 1.692335 acc:  0.000, mean grad: -0.000006\n",
      "training loss: 1.550857 acc: 31.250, mean grad: 0.000023\n",
      "training loss: 1.663358 acc:  0.000, mean grad: -0.000008\n",
      "training loss: 1.553559 acc: 31.250, mean grad: 0.000019\n",
      "training loss: 1.637409 acc:  0.000, mean grad: -0.000010\n",
      "training loss: 1.534189 acc: 43.750, mean grad: 0.000005\n",
      "training loss: 1.861985 acc:  0.000, mean grad: 0.000017\n",
      "training loss: 1.540754 acc: 43.750, mean grad: 0.000008\n",
      "training loss: 1.817817 acc:  0.000, mean grad: 0.000017\n",
      "training loss: 1.544970 acc: 43.750, mean grad: 0.000011\n",
      "training loss: 1.775455 acc:  0.000, mean grad: 0.000016\n",
      "training loss: 1.548853 acc: 37.500, mean grad: 0.000008\n",
      "training loss: 1.736137 acc:  0.000, mean grad: 0.000016\n",
      "training loss: 1.552390 acc: 37.500, mean grad: 0.000009\n",
      "training loss: 1.700441 acc: 11.111, mean grad: 0.000012\n",
      "training loss: 1.555621 acc: 43.750, mean grad: 0.000008\n",
      "training loss: 1.668333 acc: 11.111, mean grad: 0.000014\n",
      "training loss: 1.558559 acc: 37.500, mean grad: 0.000007\n",
      "training loss: 1.639582 acc: 11.111, mean grad: 0.000012\n",
      "training loss: 1.561247 acc: 37.500, mean grad: 0.000007\n",
      "training loss: 1.613830 acc: 11.111, mean grad: 0.000008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.497232 acc: 31.250, mean grad: -0.000005\n",
      "training loss: 1.829999 acc:  0.000, mean grad: 0.000019\n",
      "training loss: 1.505761 acc: 31.250, mean grad: -0.000005\n",
      "training loss: 1.780103 acc:  0.000, mean grad: 0.000019\n",
      "training loss: 1.512410 acc: 31.250, mean grad: -0.000005\n",
      "training loss: 1.732516 acc: 11.111, mean grad: 0.000018\n",
      "training loss: 1.518696 acc: 31.250, mean grad: -0.000002\n",
      "training loss: 1.688681 acc: 11.111, mean grad: 0.000016\n",
      "training loss: 1.524488 acc: 31.250, mean grad: -0.000001\n",
      "training loss: 1.649122 acc: 11.111, mean grad: 0.000017\n",
      "training loss: 1.529767 acc: 31.250, mean grad: -0.000000\n",
      "training loss: 1.613954 acc: 33.333, mean grad: 0.000015\n",
      "training loss: 1.534633 acc: 31.250, mean grad: -0.000001\n",
      "training loss: 1.582788 acc: 33.333, mean grad: 0.000012\n",
      "training loss: 1.539141 acc: 31.250, mean grad: -0.000001\n",
      "training loss: 1.555204 acc: 33.333, mean grad: 0.000011\n",
      "training loss: 1.571529 acc: 31.250, mean grad: 0.000036\n",
      "training loss: 1.901347 acc:  0.000, mean grad: 0.000019\n",
      "training loss: 1.576225 acc: 31.250, mean grad: 0.000034\n",
      "training loss: 1.853819 acc:  0.000, mean grad: 0.000017\n",
      "training loss: 1.578688 acc: 31.250, mean grad: 0.000034\n",
      "training loss: 1.808221 acc: 11.111, mean grad: 0.000015\n",
      "training loss: 1.580931 acc: 31.250, mean grad: 0.000033\n",
      "training loss: 1.765806 acc: 11.111, mean grad: 0.000012\n",
      "training loss: 1.582983 acc: 31.250, mean grad: 0.000033\n",
      "training loss: 1.727376 acc: 11.111, mean grad: 0.000010\n",
      "training loss: 1.584870 acc: 31.250, mean grad: 0.000032\n",
      "training loss: 1.692822 acc: 11.111, mean grad: 0.000007\n",
      "training loss: 1.586652 acc: 31.250, mean grad: 0.000029\n",
      "training loss: 1.662046 acc: 33.333, mean grad: 0.000007\n",
      "training loss: 1.588334 acc: 31.250, mean grad: 0.000028\n",
      "training loss: 1.634726 acc: 33.333, mean grad: 0.000007\n",
      "training loss: 1.488407 acc: 43.750, mean grad: 0.000028\n",
      "training loss: 1.931669 acc:  0.000, mean grad: -0.000017\n",
      "training loss: 1.498157 acc: 43.750, mean grad: 0.000029\n",
      "training loss: 1.871101 acc:  0.000, mean grad: -0.000018\n",
      "training loss: 1.506413 acc: 43.750, mean grad: 0.000028\n",
      "training loss: 1.813010 acc:  0.000, mean grad: -0.000020\n",
      "training loss: 1.514328 acc: 43.750, mean grad: 0.000026\n",
      "training loss: 1.759092 acc: 11.111, mean grad: -0.000024\n",
      "training loss: 1.521718 acc: 43.750, mean grad: 0.000026\n",
      "training loss: 1.710600 acc: 11.111, mean grad: -0.000026\n",
      "training loss: 1.528665 acc: 43.750, mean grad: 0.000026\n",
      "training loss: 1.667380 acc: 11.111, mean grad: -0.000030\n",
      "training loss: 1.535043 acc: 50.000, mean grad: 0.000025\n",
      "training loss: 1.628973 acc: 11.111, mean grad: -0.000028\n",
      "training loss: 1.540739 acc: 37.500, mean grad: 0.000024\n",
      "training loss: 1.595124 acc: 11.111, mean grad: -0.000033\n",
      "training loss: 1.506683 acc: 56.250, mean grad: 0.000020\n",
      "training loss: 1.905805 acc:  0.000, mean grad: -0.000046\n",
      "training loss: 1.509102 acc: 62.500, mean grad: 0.000019\n",
      "training loss: 1.860518 acc:  0.000, mean grad: -0.000041\n",
      "training loss: 1.508973 acc: 62.500, mean grad: 0.000018\n",
      "training loss: 1.816854 acc:  0.000, mean grad: -0.000035\n",
      "training loss: 1.508690 acc: 62.500, mean grad: 0.000018\n",
      "training loss: 1.776416 acc: 11.111, mean grad: -0.000031\n",
      "training loss: 1.508523 acc: 56.250, mean grad: 0.000017\n",
      "training loss: 1.739694 acc: 11.111, mean grad: -0.000028\n",
      "training loss: 1.508408 acc: 56.250, mean grad: 0.000016\n",
      "training loss: 1.706760 acc: 11.111, mean grad: -0.000026\n",
      "training loss: 1.508331 acc: 56.250, mean grad: 0.000017\n",
      "training loss: 1.677379 acc: 11.111, mean grad: -0.000025\n",
      "training loss: 1.508327 acc: 56.250, mean grad: 0.000016\n",
      "training loss: 1.651206 acc: 22.222, mean grad: -0.000024\n",
      "training loss: 1.571202 acc: 18.750, mean grad: 0.000001\n",
      "training loss: 1.735312 acc: 22.222, mean grad: 0.000022\n",
      "training loss: 1.570407 acc: 18.750, mean grad: 0.000002\n",
      "training loss: 1.686224 acc: 22.222, mean grad: 0.000021\n",
      "training loss: 1.566267 acc: 18.750, mean grad: 0.000001\n",
      "training loss: 1.638961 acc: 44.444, mean grad: 0.000020\n",
      "training loss: 1.562032 acc: 18.750, mean grad: -0.000000\n",
      "training loss: 1.595467 acc: 55.556, mean grad: 0.000019\n",
      "training loss: 1.557964 acc: 18.750, mean grad: 0.000001\n",
      "training loss: 1.556452 acc: 55.556, mean grad: 0.000019\n",
      "training loss: 1.554155 acc: 18.750, mean grad: 0.000000\n",
      "training loss: 1.521819 acc: 55.556, mean grad: 0.000021\n",
      "training loss: 1.550662 acc: 18.750, mean grad: 0.000001\n",
      "training loss: 1.491500 acc: 55.556, mean grad: 0.000020\n",
      "training loss: 1.547505 acc: 18.750, mean grad: -0.000000\n",
      "training loss: 1.465044 acc: 66.667, mean grad: 0.000017\n",
      "training loss: 1.582847 acc: 18.750, mean grad: 0.000022\n",
      "training loss: 1.739295 acc: 33.333, mean grad: -0.000009\n",
      "training loss: 1.587062 acc: 18.750, mean grad: 0.000022\n",
      "training loss: 1.687139 acc: 33.333, mean grad: -0.000009\n",
      "training loss: 1.589411 acc: 18.750, mean grad: 0.000020\n",
      "training loss: 1.637086 acc: 33.333, mean grad: -0.000011\n",
      "training loss: 1.591735 acc: 18.750, mean grad: 0.000017\n",
      "training loss: 1.591138 acc: 33.333, mean grad: -0.000010\n",
      "training loss: 1.594059 acc: 18.750, mean grad: 0.000014\n",
      "training loss: 1.550086 acc: 44.444, mean grad: -0.000008\n",
      "training loss: 1.596240 acc: 12.500, mean grad: 0.000010\n",
      "training loss: 1.513938 acc: 44.444, mean grad: -0.000010\n",
      "training loss: 1.598319 acc: 18.750, mean grad: 0.000008\n",
      "training loss: 1.482671 acc: 55.556, mean grad: -0.000013\n",
      "training loss: 1.600273 acc: 18.750, mean grad: 0.000008\n",
      "training loss: 1.455626 acc: 66.667, mean grad: -0.000014\n",
      "training loss: 1.587653 acc:  6.250, mean grad: 0.000012\n",
      "training loss: 1.882884 acc: 11.111, mean grad: -0.000031\n",
      "training loss: 1.589164 acc: 12.500, mean grad: 0.000010\n",
      "training loss: 1.833192 acc: 11.111, mean grad: -0.000030\n",
      "training loss: 1.588964 acc: 18.750, mean grad: 0.000010\n",
      "training loss: 1.784437 acc: 11.111, mean grad: -0.000028\n",
      "training loss: 1.588975 acc: 18.750, mean grad: 0.000009\n",
      "training loss: 1.738666 acc: 11.111, mean grad: -0.000028\n",
      "training loss: 1.589245 acc: 18.750, mean grad: 0.000005\n",
      "training loss: 1.696918 acc: 11.111, mean grad: -0.000027\n",
      "training loss: 1.589699 acc: 18.750, mean grad: 0.000003\n",
      "training loss: 1.659562 acc: 11.111, mean grad: -0.000023\n",
      "training loss: 1.590278 acc: 18.750, mean grad: 0.000002\n",
      "training loss: 1.626455 acc: 22.222, mean grad: -0.000022\n",
      "training loss: 1.590886 acc: 18.750, mean grad: 0.000002\n",
      "training loss: 1.597353 acc: 33.333, mean grad: -0.000020\n",
      "training loss: 1.562629 acc: 31.250, mean grad: -0.000012\n",
      "training loss: 1.773231 acc: 33.333, mean grad: -0.000005\n",
      "training loss: 1.563995 acc: 31.250, mean grad: -0.000014\n",
      "training loss: 1.739020 acc: 33.333, mean grad: -0.000005\n",
      "training loss: 1.562738 acc: 31.250, mean grad: -0.000014\n",
      "training loss: 1.705497 acc: 33.333, mean grad: -0.000005\n",
      "training loss: 1.561486 acc: 31.250, mean grad: -0.000014\n",
      "training loss: 1.674153 acc: 33.333, mean grad: -0.000005\n",
      "training loss: 1.560429 acc: 31.250, mean grad: -0.000015\n",
      "training loss: 1.645910 acc: 33.333, mean grad: -0.000005\n",
      "training loss: 1.559609 acc: 31.250, mean grad: -0.000016\n",
      "training loss: 1.620936 acc: 33.333, mean grad: -0.000003\n",
      "training loss: 1.558995 acc: 31.250, mean grad: -0.000015\n",
      "training loss: 1.599089 acc: 33.333, mean grad: -0.000003\n",
      "training loss: 1.558571 acc: 31.250, mean grad: -0.000015\n",
      "training loss: 1.580159 acc: 33.333, mean grad: -0.000002\n",
      "training loss: 1.507539 acc: 18.750, mean grad: -0.000023\n",
      "training loss: 1.800846 acc: 33.333, mean grad: -0.000005\n",
      "training loss: 1.516586 acc: 18.750, mean grad: -0.000021\n",
      "training loss: 1.755471 acc: 44.444, mean grad: -0.000007\n",
      "training loss: 1.523730 acc: 18.750, mean grad: -0.000022\n",
      "training loss: 1.710248 acc: 44.444, mean grad: -0.000008\n",
      "training loss: 1.530619 acc: 18.750, mean grad: -0.000021\n",
      "training loss: 1.667404 acc: 44.444, mean grad: -0.000010\n",
      "training loss: 1.537082 acc: 18.750, mean grad: -0.000021\n",
      "training loss: 1.628373 acc: 44.444, mean grad: -0.000008\n",
      "training loss: 1.542924 acc: 18.750, mean grad: -0.000022\n",
      "training loss: 1.593480 acc: 44.444, mean grad: -0.000009\n",
      "training loss: 1.548228 acc: 18.750, mean grad: -0.000021\n",
      "training loss: 1.562659 acc: 44.444, mean grad: -0.000009\n",
      "training loss: 1.552976 acc: 18.750, mean grad: -0.000021\n",
      "training loss: 1.535592 acc: 44.444, mean grad: -0.000009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.486312 acc: 31.250, mean grad: 0.000006\n",
      "training loss: 1.761653 acc: 11.111, mean grad: 0.000006\n",
      "training loss: 1.494596 acc: 25.000, mean grad: 0.000004\n",
      "training loss: 1.726277 acc: 11.111, mean grad: 0.000006\n",
      "training loss: 1.501164 acc: 25.000, mean grad: 0.000003\n",
      "training loss: 1.690986 acc: 22.222, mean grad: 0.000007\n",
      "training loss: 1.507230 acc: 25.000, mean grad: 0.000001\n",
      "training loss: 1.657847 acc: 22.222, mean grad: 0.000009\n",
      "training loss: 1.512753 acc: 25.000, mean grad: 0.000002\n",
      "training loss: 1.627978 acc: 22.222, mean grad: 0.000010\n",
      "training loss: 1.517846 acc: 31.250, mean grad: 0.000002\n",
      "training loss: 1.601890 acc: 22.222, mean grad: 0.000011\n",
      "training loss: 1.522535 acc: 31.250, mean grad: 0.000001\n",
      "training loss: 1.579172 acc: 22.222, mean grad: 0.000011\n",
      "training loss: 1.526761 acc: 25.000, mean grad: 0.000000\n",
      "training loss: 1.559462 acc: 44.444, mean grad: 0.000014\n",
      "training loss: 1.591564 acc: 25.000, mean grad: -0.000051\n",
      "training loss: 1.816568 acc: 11.111, mean grad: 0.000019\n",
      "training loss: 1.594267 acc: 25.000, mean grad: -0.000051\n",
      "training loss: 1.771918 acc: 33.333, mean grad: 0.000018\n",
      "training loss: 1.593622 acc: 25.000, mean grad: -0.000048\n",
      "training loss: 1.727263 acc: 33.333, mean grad: 0.000018\n",
      "training loss: 1.592970 acc: 31.250, mean grad: -0.000047\n",
      "training loss: 1.685141 acc: 33.333, mean grad: 0.000018\n",
      "training loss: 1.592459 acc: 31.250, mean grad: -0.000042\n",
      "training loss: 1.647191 acc: 33.333, mean grad: 0.000020\n",
      "training loss: 1.592247 acc: 31.250, mean grad: -0.000042\n",
      "training loss: 1.613776 acc: 33.333, mean grad: 0.000019\n",
      "training loss: 1.592179 acc: 31.250, mean grad: -0.000039\n",
      "training loss: 1.584806 acc: 33.333, mean grad: 0.000020\n",
      "training loss: 1.592281 acc: 25.000, mean grad: -0.000036\n",
      "training loss: 1.559829 acc: 33.333, mean grad: 0.000017\n",
      "training loss: 1.534902 acc: 31.250, mean grad: 0.000027\n",
      "training loss: 1.661452 acc:  0.000, mean grad: -0.000029\n",
      "training loss: 1.541372 acc: 18.750, mean grad: 0.000026\n",
      "training loss: 1.623455 acc:  0.000, mean grad: -0.000031\n",
      "training loss: 1.545294 acc: 25.000, mean grad: 0.000026\n",
      "training loss: 1.585729 acc: 11.111, mean grad: -0.000034\n",
      "training loss: 1.548720 acc: 18.750, mean grad: 0.000025\n",
      "training loss: 1.550625 acc: 22.222, mean grad: -0.000037\n",
      "training loss: 1.551749 acc: 18.750, mean grad: 0.000025\n",
      "training loss: 1.519527 acc: 44.444, mean grad: -0.000036\n",
      "training loss: 1.554257 acc: 25.000, mean grad: 0.000024\n",
      "training loss: 1.492753 acc: 55.556, mean grad: -0.000034\n",
      "training loss: 1.556318 acc: 25.000, mean grad: 0.000025\n",
      "training loss: 1.469965 acc: 55.556, mean grad: -0.000036\n",
      "training loss: 1.558039 acc: 25.000, mean grad: 0.000024\n",
      "training loss: 1.450774 acc: 55.556, mean grad: -0.000039\n",
      "training loss: 1.617876 acc: 18.750, mean grad: -0.000013\n",
      "training loss: 1.763016 acc:  0.000, mean grad: 0.000015\n",
      "training loss: 1.620219 acc: 18.750, mean grad: -0.000014\n",
      "training loss: 1.731250 acc:  0.000, mean grad: 0.000014\n",
      "training loss: 1.619407 acc: 18.750, mean grad: -0.000011\n",
      "training loss: 1.698926 acc:  0.000, mean grad: 0.000012\n",
      "training loss: 1.618283 acc: 18.750, mean grad: -0.000012\n",
      "training loss: 1.668365 acc: 22.222, mean grad: 0.000014\n",
      "training loss: 1.617078 acc: 12.500, mean grad: -0.000012\n",
      "training loss: 1.640770 acc: 22.222, mean grad: 0.000013\n",
      "training loss: 1.615824 acc:  6.250, mean grad: -0.000011\n",
      "training loss: 1.616642 acc: 22.222, mean grad: 0.000013\n",
      "training loss: 1.614555 acc:  6.250, mean grad: -0.000013\n",
      "training loss: 1.595860 acc: 33.333, mean grad: 0.000013\n",
      "training loss: 1.613385 acc:  6.250, mean grad: -0.000014\n",
      "training loss: 1.578001 acc: 33.333, mean grad: 0.000011\n",
      "training loss: 1.577013 acc: 25.000, mean grad: -0.000014\n",
      "training loss: 1.735724 acc:  0.000, mean grad: 0.000005\n",
      "training loss: 1.581145 acc: 25.000, mean grad: -0.000012\n",
      "training loss: 1.688789 acc:  0.000, mean grad: 0.000005\n",
      "training loss: 1.582647 acc: 25.000, mean grad: -0.000012\n",
      "training loss: 1.641479 acc: 11.111, mean grad: 0.000003\n",
      "training loss: 1.584334 acc: 25.000, mean grad: -0.000013\n",
      "training loss: 1.596935 acc: 22.222, mean grad: -0.000001\n",
      "training loss: 1.586034 acc: 25.000, mean grad: -0.000013\n",
      "training loss: 1.557233 acc: 22.222, mean grad: -0.000002\n",
      "training loss: 1.587747 acc: 18.750, mean grad: -0.000013\n",
      "training loss: 1.522951 acc: 44.444, mean grad: -0.000004\n",
      "training loss: 1.589393 acc: 12.500, mean grad: -0.000011\n",
      "training loss: 1.493816 acc: 44.444, mean grad: 0.000001\n",
      "training loss: 1.590922 acc: 12.500, mean grad: -0.000012\n",
      "training loss: 1.469217 acc: 66.667, mean grad: 0.000001\n",
      "training loss: 1.520782 acc: 31.250, mean grad: 0.000006\n",
      "training loss: 1.706200 acc:  0.000, mean grad: 0.000003\n",
      "training loss: 1.527308 acc: 37.500, mean grad: 0.000005\n",
      "training loss: 1.667150 acc: 22.222, mean grad: 0.000007\n",
      "training loss: 1.531662 acc: 37.500, mean grad: 0.000003\n",
      "training loss: 1.627466 acc: 22.222, mean grad: 0.000003\n",
      "training loss: 1.535664 acc: 37.500, mean grad: 0.000003\n",
      "training loss: 1.589752 acc: 33.333, mean grad: 0.000004\n",
      "training loss: 1.539346 acc: 37.500, mean grad: 0.000004\n",
      "training loss: 1.556237 acc: 33.333, mean grad: 0.000006\n",
      "training loss: 1.542605 acc: 43.750, mean grad: 0.000002\n",
      "training loss: 1.527275 acc: 44.444, mean grad: 0.000004\n",
      "training loss: 1.545477 acc: 37.500, mean grad: 0.000002\n",
      "training loss: 1.502639 acc: 44.444, mean grad: 0.000003\n",
      "training loss: 1.548048 acc: 31.250, mean grad: 0.000001\n",
      "training loss: 1.481887 acc: 44.444, mean grad: -0.000001\n",
      "training loss: 1.488521 acc: 37.500, mean grad: 0.000008\n",
      "training loss: 1.780286 acc:  0.000, mean grad: -0.000006\n",
      "training loss: 1.495676 acc: 37.500, mean grad: 0.000006\n",
      "training loss: 1.738380 acc:  0.000, mean grad: -0.000009\n",
      "training loss: 1.500538 acc: 37.500, mean grad: 0.000007\n",
      "training loss: 1.695365 acc:  0.000, mean grad: -0.000012\n",
      "training loss: 1.505027 acc: 50.000, mean grad: 0.000003\n",
      "training loss: 1.654724 acc:  0.000, mean grad: -0.000018\n",
      "training loss: 1.509346 acc: 50.000, mean grad: -0.000000\n",
      "training loss: 1.618860 acc:  0.000, mean grad: -0.000016\n",
      "training loss: 1.513206 acc: 43.750, mean grad: -0.000001\n",
      "training loss: 1.587685 acc:  0.000, mean grad: -0.000014\n",
      "training loss: 1.516535 acc: 43.750, mean grad: -0.000001\n",
      "training loss: 1.561142 acc: 22.222, mean grad: -0.000014\n",
      "training loss: 1.519535 acc: 43.750, mean grad: -0.000003\n",
      "training loss: 1.538652 acc: 22.222, mean grad: -0.000015\n",
      "training loss: 1.601985 acc: 31.250, mean grad: -0.000019\n",
      "training loss: 1.867261 acc:  0.000, mean grad: 0.000012\n",
      "training loss: 1.605873 acc: 25.000, mean grad: -0.000019\n",
      "training loss: 1.812644 acc:  0.000, mean grad: 0.000009\n",
      "training loss: 1.606323 acc: 25.000, mean grad: -0.000019\n",
      "training loss: 1.756711 acc:  0.000, mean grad: 0.000006\n",
      "training loss: 1.606423 acc: 25.000, mean grad: -0.000016\n",
      "training loss: 1.703989 acc:  0.000, mean grad: 0.000006\n",
      "training loss: 1.606232 acc: 25.000, mean grad: -0.000014\n",
      "training loss: 1.657320 acc:  0.000, mean grad: 0.000004\n",
      "training loss: 1.606017 acc: 25.000, mean grad: -0.000013\n",
      "training loss: 1.617453 acc: 11.111, mean grad: 0.000002\n",
      "training loss: 1.605765 acc: 31.250, mean grad: -0.000012\n",
      "training loss: 1.583814 acc: 22.222, mean grad: -0.000001\n",
      "training loss: 1.605354 acc: 31.250, mean grad: -0.000010\n",
      "training loss: 1.555590 acc: 33.333, mean grad: -0.000002\n",
      "training loss: 1.696591 acc: 18.750, mean grad: 0.000005\n",
      "training loss: 1.844055 acc: 11.111, mean grad: -0.000001\n",
      "training loss: 1.692040 acc: 12.500, mean grad: 0.000006\n",
      "training loss: 1.794806 acc: 11.111, mean grad: -0.000002\n",
      "training loss: 1.682105 acc: 12.500, mean grad: 0.000006\n",
      "training loss: 1.743107 acc: 11.111, mean grad: -0.000001\n",
      "training loss: 1.671840 acc: 12.500, mean grad: 0.000006\n",
      "training loss: 1.693573 acc: 11.111, mean grad: 0.000000\n",
      "training loss: 1.662231 acc: 12.500, mean grad: 0.000006\n",
      "training loss: 1.649461 acc: 11.111, mean grad: 0.000001\n",
      "training loss: 1.653669 acc: 12.500, mean grad: 0.000004\n",
      "training loss: 1.611472 acc: 22.222, mean grad: 0.000002\n",
      "training loss: 1.646191 acc: 12.500, mean grad: 0.000005\n",
      "training loss: 1.579058 acc: 22.222, mean grad: 0.000002\n",
      "training loss: 1.639700 acc:  6.250, mean grad: 0.000006\n",
      "training loss: 1.551703 acc: 22.222, mean grad: 0.000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.591355 acc: 18.750, mean grad: -0.000011\n",
      "training loss: 1.727767 acc: 11.111, mean grad: -0.000036\n",
      "training loss: 1.592482 acc: 18.750, mean grad: -0.000012\n",
      "training loss: 1.696572 acc: 11.111, mean grad: -0.000038\n",
      "training loss: 1.590141 acc: 18.750, mean grad: -0.000017\n",
      "training loss: 1.663350 acc: 11.111, mean grad: -0.000027\n",
      "training loss: 1.587325 acc: 18.750, mean grad: -0.000016\n",
      "training loss: 1.631685 acc: 11.111, mean grad: -0.000026\n",
      "training loss: 1.584625 acc: 18.750, mean grad: -0.000017\n",
      "training loss: 1.603289 acc: 22.222, mean grad: -0.000025\n",
      "training loss: 1.582257 acc: 18.750, mean grad: -0.000015\n",
      "training loss: 1.578854 acc: 22.222, mean grad: -0.000021\n",
      "training loss: 1.580306 acc: 18.750, mean grad: -0.000015\n",
      "training loss: 1.558269 acc: 33.333, mean grad: -0.000019\n",
      "training loss: 1.578660 acc: 18.750, mean grad: -0.000016\n",
      "training loss: 1.541076 acc: 33.333, mean grad: -0.000016\n",
      "training loss: 1.447617 acc: 43.750, mean grad: -0.000001\n",
      "training loss: 1.807364 acc:  0.000, mean grad: 0.000012\n",
      "training loss: 1.461689 acc: 50.000, mean grad: -0.000001\n",
      "training loss: 1.755035 acc:  0.000, mean grad: 0.000011\n",
      "training loss: 1.475320 acc: 43.750, mean grad: -0.000001\n",
      "training loss: 1.699774 acc: 22.222, mean grad: 0.000011\n",
      "training loss: 1.488866 acc: 43.750, mean grad: -0.000002\n",
      "training loss: 1.646686 acc: 44.444, mean grad: 0.000011\n",
      "training loss: 1.501283 acc: 43.750, mean grad: -0.000003\n",
      "training loss: 1.599757 acc: 44.444, mean grad: 0.000010\n",
      "training loss: 1.512314 acc: 37.500, mean grad: -0.000003\n",
      "training loss: 1.559979 acc: 55.556, mean grad: 0.000011\n",
      "training loss: 1.521939 acc: 37.500, mean grad: -0.000002\n",
      "training loss: 1.526783 acc: 55.556, mean grad: 0.000009\n",
      "training loss: 1.530372 acc: 37.500, mean grad: -0.000001\n",
      "training loss: 1.499415 acc: 55.556, mean grad: 0.000007\n",
      "training loss: 1.656532 acc: 12.500, mean grad: -0.000053\n",
      "training loss: 1.812793 acc: 11.111, mean grad: -0.000004\n",
      "training loss: 1.658977 acc: 12.500, mean grad: -0.000052\n",
      "training loss: 1.772836 acc: 11.111, mean grad: -0.000006\n",
      "training loss: 1.657388 acc: 12.500, mean grad: -0.000050\n",
      "training loss: 1.729681 acc: 11.111, mean grad: -0.000006\n",
      "training loss: 1.654831 acc: 12.500, mean grad: -0.000047\n",
      "training loss: 1.687988 acc: 11.111, mean grad: -0.000008\n",
      "training loss: 1.651813 acc: 12.500, mean grad: -0.000045\n",
      "training loss: 1.650944 acc: 11.111, mean grad: -0.000011\n",
      "training loss: 1.648709 acc: 12.500, mean grad: -0.000040\n",
      "training loss: 1.619379 acc: 11.111, mean grad: -0.000014\n",
      "training loss: 1.645773 acc: 12.500, mean grad: -0.000038\n",
      "training loss: 1.592871 acc: 11.111, mean grad: -0.000018\n",
      "training loss: 1.643030 acc: 12.500, mean grad: -0.000034\n",
      "training loss: 1.570867 acc: 33.333, mean grad: -0.000017\n",
      "training loss: 1.599855 acc: 18.750, mean grad: 0.000017\n",
      "training loss: 1.838072 acc:  0.000, mean grad: -0.000020\n",
      "training loss: 1.605119 acc: 12.500, mean grad: 0.000016\n",
      "training loss: 1.785037 acc:  0.000, mean grad: -0.000019\n",
      "training loss: 1.608195 acc: 12.500, mean grad: 0.000014\n",
      "training loss: 1.729327 acc:  0.000, mean grad: -0.000021\n",
      "training loss: 1.611251 acc: 12.500, mean grad: 0.000012\n",
      "training loss: 1.676981 acc:  0.000, mean grad: -0.000019\n",
      "training loss: 1.614074 acc: 12.500, mean grad: 0.000008\n",
      "training loss: 1.631914 acc: 22.222, mean grad: -0.000019\n",
      "training loss: 1.616502 acc: 18.750, mean grad: 0.000006\n",
      "training loss: 1.594745 acc: 22.222, mean grad: -0.000016\n",
      "training loss: 1.618457 acc: 12.500, mean grad: 0.000004\n",
      "training loss: 1.564585 acc: 22.222, mean grad: -0.000018\n",
      "training loss: 1.619995 acc: 12.500, mean grad: 0.000002\n",
      "training loss: 1.540199 acc: 22.222, mean grad: -0.000019\n",
      "training loss: 1.579386 acc: 25.000, mean grad: 0.000004\n",
      "training loss: 1.889719 acc:  0.000, mean grad: 0.000026\n",
      "training loss: 1.584657 acc: 25.000, mean grad: 0.000002\n",
      "training loss: 1.833350 acc:  0.000, mean grad: 0.000027\n",
      "training loss: 1.586287 acc: 25.000, mean grad: 0.000002\n",
      "training loss: 1.772960 acc:  0.000, mean grad: 0.000019\n",
      "training loss: 1.587492 acc: 31.250, mean grad: 0.000002\n",
      "training loss: 1.715422 acc:  0.000, mean grad: 0.000017\n",
      "training loss: 1.588311 acc: 25.000, mean grad: -0.000000\n",
      "training loss: 1.665503 acc:  0.000, mean grad: 0.000013\n",
      "training loss: 1.588737 acc: 25.000, mean grad: -0.000002\n",
      "training loss: 1.624091 acc: 11.111, mean grad: 0.000014\n",
      "training loss: 1.589147 acc: 18.750, mean grad: -0.000001\n",
      "training loss: 1.590261 acc: 22.222, mean grad: 0.000011\n",
      "training loss: 1.589341 acc: 18.750, mean grad: -0.000002\n",
      "training loss: 1.562835 acc: 44.444, mean grad: 0.000010\n",
      "training loss: 1.543911 acc: 31.250, mean grad: 0.000033\n",
      "training loss: 1.829118 acc:  0.000, mean grad: -0.000026\n",
      "training loss: 1.549931 acc: 31.250, mean grad: 0.000032\n",
      "training loss: 1.771258 acc:  0.000, mean grad: -0.000024\n",
      "training loss: 1.552628 acc: 31.250, mean grad: 0.000032\n",
      "training loss: 1.710190 acc:  0.000, mean grad: -0.000023\n",
      "training loss: 1.554800 acc: 37.500, mean grad: 0.000029\n",
      "training loss: 1.653095 acc: 11.111, mean grad: -0.000021\n",
      "training loss: 1.556931 acc: 37.500, mean grad: 0.000026\n",
      "training loss: 1.604426 acc: 22.222, mean grad: -0.000020\n",
      "training loss: 1.558868 acc: 25.000, mean grad: 0.000023\n",
      "training loss: 1.564750 acc: 33.333, mean grad: -0.000021\n",
      "training loss: 1.560628 acc: 25.000, mean grad: 0.000020\n",
      "training loss: 1.532916 acc: 44.444, mean grad: -0.000021\n",
      "training loss: 1.562227 acc: 25.000, mean grad: 0.000018\n",
      "training loss: 1.507619 acc: 44.444, mean grad: -0.000021\n",
      "training loss: 1.484434 acc: 25.000, mean grad: 0.000032\n",
      "training loss: 1.859216 acc:  0.000, mean grad: -0.000008\n",
      "training loss: 1.489468 acc: 31.250, mean grad: 0.000029\n",
      "training loss: 1.802717 acc:  0.000, mean grad: -0.000010\n",
      "training loss: 1.492709 acc: 31.250, mean grad: 0.000025\n",
      "training loss: 1.741763 acc:  0.000, mean grad: -0.000012\n",
      "training loss: 1.496897 acc: 37.500, mean grad: 0.000022\n",
      "training loss: 1.683863 acc: 22.222, mean grad: -0.000015\n",
      "training loss: 1.501752 acc: 43.750, mean grad: 0.000016\n",
      "training loss: 1.634139 acc: 22.222, mean grad: -0.000016\n",
      "training loss: 1.507104 acc: 37.500, mean grad: 0.000014\n",
      "training loss: 1.593372 acc: 22.222, mean grad: -0.000017\n",
      "training loss: 1.512573 acc: 31.250, mean grad: 0.000010\n",
      "training loss: 1.560509 acc: 22.222, mean grad: -0.000019\n",
      "training loss: 1.517981 acc: 31.250, mean grad: 0.000009\n",
      "training loss: 1.534305 acc: 33.333, mean grad: -0.000022\n",
      "training loss: 1.388210 acc: 43.750, mean grad: 0.000007\n",
      "training loss: 1.919762 acc:  0.000, mean grad: 0.000028\n",
      "training loss: 1.404924 acc: 43.750, mean grad: 0.000004\n",
      "training loss: 1.855153 acc:  0.000, mean grad: 0.000026\n",
      "training loss: 1.422793 acc: 43.750, mean grad: 0.000003\n",
      "training loss: 1.784042 acc:  0.000, mean grad: 0.000022\n",
      "training loss: 1.441223 acc: 43.750, mean grad: 0.000002\n",
      "training loss: 1.715448 acc:  0.000, mean grad: 0.000018\n",
      "training loss: 1.458354 acc: 43.750, mean grad: 0.000002\n",
      "training loss: 1.656266 acc: 11.111, mean grad: 0.000016\n",
      "training loss: 1.473681 acc: 37.500, mean grad: -0.000000\n",
      "training loss: 1.607578 acc: 22.222, mean grad: 0.000015\n",
      "training loss: 1.487062 acc: 37.500, mean grad: -0.000001\n",
      "training loss: 1.568259 acc: 22.222, mean grad: 0.000014\n",
      "training loss: 1.498666 acc: 37.500, mean grad: -0.000001\n",
      "training loss: 1.536706 acc: 55.556, mean grad: 0.000011\n",
      "training loss: 1.577491 acc: 31.250, mean grad: 0.000011\n",
      "training loss: 1.939820 acc:  0.000, mean grad: -0.000028\n",
      "training loss: 1.577880 acc: 31.250, mean grad: 0.000009\n",
      "training loss: 1.883775 acc:  0.000, mean grad: -0.000029\n",
      "training loss: 1.573959 acc: 25.000, mean grad: 0.000006\n",
      "training loss: 1.822102 acc:  0.000, mean grad: -0.000026\n",
      "training loss: 1.569884 acc: 25.000, mean grad: 0.000004\n",
      "training loss: 1.763714 acc:  0.000, mean grad: -0.000024\n",
      "training loss: 1.566340 acc: 25.000, mean grad: 0.000003\n",
      "training loss: 1.714075 acc:  0.000, mean grad: -0.000023\n",
      "training loss: 1.563725 acc: 31.250, mean grad: 0.000001\n",
      "training loss: 1.673532 acc:  0.000, mean grad: -0.000022\n",
      "training loss: 1.561896 acc: 31.250, mean grad: 0.000000\n",
      "training loss: 1.640731 acc: 11.111, mean grad: -0.000020\n",
      "training loss: 1.560610 acc: 31.250, mean grad: -0.000001\n",
      "training loss: 1.614332 acc: 11.111, mean grad: -0.000017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.610139 acc: 18.750, mean grad: 0.000013\n",
      "training loss: 1.785828 acc: 22.222, mean grad: -0.000010\n",
      "training loss: 1.611970 acc: 12.500, mean grad: 0.000010\n",
      "training loss: 1.723380 acc: 33.333, mean grad: -0.000011\n",
      "training loss: 1.610051 acc: 12.500, mean grad: 0.000009\n",
      "training loss: 1.655398 acc: 44.444, mean grad: -0.000009\n",
      "training loss: 1.608013 acc: 12.500, mean grad: 0.000006\n",
      "training loss: 1.592206 acc: 44.444, mean grad: -0.000012\n",
      "training loss: 1.606465 acc:  6.250, mean grad: 0.000004\n",
      "training loss: 1.540340 acc: 44.444, mean grad: -0.000013\n",
      "training loss: 1.605395 acc: 12.500, mean grad: 0.000003\n",
      "training loss: 1.499936 acc: 44.444, mean grad: -0.000015\n",
      "training loss: 1.604607 acc:  6.250, mean grad: 0.000002\n",
      "training loss: 1.469120 acc: 55.556, mean grad: -0.000015\n",
      "training loss: 1.603959 acc: 12.500, mean grad: 0.000002\n",
      "training loss: 1.445957 acc: 55.556, mean grad: -0.000015\n",
      "training loss: 1.482372 acc: 31.250, mean grad: -0.000014\n",
      "training loss: 1.923406 acc:  0.000, mean grad: 0.000036\n",
      "training loss: 1.494490 acc: 31.250, mean grad: -0.000015\n",
      "training loss: 1.858273 acc:  0.000, mean grad: 0.000030\n",
      "training loss: 1.507355 acc: 31.250, mean grad: -0.000015\n",
      "training loss: 1.785885 acc:  0.000, mean grad: 0.000027\n",
      "training loss: 1.520780 acc: 31.250, mean grad: -0.000016\n",
      "training loss: 1.717989 acc:  0.000, mean grad: 0.000022\n",
      "training loss: 1.533174 acc: 31.250, mean grad: -0.000016\n",
      "training loss: 1.661955 acc: 11.111, mean grad: 0.000018\n",
      "training loss: 1.544021 acc: 31.250, mean grad: -0.000017\n",
      "training loss: 1.618029 acc: 11.111, mean grad: 0.000014\n",
      "training loss: 1.553248 acc: 31.250, mean grad: -0.000018\n",
      "training loss: 1.584096 acc: 22.222, mean grad: 0.000012\n",
      "training loss: 1.561064 acc: 31.250, mean grad: -0.000018\n",
      "training loss: 1.558033 acc: 22.222, mean grad: 0.000010\n",
      "training loss: 1.489640 acc: 25.000, mean grad: 0.000028\n",
      "training loss: 1.830335 acc:  0.000, mean grad: 0.000002\n",
      "training loss: 1.501699 acc: 31.250, mean grad: 0.000025\n",
      "training loss: 1.774900 acc:  0.000, mean grad: 0.000004\n",
      "training loss: 1.514355 acc: 31.250, mean grad: 0.000023\n",
      "training loss: 1.712583 acc:  0.000, mean grad: 0.000004\n",
      "training loss: 1.527303 acc: 37.500, mean grad: 0.000018\n",
      "training loss: 1.654418 acc:  0.000, mean grad: 0.000001\n",
      "training loss: 1.538964 acc: 37.500, mean grad: 0.000012\n",
      "training loss: 1.607145 acc: 11.111, mean grad: 0.000000\n",
      "training loss: 1.549020 acc: 37.500, mean grad: 0.000011\n",
      "training loss: 1.570685 acc: 22.222, mean grad: -0.000001\n",
      "training loss: 1.557453 acc: 37.500, mean grad: 0.000007\n",
      "training loss: 1.543375 acc: 33.333, mean grad: -0.000002\n",
      "training loss: 1.564559 acc: 43.750, mean grad: 0.000003\n",
      "training loss: 1.523032 acc: 44.444, mean grad: 0.000000\n",
      "training loss: 1.483572 acc: 37.500, mean grad: -0.000014\n",
      "training loss: 1.826887 acc:  0.000, mean grad: -0.000014\n",
      "training loss: 1.488502 acc: 37.500, mean grad: -0.000017\n",
      "training loss: 1.767619 acc:  0.000, mean grad: -0.000016\n",
      "training loss: 1.490702 acc: 37.500, mean grad: -0.000018\n",
      "training loss: 1.700426 acc:  0.000, mean grad: -0.000016\n",
      "training loss: 1.493708 acc: 43.750, mean grad: -0.000019\n",
      "training loss: 1.637776 acc:  0.000, mean grad: -0.000017\n",
      "training loss: 1.497471 acc: 50.000, mean grad: -0.000021\n",
      "training loss: 1.586984 acc: 11.111, mean grad: -0.000018\n",
      "training loss: 1.501828 acc: 62.500, mean grad: -0.000021\n",
      "training loss: 1.547956 acc: 33.333, mean grad: -0.000018\n",
      "training loss: 1.506281 acc: 62.500, mean grad: -0.000022\n",
      "training loss: 1.518591 acc: 33.333, mean grad: -0.000015\n",
      "training loss: 1.510649 acc: 50.000, mean grad: -0.000022\n",
      "training loss: 1.496644 acc: 44.444, mean grad: -0.000016\n",
      "training loss: 1.502741 acc: 31.250, mean grad: 0.000022\n",
      "training loss: 1.917907 acc:  0.000, mean grad: -0.000021\n",
      "training loss: 1.515200 acc: 31.250, mean grad: 0.000020\n",
      "training loss: 1.856474 acc:  0.000, mean grad: -0.000024\n",
      "training loss: 1.528645 acc: 31.250, mean grad: 0.000018\n",
      "training loss: 1.785246 acc:  0.000, mean grad: -0.000024\n",
      "training loss: 1.542132 acc: 31.250, mean grad: 0.000015\n",
      "training loss: 1.717754 acc: 22.222, mean grad: -0.000024\n",
      "training loss: 1.553770 acc: 31.250, mean grad: 0.000013\n",
      "training loss: 1.662250 acc: 22.222, mean grad: -0.000025\n",
      "training loss: 1.563396 acc: 31.250, mean grad: 0.000010\n",
      "training loss: 1.618826 acc: 33.333, mean grad: -0.000025\n",
      "training loss: 1.571329 acc: 31.250, mean grad: 0.000009\n",
      "training loss: 1.585378 acc: 33.333, mean grad: -0.000025\n",
      "training loss: 1.577785 acc: 31.250, mean grad: 0.000009\n",
      "training loss: 1.559758 acc: 33.333, mean grad: -0.000024\n",
      "training loss: 1.518277 acc: 31.250, mean grad: -0.000012\n",
      "training loss: 2.027803 acc:  0.000, mean grad: -0.000023\n",
      "training loss: 1.529725 acc: 31.250, mean grad: -0.000010\n",
      "training loss: 1.953665 acc:  0.000, mean grad: -0.000028\n",
      "training loss: 1.540239 acc: 31.250, mean grad: -0.000008\n",
      "training loss: 1.866674 acc:  0.000, mean grad: -0.000028\n",
      "training loss: 1.549987 acc: 31.250, mean grad: -0.000011\n",
      "training loss: 1.783974 acc:  0.000, mean grad: -0.000025\n",
      "training loss: 1.557948 acc: 25.000, mean grad: -0.000010\n",
      "training loss: 1.716525 acc:  0.000, mean grad: -0.000025\n",
      "training loss: 1.564483 acc: 25.000, mean grad: -0.000009\n",
      "training loss: 1.664079 acc: 11.111, mean grad: -0.000027\n",
      "training loss: 1.569986 acc: 25.000, mean grad: -0.000009\n",
      "training loss: 1.624094 acc: 22.222, mean grad: -0.000024\n",
      "training loss: 1.574472 acc: 31.250, mean grad: -0.000009\n",
      "training loss: 1.593597 acc: 22.222, mean grad: -0.000025\n",
      "training loss: 1.465917 acc: 43.750, mean grad: 0.000010\n",
      "training loss: 1.913057 acc:  0.000, mean grad: -0.000019\n",
      "training loss: 1.480394 acc: 37.500, mean grad: 0.000008\n",
      "training loss: 1.842476 acc:  0.000, mean grad: -0.000020\n",
      "training loss: 1.497427 acc: 37.500, mean grad: 0.000006\n",
      "training loss: 1.759845 acc:  0.000, mean grad: -0.000020\n",
      "training loss: 1.515453 acc: 37.500, mean grad: 0.000006\n",
      "training loss: 1.682207 acc: 11.111, mean grad: -0.000020\n",
      "training loss: 1.531694 acc: 37.500, mean grad: 0.000005\n",
      "training loss: 1.619877 acc: 22.222, mean grad: -0.000018\n",
      "training loss: 1.545672 acc: 43.750, mean grad: 0.000005\n",
      "training loss: 1.572447 acc: 33.333, mean grad: -0.000018\n",
      "training loss: 1.557384 acc: 43.750, mean grad: 0.000005\n",
      "training loss: 1.537019 acc: 44.444, mean grad: -0.000017\n",
      "training loss: 1.567134 acc: 43.750, mean grad: 0.000005\n",
      "training loss: 1.510853 acc: 66.667, mean grad: -0.000018\n",
      "training loss: 1.431462 acc: 43.750, mean grad: 0.000012\n",
      "training loss: 2.038463 acc:  0.000, mean grad: -0.000043\n",
      "training loss: 1.440958 acc: 43.750, mean grad: 0.000009\n",
      "training loss: 1.955968 acc:  0.000, mean grad: -0.000042\n",
      "training loss: 1.451291 acc: 50.000, mean grad: 0.000004\n",
      "training loss: 1.859532 acc:  0.000, mean grad: -0.000037\n",
      "training loss: 1.462813 acc: 50.000, mean grad: 0.000002\n",
      "training loss: 1.769657 acc: 11.111, mean grad: -0.000032\n",
      "training loss: 1.473935 acc: 56.250, mean grad: -0.000003\n",
      "training loss: 1.697204 acc: 11.111, mean grad: -0.000029\n",
      "training loss: 1.484247 acc: 56.250, mean grad: -0.000004\n",
      "training loss: 1.641840 acc: 22.222, mean grad: -0.000026\n",
      "training loss: 1.493558 acc: 56.250, mean grad: -0.000008\n",
      "training loss: 1.599924 acc: 33.333, mean grad: -0.000023\n",
      "training loss: 1.501765 acc: 56.250, mean grad: -0.000010\n",
      "training loss: 1.568526 acc: 33.333, mean grad: -0.000022\n",
      "training loss: 1.525540 acc: 25.000, mean grad: -0.000002\n",
      "training loss: 1.976246 acc:  0.000, mean grad: -0.000010\n",
      "training loss: 1.529026 acc: 31.250, mean grad: -0.000003\n",
      "training loss: 1.905816 acc:  0.000, mean grad: -0.000013\n",
      "training loss: 1.529471 acc: 31.250, mean grad: -0.000004\n",
      "training loss: 1.822989 acc:  0.000, mean grad: -0.000013\n",
      "training loss: 1.530391 acc: 31.250, mean grad: -0.000001\n",
      "training loss: 1.746499 acc:  0.000, mean grad: -0.000012\n",
      "training loss: 1.532172 acc: 31.250, mean grad: -0.000000\n",
      "training loss: 1.685714 acc: 11.111, mean grad: -0.000012\n",
      "training loss: 1.534534 acc: 43.750, mean grad: -0.000002\n",
      "training loss: 1.639384 acc: 11.111, mean grad: -0.000012\n",
      "training loss: 1.537160 acc: 37.500, mean grad: -0.000003\n",
      "training loss: 1.604448 acc: 11.111, mean grad: -0.000011\n",
      "training loss: 1.539688 acc: 37.500, mean grad: -0.000003\n",
      "training loss: 1.578107 acc: 11.111, mean grad: -0.000012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.532235 acc:  6.250, mean grad: 0.000006\n",
      "training loss: 1.982100 acc:  0.000, mean grad: -0.000010\n",
      "training loss: 1.541743 acc:  6.250, mean grad: 0.000006\n",
      "training loss: 1.901576 acc:  0.000, mean grad: -0.000012\n",
      "training loss: 1.550262 acc:  6.250, mean grad: 0.000007\n",
      "training loss: 1.807729 acc:  0.000, mean grad: -0.000009\n",
      "training loss: 1.558597 acc: 12.500, mean grad: 0.000007\n",
      "training loss: 1.722669 acc:  0.000, mean grad: -0.000011\n",
      "training loss: 1.566021 acc: 12.500, mean grad: 0.000006\n",
      "training loss: 1.657039 acc:  0.000, mean grad: -0.000011\n",
      "training loss: 1.572283 acc: 12.500, mean grad: 0.000007\n",
      "training loss: 1.608618 acc:  0.000, mean grad: -0.000011\n",
      "training loss: 1.577447 acc: 12.500, mean grad: 0.000006\n",
      "training loss: 1.573421 acc: 22.222, mean grad: -0.000011\n",
      "training loss: 1.581637 acc: 18.750, mean grad: 0.000006\n",
      "training loss: 1.548010 acc: 44.444, mean grad: -0.000010\n",
      "training loss: 1.424664 acc: 37.500, mean grad: 0.000005\n",
      "training loss: 1.993947 acc:  0.000, mean grad: -0.000027\n",
      "training loss: 1.439658 acc: 37.500, mean grad: 0.000008\n",
      "training loss: 1.920452 acc:  0.000, mean grad: -0.000030\n",
      "training loss: 1.457207 acc: 37.500, mean grad: 0.000008\n",
      "training loss: 1.832437 acc:  0.000, mean grad: -0.000029\n",
      "training loss: 1.475527 acc: 37.500, mean grad: 0.000006\n",
      "training loss: 1.750098 acc: 22.222, mean grad: -0.000028\n",
      "training loss: 1.491764 acc: 37.500, mean grad: 0.000005\n",
      "training loss: 1.684792 acc: 33.333, mean grad: -0.000029\n",
      "training loss: 1.505558 acc: 37.500, mean grad: 0.000006\n",
      "training loss: 1.635504 acc: 33.333, mean grad: -0.000028\n",
      "training loss: 1.517172 acc: 50.000, mean grad: 0.000003\n",
      "training loss: 1.598642 acc: 33.333, mean grad: -0.000026\n",
      "training loss: 1.526927 acc: 43.750, mean grad: 0.000003\n",
      "training loss: 1.571289 acc: 33.333, mean grad: -0.000025\n",
      "training loss: 1.404790 acc: 31.250, mean grad: 0.000013\n",
      "training loss: 2.149356 acc:  0.000, mean grad: 0.000016\n",
      "training loss: 1.416492 acc: 37.500, mean grad: 0.000009\n",
      "training loss: 2.049839 acc:  0.000, mean grad: 0.000014\n",
      "training loss: 1.429746 acc: 37.500, mean grad: 0.000005\n",
      "training loss: 1.931452 acc:  0.000, mean grad: 0.000010\n",
      "training loss: 1.444748 acc: 43.750, mean grad: -0.000002\n",
      "training loss: 1.821617 acc:  0.000, mean grad: 0.000005\n",
      "training loss: 1.459225 acc: 50.000, mean grad: -0.000003\n",
      "training loss: 1.734917 acc:  0.000, mean grad: 0.000004\n",
      "training loss: 1.472395 acc: 50.000, mean grad: -0.000002\n",
      "training loss: 1.669514 acc:  0.000, mean grad: -0.000000\n",
      "training loss: 1.484012 acc: 50.000, mean grad: -0.000004\n",
      "training loss: 1.620741 acc: 11.111, mean grad: -0.000003\n",
      "training loss: 1.494147 acc: 50.000, mean grad: -0.000005\n",
      "training loss: 1.584428 acc: 22.222, mean grad: -0.000004\n",
      "training loss: 1.572647 acc:  0.000, mean grad: 0.000011\n",
      "training loss: 1.985758 acc:  0.000, mean grad: 0.000001\n",
      "training loss: 1.580314 acc:  0.000, mean grad: 0.000010\n",
      "training loss: 1.903463 acc:  0.000, mean grad: -0.000001\n",
      "training loss: 1.586590 acc:  0.000, mean grad: 0.000009\n",
      "training loss: 1.806435 acc:  0.000, mean grad: -0.000004\n",
      "training loss: 1.592242 acc:  0.000, mean grad: 0.000011\n",
      "training loss: 1.718455 acc:  0.000, mean grad: -0.000003\n",
      "training loss: 1.596766 acc:  6.250, mean grad: 0.000010\n",
      "training loss: 1.650758 acc:  0.000, mean grad: -0.000003\n",
      "training loss: 1.600225 acc:  6.250, mean grad: 0.000008\n",
      "training loss: 1.600960 acc:  0.000, mean grad: -0.000003\n",
      "training loss: 1.602513 acc: 12.500, mean grad: 0.000006\n",
      "training loss: 1.564916 acc: 55.556, mean grad: -0.000002\n",
      "training loss: 1.604064 acc:  6.250, mean grad: 0.000004\n",
      "training loss: 1.538923 acc: 55.556, mean grad: -0.000004\n",
      "training loss: 1.351325 acc: 25.000, mean grad: -0.000010\n",
      "training loss: 2.052408 acc:  0.000, mean grad: 0.000021\n",
      "training loss: 1.364610 acc: 25.000, mean grad: -0.000011\n",
      "training loss: 1.963963 acc:  0.000, mean grad: 0.000021\n",
      "training loss: 1.382459 acc: 37.500, mean grad: -0.000011\n",
      "training loss: 1.858717 acc:  0.000, mean grad: 0.000019\n",
      "training loss: 1.403187 acc: 43.750, mean grad: -0.000010\n",
      "training loss: 1.763030 acc:  0.000, mean grad: 0.000015\n",
      "training loss: 1.422951 acc: 50.000, mean grad: -0.000010\n",
      "training loss: 1.689120 acc:  0.000, mean grad: 0.000012\n",
      "training loss: 1.440680 acc: 56.250, mean grad: -0.000011\n",
      "training loss: 1.634499 acc:  0.000, mean grad: 0.000012\n",
      "training loss: 1.456247 acc: 56.250, mean grad: -0.000011\n",
      "training loss: 1.594527 acc: 11.111, mean grad: 0.000008\n",
      "training loss: 1.469795 acc: 50.000, mean grad: -0.000013\n",
      "training loss: 1.565535 acc: 22.222, mean grad: 0.000005\n",
      "training loss: 1.519469 acc: 25.000, mean grad: 0.000032\n",
      "training loss: 2.077725 acc:  0.000, mean grad: 0.000013\n",
      "training loss: 1.523100 acc: 31.250, mean grad: 0.000028\n",
      "training loss: 1.980376 acc:  0.000, mean grad: 0.000012\n",
      "training loss: 1.526542 acc: 37.500, mean grad: 0.000025\n",
      "training loss: 1.867811 acc:  0.000, mean grad: 0.000008\n",
      "training loss: 1.532516 acc: 31.250, mean grad: 0.000020\n",
      "training loss: 1.769236 acc:  0.000, mean grad: 0.000004\n",
      "training loss: 1.539876 acc: 37.500, mean grad: 0.000015\n",
      "training loss: 1.695049 acc:  0.000, mean grad: 0.000002\n",
      "training loss: 1.547158 acc: 31.250, mean grad: 0.000012\n",
      "training loss: 1.641384 acc: 11.111, mean grad: 0.000001\n",
      "training loss: 1.553762 acc: 31.250, mean grad: 0.000008\n",
      "training loss: 1.602489 acc: 11.111, mean grad: 0.000001\n",
      "training loss: 1.559367 acc: 37.500, mean grad: 0.000006\n",
      "training loss: 1.574454 acc: 33.333, mean grad: 0.000001\n",
      "training loss: 1.349241 acc: 56.250, mean grad: -0.000005\n",
      "training loss: 2.118488 acc:  0.000, mean grad: -0.000002\n",
      "training loss: 1.366897 acc: 56.250, mean grad: -0.000005\n",
      "training loss: 2.009348 acc:  0.000, mean grad: -0.000007\n",
      "training loss: 1.392207 acc: 62.500, mean grad: -0.000009\n",
      "training loss: 1.879947 acc:  0.000, mean grad: -0.000005\n",
      "training loss: 1.421131 acc: 62.500, mean grad: -0.000008\n",
      "training loss: 1.765343 acc:  0.000, mean grad: -0.000006\n",
      "training loss: 1.448139 acc: 62.500, mean grad: -0.000008\n",
      "training loss: 1.679846 acc:  0.000, mean grad: -0.000002\n",
      "training loss: 1.471560 acc: 62.500, mean grad: -0.000008\n",
      "training loss: 1.618909 acc: 11.111, mean grad: -0.000005\n",
      "training loss: 1.491249 acc: 68.750, mean grad: -0.000008\n",
      "training loss: 1.575758 acc: 22.222, mean grad: -0.000006\n",
      "training loss: 1.507626 acc: 68.750, mean grad: -0.000007\n",
      "training loss: 1.545408 acc: 33.333, mean grad: -0.000005\n",
      "training loss: 1.414889 acc: 31.250, mean grad: -0.000023\n",
      "training loss: 2.197326 acc:  0.000, mean grad: 0.000011\n",
      "training loss: 1.429896 acc: 31.250, mean grad: -0.000023\n",
      "training loss: 2.067280 acc:  0.000, mean grad: 0.000009\n",
      "training loss: 1.451347 acc: 37.500, mean grad: -0.000022\n",
      "training loss: 1.913920 acc:  0.000, mean grad: 0.000006\n",
      "training loss: 1.477144 acc: 43.750, mean grad: -0.000021\n",
      "training loss: 1.778627 acc:  0.000, mean grad: 0.000002\n",
      "training loss: 1.501728 acc: 43.750, mean grad: -0.000021\n",
      "training loss: 1.678055 acc: 11.111, mean grad: -0.000001\n",
      "training loss: 1.522692 acc: 37.500, mean grad: -0.000019\n",
      "training loss: 1.606529 acc: 22.222, mean grad: -0.000002\n",
      "training loss: 1.539914 acc: 37.500, mean grad: -0.000018\n",
      "training loss: 1.556092 acc: 22.222, mean grad: -0.000006\n",
      "training loss: 1.553765 acc: 31.250, mean grad: -0.000018\n",
      "training loss: 1.520830 acc: 55.556, mean grad: -0.000007\n",
      "training loss: 1.503175 acc: 25.000, mean grad: -0.000002\n",
      "training loss: 2.193272 acc:  0.000, mean grad: 0.000003\n",
      "training loss: 1.513805 acc: 25.000, mean grad: -0.000002\n",
      "training loss: 2.079032 acc:  0.000, mean grad: 0.000001\n",
      "training loss: 1.526664 acc: 25.000, mean grad: -0.000004\n",
      "training loss: 1.942576 acc:  0.000, mean grad: -0.000001\n",
      "training loss: 1.541398 acc: 18.750, mean grad: -0.000003\n",
      "training loss: 1.820387 acc:  0.000, mean grad: -0.000000\n",
      "training loss: 1.554920 acc: 18.750, mean grad: -0.000005\n",
      "training loss: 1.727607 acc:  0.000, mean grad: 0.000000\n",
      "training loss: 1.566431 acc: 25.000, mean grad: -0.000006\n",
      "training loss: 1.659887 acc:  0.000, mean grad: -0.000001\n",
      "training loss: 1.575787 acc: 25.000, mean grad: -0.000007\n",
      "training loss: 1.610703 acc: 22.222, mean grad: -0.000004\n",
      "training loss: 1.583045 acc: 25.000, mean grad: -0.000006\n",
      "training loss: 1.575261 acc: 44.444, mean grad: -0.000004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.522079 acc: 31.250, mean grad: -0.000002\n",
      "training loss: 2.153321 acc:  0.000, mean grad: -0.000026\n",
      "training loss: 1.527368 acc: 25.000, mean grad: -0.000003\n",
      "training loss: 2.055964 acc:  0.000, mean grad: -0.000022\n",
      "training loss: 1.532482 acc: 25.000, mean grad: -0.000003\n",
      "training loss: 1.939831 acc:  0.000, mean grad: -0.000021\n",
      "training loss: 1.539938 acc: 25.000, mean grad: -0.000003\n",
      "training loss: 1.836980 acc:  0.000, mean grad: -0.000022\n",
      "training loss: 1.548025 acc: 25.000, mean grad: -0.000005\n",
      "training loss: 1.758992 acc:  0.000, mean grad: -0.000023\n",
      "training loss: 1.555808 acc: 31.250, mean grad: -0.000006\n",
      "training loss: 1.701873 acc:  0.000, mean grad: -0.000023\n",
      "training loss: 1.562608 acc: 31.250, mean grad: -0.000007\n",
      "training loss: 1.660173 acc:  0.000, mean grad: -0.000024\n",
      "training loss: 1.568331 acc: 25.000, mean grad: -0.000009\n",
      "training loss: 1.629761 acc:  0.000, mean grad: -0.000021\n",
      "training loss: 1.498671 acc: 25.000, mean grad: 0.000009\n",
      "training loss: 2.204645 acc:  0.000, mean grad: 0.000024\n",
      "training loss: 1.498905 acc: 25.000, mean grad: 0.000007\n",
      "training loss: 2.090173 acc:  0.000, mean grad: 0.000018\n",
      "training loss: 1.498311 acc: 25.000, mean grad: 0.000001\n",
      "training loss: 1.955060 acc:  0.000, mean grad: 0.000014\n",
      "training loss: 1.503329 acc: 25.000, mean grad: -0.000002\n",
      "training loss: 1.836132 acc:  0.000, mean grad: 0.000008\n",
      "training loss: 1.511935 acc: 25.000, mean grad: -0.000004\n",
      "training loss: 1.746350 acc:  0.000, mean grad: 0.000004\n",
      "training loss: 1.521629 acc: 31.250, mean grad: -0.000006\n",
      "training loss: 1.680690 acc:  0.000, mean grad: 0.000003\n",
      "training loss: 1.531086 acc: 37.500, mean grad: -0.000007\n",
      "training loss: 1.632924 acc: 11.111, mean grad: 0.000000\n",
      "training loss: 1.539557 acc: 37.500, mean grad: -0.000008\n",
      "training loss: 1.598201 acc: 22.222, mean grad: -0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[  100/  500] loss: 1.5990 (1.6155), acc: 33.333% (22.086%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.473080 acc: 50.000, mean grad: -0.000010\n",
      "training loss: 2.128849 acc:  0.000, mean grad: -0.000040\n",
      "training loss: 1.473708 acc: 50.000, mean grad: -0.000011\n",
      "training loss: 2.027520 acc:  0.000, mean grad: -0.000039\n",
      "training loss: 1.473706 acc: 50.000, mean grad: -0.000011\n",
      "training loss: 1.907136 acc:  0.000, mean grad: -0.000038\n",
      "training loss: 1.478385 acc: 43.750, mean grad: -0.000013\n",
      "training loss: 1.800863 acc:  0.000, mean grad: -0.000033\n",
      "training loss: 1.486657 acc: 43.750, mean grad: -0.000013\n",
      "training loss: 1.720634 acc:  0.000, mean grad: -0.000034\n",
      "training loss: 1.496267 acc: 43.750, mean grad: -0.000014\n",
      "training loss: 1.662460 acc: 11.111, mean grad: -0.000033\n",
      "training loss: 1.505770 acc: 37.500, mean grad: -0.000013\n",
      "training loss: 1.620565 acc: 11.111, mean grad: -0.000033\n",
      "training loss: 1.514810 acc: 43.750, mean grad: -0.000013\n",
      "training loss: 1.590402 acc: 33.333, mean grad: -0.000033\n",
      "training loss: 1.373509 acc: 31.250, mean grad: -0.000007\n",
      "training loss: 2.236552 acc:  0.000, mean grad: 0.000005\n",
      "training loss: 1.382451 acc: 37.500, mean grad: -0.000007\n",
      "training loss: 2.106187 acc:  0.000, mean grad: 0.000004\n",
      "training loss: 1.396872 acc: 43.750, mean grad: -0.000007\n",
      "training loss: 1.948287 acc:  0.000, mean grad: 0.000001\n",
      "training loss: 1.417417 acc: 43.750, mean grad: -0.000009\n",
      "training loss: 1.806018 acc:  0.000, mean grad: -0.000001\n",
      "training loss: 1.439463 acc: 50.000, mean grad: -0.000009\n",
      "training loss: 1.698441 acc:  0.000, mean grad: -0.000002\n",
      "training loss: 1.460178 acc: 56.250, mean grad: -0.000009\n",
      "training loss: 1.621027 acc: 11.111, mean grad: -0.000002\n",
      "training loss: 1.478515 acc: 68.750, mean grad: -0.000008\n",
      "training loss: 1.566032 acc: 11.111, mean grad: -0.000002\n",
      "training loss: 1.494312 acc: 62.500, mean grad: -0.000009\n",
      "training loss: 1.527216 acc: 44.444, mean grad: -0.000001\n",
      "training loss: 1.621524 acc: 18.750, mean grad: 0.000001\n",
      "training loss: 2.248201 acc:  0.000, mean grad: -0.000018\n",
      "training loss: 1.614196 acc: 18.750, mean grad: 0.000001\n",
      "training loss: 2.116301 acc:  0.000, mean grad: -0.000018\n",
      "training loss: 1.601516 acc: 18.750, mean grad: 0.000000\n",
      "training loss: 1.960149 acc:  0.000, mean grad: -0.000019\n",
      "training loss: 1.592859 acc: 18.750, mean grad: -0.000001\n",
      "training loss: 1.825108 acc:  0.000, mean grad: -0.000021\n",
      "training loss: 1.589104 acc: 18.750, mean grad: 0.000001\n",
      "training loss: 1.725252 acc: 11.111, mean grad: -0.000021\n",
      "training loss: 1.588153 acc: 12.500, mean grad: 0.000002\n",
      "training loss: 1.654059 acc: 11.111, mean grad: -0.000022\n",
      "training loss: 1.588582 acc: 18.750, mean grad: 0.000001\n",
      "training loss: 1.603555 acc: 22.222, mean grad: -0.000022\n",
      "training loss: 1.589653 acc: 12.500, mean grad: -0.000001\n",
      "training loss: 1.567824 acc: 33.333, mean grad: -0.000022\n",
      "training loss: 1.506632 acc: 18.750, mean grad: 0.000014\n",
      "training loss: 2.391639 acc:  0.000, mean grad: 0.000051\n",
      "training loss: 1.508473 acc: 25.000, mean grad: 0.000011\n",
      "training loss: 2.259119 acc:  0.000, mean grad: 0.000042\n",
      "training loss: 1.511209 acc: 25.000, mean grad: 0.000007\n",
      "training loss: 2.097726 acc:  0.000, mean grad: 0.000038\n",
      "training loss: 1.519512 acc: 25.000, mean grad: 0.000006\n",
      "training loss: 1.955602 acc:  0.000, mean grad: 0.000026\n",
      "training loss: 1.530993 acc: 25.000, mean grad: 0.000003\n",
      "training loss: 1.849274 acc:  0.000, mean grad: 0.000022\n",
      "training loss: 1.542753 acc: 31.250, mean grad: -0.000001\n",
      "training loss: 1.771682 acc:  0.000, mean grad: 0.000015\n",
      "training loss: 1.553235 acc: 31.250, mean grad: -0.000003\n",
      "training loss: 1.714735 acc:  0.000, mean grad: 0.000013\n",
      "training loss: 1.562195 acc: 37.500, mean grad: -0.000005\n",
      "training loss: 1.672925 acc:  0.000, mean grad: 0.000011\n",
      "training loss: 1.419056 acc: 31.250, mean grad: 0.000010\n",
      "training loss: 2.235730 acc:  0.000, mean grad: 0.000010\n",
      "training loss: 1.432952 acc: 31.250, mean grad: 0.000008\n",
      "training loss: 2.121678 acc:  0.000, mean grad: 0.000011\n",
      "training loss: 1.451914 acc: 31.250, mean grad: 0.000006\n",
      "training loss: 1.979577 acc:  0.000, mean grad: 0.000002\n",
      "training loss: 1.474084 acc: 31.250, mean grad: 0.000004\n",
      "training loss: 1.853187 acc:  0.000, mean grad: -0.000002\n",
      "training loss: 1.494780 acc: 31.250, mean grad: 0.000002\n",
      "training loss: 1.758864 acc:  0.000, mean grad: -0.000005\n",
      "training loss: 1.512442 acc: 31.250, mean grad: 0.000002\n",
      "training loss: 1.690892 acc: 11.111, mean grad: -0.000007\n",
      "training loss: 1.527102 acc: 25.000, mean grad: -0.000002\n",
      "training loss: 1.641911 acc: 11.111, mean grad: -0.000009\n",
      "training loss: 1.538979 acc: 31.250, mean grad: 0.000001\n",
      "training loss: 1.606835 acc: 11.111, mean grad: -0.000011\n",
      "training loss: 1.425480 acc: 43.750, mean grad: -0.000008\n",
      "training loss: 2.248305 acc:  0.000, mean grad: 0.000003\n",
      "training loss: 1.436779 acc: 37.500, mean grad: -0.000010\n",
      "training loss: 2.124012 acc:  0.000, mean grad: 0.000000\n",
      "training loss: 1.454371 acc: 37.500, mean grad: -0.000012\n",
      "training loss: 1.972343 acc:  0.000, mean grad: 0.000001\n",
      "training loss: 1.476756 acc: 37.500, mean grad: -0.000014\n",
      "training loss: 1.841412 acc:  0.000, mean grad: -0.000001\n",
      "training loss: 1.497944 acc: 37.500, mean grad: -0.000015\n",
      "training loss: 1.745846 acc:  0.000, mean grad: -0.000003\n",
      "training loss: 1.516072 acc: 37.500, mean grad: -0.000014\n",
      "training loss: 1.678709 acc: 22.222, mean grad: -0.000004\n",
      "training loss: 1.531363 acc: 31.250, mean grad: -0.000015\n",
      "training loss: 1.631788 acc: 33.333, mean grad: 0.000000\n",
      "training loss: 1.543870 acc: 31.250, mean grad: -0.000013\n",
      "training loss: 1.598626 acc: 33.333, mean grad: 0.000000\n",
      "training loss: 1.468751 acc: 31.250, mean grad: 0.000025\n",
      "training loss: 2.231699 acc:  0.000, mean grad: 0.000015\n",
      "training loss: 1.471213 acc: 37.500, mean grad: 0.000023\n",
      "training loss: 2.096794 acc:  0.000, mean grad: 0.000009\n",
      "training loss: 1.474991 acc: 37.500, mean grad: 0.000014\n",
      "training loss: 1.934471 acc:  0.000, mean grad: 0.000008\n",
      "training loss: 1.484827 acc: 31.250, mean grad: 0.000008\n",
      "training loss: 1.796873 acc:  0.000, mean grad: 0.000004\n",
      "training loss: 1.496807 acc: 37.500, mean grad: -0.000000\n",
      "training loss: 1.698431 acc: 11.111, mean grad: -0.000004\n",
      "training loss: 1.508784 acc: 31.250, mean grad: -0.000005\n",
      "training loss: 1.630175 acc: 33.333, mean grad: -0.000007\n",
      "training loss: 1.519724 acc: 31.250, mean grad: -0.000007\n",
      "training loss: 1.583214 acc: 55.556, mean grad: -0.000010\n",
      "training loss: 1.529326 acc: 37.500, mean grad: -0.000006\n",
      "training loss: 1.550956 acc: 55.556, mean grad: -0.000011\n",
      "training loss: 1.548902 acc: 18.750, mean grad: -0.000015\n",
      "training loss: 2.361996 acc:  0.000, mean grad: 0.000027\n",
      "training loss: 1.549358 acc: 18.750, mean grad: -0.000016\n",
      "training loss: 2.224111 acc:  0.000, mean grad: 0.000027\n",
      "training loss: 1.548733 acc: 18.750, mean grad: -0.000015\n",
      "training loss: 2.055882 acc:  0.000, mean grad: 0.000024\n",
      "training loss: 1.553501 acc: 18.750, mean grad: -0.000014\n",
      "training loss: 1.913285 acc:  0.000, mean grad: 0.000022\n",
      "training loss: 1.560706 acc: 18.750, mean grad: -0.000014\n",
      "training loss: 1.810234 acc:  0.000, mean grad: 0.000014\n",
      "training loss: 1.568096 acc: 12.500, mean grad: -0.000014\n",
      "training loss: 1.737251 acc:  0.000, mean grad: 0.000013\n",
      "training loss: 1.574563 acc: 25.000, mean grad: -0.000014\n",
      "training loss: 1.685557 acc: 11.111, mean grad: 0.000009\n",
      "training loss: 1.579968 acc: 25.000, mean grad: -0.000015\n",
      "training loss: 1.648623 acc: 22.222, mean grad: 0.000006\n",
      "training loss: 1.459728 acc: 25.000, mean grad: 0.000019\n",
      "training loss: 2.248196 acc:  0.000, mean grad: -0.000007\n",
      "training loss: 1.466162 acc: 31.250, mean grad: 0.000020\n",
      "training loss: 2.110117 acc:  0.000, mean grad: -0.000009\n",
      "training loss: 1.475000 acc: 37.500, mean grad: 0.000018\n",
      "training loss: 1.941979 acc:  0.000, mean grad: -0.000010\n",
      "training loss: 1.488671 acc: 25.000, mean grad: 0.000016\n",
      "training loss: 1.801599 acc:  0.000, mean grad: -0.000010\n",
      "training loss: 1.503310 acc: 25.000, mean grad: 0.000011\n",
      "training loss: 1.702743 acc: 11.111, mean grad: -0.000008\n",
      "training loss: 1.516738 acc: 31.250, mean grad: 0.000008\n",
      "training loss: 1.635027 acc: 22.222, mean grad: -0.000007\n",
      "training loss: 1.528250 acc: 25.000, mean grad: 0.000007\n",
      "training loss: 1.588910 acc: 22.222, mean grad: -0.000004\n",
      "training loss: 1.538014 acc: 25.000, mean grad: 0.000006\n",
      "training loss: 1.557653 acc: 33.333, mean grad: -0.000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.547016 acc: 12.500, mean grad: 0.000017\n",
      "training loss: 2.229977 acc:  0.000, mean grad: 0.000031\n",
      "training loss: 1.549480 acc: 12.500, mean grad: 0.000013\n",
      "training loss: 2.080208 acc:  0.000, mean grad: 0.000028\n",
      "training loss: 1.552416 acc: 12.500, mean grad: 0.000011\n",
      "training loss: 1.899441 acc:  0.000, mean grad: 0.000020\n",
      "training loss: 1.559313 acc: 12.500, mean grad: 0.000006\n",
      "training loss: 1.750306 acc:  0.000, mean grad: 0.000017\n",
      "training loss: 1.567112 acc: 18.750, mean grad: 0.000002\n",
      "training loss: 1.648193 acc: 22.222, mean grad: 0.000010\n",
      "training loss: 1.574076 acc: 18.750, mean grad: -0.000003\n",
      "training loss: 1.580710 acc: 55.556, mean grad: 0.000006\n",
      "training loss: 1.579781 acc: 12.500, mean grad: -0.000005\n",
      "training loss: 1.536412 acc: 66.667, mean grad: 0.000004\n",
      "training loss: 1.584124 acc: 18.750, mean grad: -0.000008\n",
      "training loss: 1.507735 acc: 66.667, mean grad: 0.000000\n",
      "training loss: 1.397527 acc: 43.750, mean grad: 0.000007\n",
      "training loss: 2.338806 acc:  0.000, mean grad: -0.000013\n",
      "training loss: 1.409497 acc: 43.750, mean grad: 0.000001\n",
      "training loss: 2.201167 acc:  0.000, mean grad: -0.000010\n",
      "training loss: 1.429295 acc: 43.750, mean grad: -0.000001\n",
      "training loss: 2.026027 acc:  0.000, mean grad: -0.000008\n",
      "training loss: 1.454604 acc: 50.000, mean grad: -0.000003\n",
      "training loss: 1.876536 acc:  0.000, mean grad: -0.000011\n",
      "training loss: 1.479253 acc: 50.000, mean grad: -0.000005\n",
      "training loss: 1.768917 acc:  0.000, mean grad: -0.000012\n",
      "training loss: 1.500852 acc: 50.000, mean grad: -0.000006\n",
      "training loss: 1.693938 acc:  0.000, mean grad: -0.000012\n",
      "training loss: 1.518985 acc: 43.750, mean grad: -0.000006\n",
      "training loss: 1.641640 acc:  0.000, mean grad: -0.000013\n",
      "training loss: 1.533857 acc: 31.250, mean grad: -0.000007\n",
      "training loss: 1.605416 acc: 44.444, mean grad: -0.000013\n",
      "training loss: 1.475838 acc: 31.250, mean grad: 0.000012\n",
      "training loss: 2.283547 acc:  0.000, mean grad: -0.000004\n",
      "training loss: 1.479970 acc: 31.250, mean grad: 0.000008\n",
      "training loss: 2.148577 acc:  0.000, mean grad: -0.000004\n",
      "training loss: 1.489336 acc: 31.250, mean grad: 0.000002\n",
      "training loss: 1.979737 acc:  0.000, mean grad: -0.000004\n",
      "training loss: 1.505455 acc: 31.250, mean grad: -0.000002\n",
      "training loss: 1.836388 acc:  0.000, mean grad: -0.000008\n",
      "training loss: 1.523256 acc: 37.500, mean grad: -0.000005\n",
      "training loss: 1.734600 acc: 11.111, mean grad: -0.000008\n",
      "training loss: 1.539233 acc: 31.250, mean grad: -0.000007\n",
      "training loss: 1.664654 acc: 11.111, mean grad: -0.000009\n",
      "training loss: 1.552516 acc: 37.500, mean grad: -0.000008\n",
      "training loss: 1.616586 acc: 33.333, mean grad: -0.000007\n",
      "training loss: 1.563226 acc: 37.500, mean grad: -0.000009\n",
      "training loss: 1.583656 acc: 33.333, mean grad: -0.000009\n",
      "training loss: 1.411259 acc: 25.000, mean grad: 0.000004\n",
      "training loss: 2.330410 acc:  0.000, mean grad: -0.000038\n",
      "training loss: 1.422841 acc: 31.250, mean grad: 0.000002\n",
      "training loss: 2.174500 acc:  0.000, mean grad: -0.000035\n",
      "training loss: 1.443547 acc: 37.500, mean grad: -0.000003\n",
      "training loss: 1.978370 acc:  0.000, mean grad: -0.000031\n",
      "training loss: 1.469812 acc: 37.500, mean grad: -0.000006\n",
      "training loss: 1.818577 acc:  0.000, mean grad: -0.000029\n",
      "training loss: 1.494364 acc: 43.750, mean grad: -0.000005\n",
      "training loss: 1.710000 acc:  0.000, mean grad: -0.000027\n",
      "training loss: 1.514791 acc: 37.500, mean grad: -0.000005\n",
      "training loss: 1.637967 acc: 22.222, mean grad: -0.000024\n",
      "training loss: 1.531223 acc: 31.250, mean grad: -0.000006\n",
      "training loss: 1.590338 acc: 33.333, mean grad: -0.000020\n",
      "training loss: 1.544310 acc: 25.000, mean grad: -0.000005\n",
      "training loss: 1.559122 acc: 44.444, mean grad: -0.000017\n",
      "training loss: 1.477602 acc: 31.250, mean grad: 0.000026\n",
      "training loss: 2.272656 acc:  0.000, mean grad: 0.000022\n",
      "training loss: 1.488224 acc: 37.500, mean grad: 0.000022\n",
      "training loss: 2.106938 acc:  0.000, mean grad: 0.000016\n",
      "training loss: 1.507292 acc: 37.500, mean grad: 0.000016\n",
      "training loss: 1.903889 acc:  0.000, mean grad: 0.000006\n",
      "training loss: 1.531665 acc: 25.000, mean grad: 0.000010\n",
      "training loss: 1.745376 acc: 11.111, mean grad: 0.000001\n",
      "training loss: 1.552609 acc: 18.750, mean grad: 0.000007\n",
      "training loss: 1.639768 acc: 22.222, mean grad: -0.000002\n",
      "training loss: 1.568630 acc: 18.750, mean grad: 0.000005\n",
      "training loss: 1.571484 acc: 33.333, mean grad: -0.000005\n",
      "training loss: 1.580187 acc: 12.500, mean grad: 0.000004\n",
      "training loss: 1.527244 acc: 33.333, mean grad: -0.000005\n",
      "training loss: 1.588268 acc: 12.500, mean grad: 0.000003\n",
      "training loss: 1.499166 acc: 55.556, mean grad: -0.000008\n",
      "training loss: 1.369988 acc: 43.750, mean grad: 0.000020\n",
      "training loss: 2.348121 acc:  0.000, mean grad: 0.000023\n",
      "training loss: 1.384470 acc: 43.750, mean grad: 0.000016\n",
      "training loss: 2.194127 acc:  0.000, mean grad: 0.000020\n",
      "training loss: 1.407843 acc: 50.000, mean grad: 0.000012\n",
      "training loss: 1.997681 acc:  0.000, mean grad: 0.000012\n",
      "training loss: 1.436275 acc: 50.000, mean grad: 0.000008\n",
      "training loss: 1.837421 acc:  0.000, mean grad: 0.000009\n",
      "training loss: 1.462427 acc: 50.000, mean grad: 0.000005\n",
      "training loss: 1.728285 acc:  0.000, mean grad: 0.000007\n",
      "training loss: 1.484246 acc: 37.500, mean grad: 0.000003\n",
      "training loss: 1.655693 acc: 11.111, mean grad: 0.000002\n",
      "training loss: 1.502079 acc: 43.750, mean grad: 0.000002\n",
      "training loss: 1.607535 acc: 22.222, mean grad: 0.000004\n",
      "training loss: 1.516745 acc: 37.500, mean grad: 0.000002\n",
      "training loss: 1.575737 acc: 44.444, mean grad: 0.000001\n",
      "training loss: 1.530644 acc: 25.000, mean grad: 0.000014\n",
      "training loss: 2.367118 acc:  0.000, mean grad: 0.000028\n",
      "training loss: 1.529654 acc: 25.000, mean grad: 0.000011\n",
      "training loss: 2.194351 acc:  0.000, mean grad: 0.000018\n",
      "training loss: 1.528852 acc: 25.000, mean grad: 0.000004\n",
      "training loss: 1.982200 acc:  0.000, mean grad: -0.000002\n",
      "training loss: 1.533728 acc: 25.000, mean grad: 0.000004\n",
      "training loss: 1.816617 acc: 11.111, mean grad: -0.000015\n",
      "training loss: 1.541294 acc: 25.000, mean grad: 0.000001\n",
      "training loss: 1.707077 acc: 11.111, mean grad: -0.000020\n",
      "training loss: 1.548744 acc: 25.000, mean grad: 0.000002\n",
      "training loss: 1.636134 acc: 22.222, mean grad: -0.000026\n",
      "training loss: 1.555380 acc: 37.500, mean grad: -0.000000\n",
      "training loss: 1.589815 acc: 22.222, mean grad: -0.000030\n",
      "training loss: 1.561148 acc: 31.250, mean grad: -0.000001\n",
      "training loss: 1.559695 acc: 44.444, mean grad: -0.000033\n",
      "training loss: 1.443909 acc: 31.250, mean grad: 0.000014\n",
      "training loss: 2.129698 acc:  0.000, mean grad: -0.000015\n",
      "training loss: 1.448582 acc: 31.250, mean grad: 0.000012\n",
      "training loss: 1.993723 acc:  0.000, mean grad: -0.000016\n",
      "training loss: 1.456121 acc: 31.250, mean grad: 0.000005\n",
      "training loss: 1.826163 acc:  0.000, mean grad: -0.000018\n",
      "training loss: 1.470826 acc: 25.000, mean grad: 0.000001\n",
      "training loss: 1.696930 acc: 11.111, mean grad: -0.000013\n",
      "training loss: 1.486320 acc: 50.000, mean grad: -0.000002\n",
      "training loss: 1.613369 acc: 22.222, mean grad: -0.000014\n",
      "training loss: 1.500456 acc: 43.750, mean grad: -0.000003\n",
      "training loss: 1.559816 acc: 44.444, mean grad: -0.000013\n",
      "training loss: 1.512536 acc: 37.500, mean grad: -0.000004\n",
      "training loss: 1.525978 acc: 55.556, mean grad: -0.000013\n",
      "training loss: 1.523244 acc: 37.500, mean grad: -0.000004\n",
      "training loss: 1.505623 acc: 66.667, mean grad: -0.000014\n",
      "training loss: 1.517255 acc: 37.500, mean grad: 0.000037\n",
      "training loss: 2.264400 acc:  0.000, mean grad: -0.000007\n",
      "training loss: 1.515350 acc: 37.500, mean grad: 0.000034\n",
      "training loss: 2.109633 acc:  0.000, mean grad: -0.000010\n",
      "training loss: 1.511800 acc: 43.750, mean grad: 0.000027\n",
      "training loss: 1.918413 acc:  0.000, mean grad: -0.000020\n",
      "training loss: 1.514701 acc: 43.750, mean grad: 0.000019\n",
      "training loss: 1.769322 acc: 11.111, mean grad: -0.000020\n",
      "training loss: 1.521420 acc: 50.000, mean grad: 0.000012\n",
      "training loss: 1.671624 acc: 11.111, mean grad: -0.000024\n",
      "training loss: 1.529225 acc: 43.750, mean grad: 0.000008\n",
      "training loss: 1.609212 acc: 33.333, mean grad: -0.000024\n",
      "training loss: 1.536893 acc: 43.750, mean grad: 0.000004\n",
      "training loss: 1.569562 acc: 55.556, mean grad: -0.000025\n",
      "training loss: 1.544126 acc: 43.750, mean grad: 0.000001\n",
      "training loss: 1.544956 acc: 55.556, mean grad: -0.000024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.406884 acc: 25.000, mean grad: -0.000014\n",
      "training loss: 2.400234 acc:  0.000, mean grad: 0.000013\n",
      "training loss: 1.416957 acc: 31.250, mean grad: -0.000015\n",
      "training loss: 2.215071 acc:  0.000, mean grad: 0.000011\n",
      "training loss: 1.435520 acc: 43.750, mean grad: -0.000019\n",
      "training loss: 1.980842 acc:  0.000, mean grad: 0.000006\n",
      "training loss: 1.460387 acc: 56.250, mean grad: -0.000018\n",
      "training loss: 1.798095 acc:  0.000, mean grad: 0.000002\n",
      "training loss: 1.483756 acc: 56.250, mean grad: -0.000018\n",
      "training loss: 1.679187 acc: 11.111, mean grad: -0.000000\n",
      "training loss: 1.503561 acc: 50.000, mean grad: -0.000020\n",
      "training loss: 1.604052 acc: 22.222, mean grad: -0.000002\n",
      "training loss: 1.519984 acc: 50.000, mean grad: -0.000020\n",
      "training loss: 1.556821 acc: 22.222, mean grad: -0.000001\n",
      "training loss: 1.533533 acc: 37.500, mean grad: -0.000020\n",
      "training loss: 1.527767 acc: 44.444, mean grad: -0.000002\n",
      "training loss: 1.466853 acc: 12.500, mean grad: 0.000021\n",
      "training loss: 2.371434 acc:  0.000, mean grad: -0.000023\n",
      "training loss: 1.468165 acc: 12.500, mean grad: 0.000019\n",
      "training loss: 2.191730 acc:  0.000, mean grad: -0.000018\n",
      "training loss: 1.472260 acc: 25.000, mean grad: 0.000016\n",
      "training loss: 1.968599 acc:  0.000, mean grad: -0.000014\n",
      "training loss: 1.483202 acc: 37.500, mean grad: 0.000012\n",
      "training loss: 1.796583 acc:  0.000, mean grad: -0.000014\n",
      "training loss: 1.496154 acc: 37.500, mean grad: 0.000009\n",
      "training loss: 1.685766 acc:  0.000, mean grad: -0.000011\n",
      "training loss: 1.508216 acc: 37.500, mean grad: 0.000007\n",
      "training loss: 1.615832 acc: 22.222, mean grad: -0.000009\n",
      "training loss: 1.518807 acc: 50.000, mean grad: 0.000005\n",
      "training loss: 1.571964 acc: 33.333, mean grad: -0.000007\n",
      "training loss: 1.528146 acc: 50.000, mean grad: 0.000004\n",
      "training loss: 1.544973 acc: 33.333, mean grad: -0.000004\n",
      "training loss: 1.350803 acc: 50.000, mean grad: 0.000002\n",
      "training loss: 2.328881 acc:  0.000, mean grad: -0.000011\n",
      "training loss: 1.370768 acc: 50.000, mean grad: 0.000004\n",
      "training loss: 2.156721 acc:  0.000, mean grad: -0.000004\n",
      "training loss: 1.408472 acc: 50.000, mean grad: -0.000002\n",
      "training loss: 1.938741 acc:  0.000, mean grad: -0.000011\n",
      "training loss: 1.451160 acc: 43.750, mean grad: -0.000006\n",
      "training loss: 1.773910 acc:  0.000, mean grad: -0.000003\n",
      "training loss: 1.485570 acc: 31.250, mean grad: -0.000008\n",
      "training loss: 1.669112 acc: 11.111, mean grad: -0.000010\n",
      "training loss: 1.511616 acc: 31.250, mean grad: -0.000009\n",
      "training loss: 1.603050 acc: 22.222, mean grad: -0.000009\n",
      "training loss: 1.531259 acc: 25.000, mean grad: -0.000011\n",
      "training loss: 1.561588 acc: 22.222, mean grad: -0.000009\n",
      "training loss: 1.546212 acc: 25.000, mean grad: -0.000010\n",
      "training loss: 1.536423 acc: 44.444, mean grad: -0.000005\n",
      "training loss: 1.507460 acc:  6.250, mean grad: 0.000005\n",
      "training loss: 2.475130 acc:  0.000, mean grad: 0.000038\n",
      "training loss: 1.509456 acc: 12.500, mean grad: 0.000006\n",
      "training loss: 2.281239 acc:  0.000, mean grad: 0.000030\n",
      "training loss: 1.514731 acc: 18.750, mean grad: 0.000008\n",
      "training loss: 2.039655 acc:  0.000, mean grad: 0.000023\n",
      "training loss: 1.526592 acc: 37.500, mean grad: 0.000008\n",
      "training loss: 1.855188 acc:  0.000, mean grad: 0.000012\n",
      "training loss: 1.539795 acc: 37.500, mean grad: 0.000006\n",
      "training loss: 1.735262 acc: 11.111, mean grad: 0.000011\n",
      "training loss: 1.551105 acc: 25.000, mean grad: 0.000006\n",
      "training loss: 1.657863 acc: 22.222, mean grad: 0.000010\n",
      "training loss: 1.560175 acc: 25.000, mean grad: 0.000003\n",
      "training loss: 1.607885 acc: 33.333, mean grad: 0.000009\n",
      "training loss: 1.567243 acc: 25.000, mean grad: 0.000002\n",
      "training loss: 1.575745 acc: 55.556, mean grad: 0.000008\n",
      "training loss: 1.492071 acc: 18.750, mean grad: 0.000024\n",
      "training loss: 2.494337 acc:  0.000, mean grad: -0.000027\n",
      "training loss: 1.492621 acc: 18.750, mean grad: 0.000025\n",
      "training loss: 2.278200 acc:  0.000, mean grad: -0.000031\n",
      "training loss: 1.495509 acc: 25.000, mean grad: 0.000018\n",
      "training loss: 2.002483 acc:  0.000, mean grad: -0.000028\n",
      "training loss: 1.507769 acc: 37.500, mean grad: 0.000011\n",
      "training loss: 1.791162 acc: 11.111, mean grad: -0.000024\n",
      "training loss: 1.522158 acc: 37.500, mean grad: 0.000007\n",
      "training loss: 1.655947 acc: 11.111, mean grad: -0.000020\n",
      "training loss: 1.534990 acc: 31.250, mean grad: 0.000003\n",
      "training loss: 1.571272 acc: 44.444, mean grad: -0.000016\n",
      "training loss: 1.545447 acc: 18.750, mean grad: -0.000000\n",
      "training loss: 1.519502 acc: 44.444, mean grad: -0.000014\n",
      "training loss: 1.553911 acc: 18.750, mean grad: -0.000003\n",
      "training loss: 1.488991 acc: 55.556, mean grad: -0.000013\n",
      "training loss: 1.522296 acc: 18.750, mean grad: -0.000006\n",
      "training loss: 2.421350 acc:  0.000, mean grad: -0.000041\n",
      "training loss: 1.523258 acc: 12.500, mean grad: -0.000005\n",
      "training loss: 2.224355 acc:  0.000, mean grad: -0.000044\n",
      "training loss: 1.526888 acc: 31.250, mean grad: -0.000001\n",
      "training loss: 1.982807 acc:  0.000, mean grad: -0.000045\n",
      "training loss: 1.536829 acc: 31.250, mean grad: 0.000001\n",
      "training loss: 1.803439 acc:  0.000, mean grad: -0.000044\n",
      "training loss: 1.547684 acc: 31.250, mean grad: 0.000002\n",
      "training loss: 1.690907 acc: 11.111, mean grad: -0.000040\n",
      "training loss: 1.557015 acc: 31.250, mean grad: 0.000002\n",
      "training loss: 1.620987 acc: 22.222, mean grad: -0.000035\n",
      "training loss: 1.564545 acc: 25.000, mean grad: 0.000002\n",
      "training loss: 1.577561 acc: 44.444, mean grad: -0.000030\n",
      "training loss: 1.570527 acc: 12.500, mean grad: 0.000001\n",
      "training loss: 1.550913 acc: 44.444, mean grad: -0.000029\n",
      "training loss: 1.410024 acc: 37.500, mean grad: -0.000007\n",
      "training loss: 2.207468 acc:  0.000, mean grad: -0.000050\n",
      "training loss: 1.416609 acc: 37.500, mean grad: -0.000008\n",
      "training loss: 2.037595 acc:  0.000, mean grad: -0.000053\n",
      "training loss: 1.433871 acc: 37.500, mean grad: -0.000011\n",
      "training loss: 1.826475 acc:  0.000, mean grad: -0.000057\n",
      "training loss: 1.460009 acc: 43.750, mean grad: -0.000015\n",
      "training loss: 1.671808 acc: 22.222, mean grad: -0.000060\n",
      "training loss: 1.484071 acc: 31.250, mean grad: -0.000015\n",
      "training loss: 1.577292 acc: 33.333, mean grad: -0.000059\n",
      "training loss: 1.503615 acc: 31.250, mean grad: -0.000015\n",
      "training loss: 1.520618 acc: 44.444, mean grad: -0.000058\n",
      "training loss: 1.518990 acc: 25.000, mean grad: -0.000015\n",
      "training loss: 1.487036 acc: 44.444, mean grad: -0.000057\n",
      "training loss: 1.531379 acc: 37.500, mean grad: -0.000015\n",
      "training loss: 1.468173 acc: 44.444, mean grad: -0.000056\n",
      "training loss: 1.509947 acc: 31.250, mean grad: 0.000022\n",
      "training loss: 2.283672 acc:  0.000, mean grad: -0.000022\n",
      "training loss: 1.503201 acc: 31.250, mean grad: 0.000017\n",
      "training loss: 2.078222 acc:  0.000, mean grad: -0.000025\n",
      "training loss: 1.499844 acc: 31.250, mean grad: 0.000008\n",
      "training loss: 1.830034 acc:  0.000, mean grad: -0.000027\n",
      "training loss: 1.510678 acc: 43.750, mean grad: 0.000000\n",
      "training loss: 1.651313 acc: 11.111, mean grad: -0.000030\n",
      "training loss: 1.525613 acc: 37.500, mean grad: -0.000004\n",
      "training loss: 1.545514 acc: 44.444, mean grad: -0.000030\n",
      "training loss: 1.538893 acc: 37.500, mean grad: -0.000006\n",
      "training loss: 1.485001 acc: 66.667, mean grad: -0.000031\n",
      "training loss: 1.549467 acc: 25.000, mean grad: -0.000008\n",
      "training loss: 1.451705 acc: 77.778, mean grad: -0.000031\n",
      "training loss: 1.557924 acc: 18.750, mean grad: -0.000008\n",
      "training loss: 1.435617 acc: 77.778, mean grad: -0.000029\n",
      "training loss: 1.407217 acc: 37.500, mean grad: 0.000008\n",
      "training loss: 2.273200 acc:  0.000, mean grad: -0.000037\n",
      "training loss: 1.418346 acc: 37.500, mean grad: 0.000007\n",
      "training loss: 2.095731 acc:  0.000, mean grad: -0.000033\n",
      "training loss: 1.439876 acc: 37.500, mean grad: 0.000003\n",
      "training loss: 1.867085 acc:  0.000, mean grad: -0.000032\n",
      "training loss: 1.466293 acc: 43.750, mean grad: 0.000003\n",
      "training loss: 1.696343 acc: 11.111, mean grad: -0.000025\n",
      "training loss: 1.488999 acc: 43.750, mean grad: 0.000001\n",
      "training loss: 1.590639 acc: 11.111, mean grad: -0.000024\n",
      "training loss: 1.506538 acc: 37.500, mean grad: -0.000002\n",
      "training loss: 1.527313 acc: 33.333, mean grad: -0.000022\n",
      "training loss: 1.519961 acc: 25.000, mean grad: -0.000002\n",
      "training loss: 1.489736 acc: 44.444, mean grad: -0.000019\n",
      "training loss: 1.530492 acc: 31.250, mean grad: -0.000003\n",
      "training loss: 1.468310 acc: 66.667, mean grad: -0.000019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.406147 acc: 25.000, mean grad: -0.000003\n",
      "training loss: 2.445801 acc:  0.000, mean grad: 0.000006\n",
      "training loss: 1.405290 acc: 37.500, mean grad: -0.000002\n",
      "training loss: 2.229115 acc:  0.000, mean grad: 0.000001\n",
      "training loss: 1.409923 acc: 56.250, mean grad: 0.000001\n",
      "training loss: 1.959930 acc:  0.000, mean grad: -0.000007\n",
      "training loss: 1.428122 acc: 68.750, mean grad: 0.000002\n",
      "training loss: 1.763112 acc:  0.000, mean grad: -0.000013\n",
      "training loss: 1.449936 acc: 68.750, mean grad: 0.000003\n",
      "training loss: 1.642172 acc: 22.222, mean grad: -0.000019\n",
      "training loss: 1.469644 acc: 62.500, mean grad: 0.000003\n",
      "training loss: 1.569057 acc: 33.333, mean grad: -0.000025\n",
      "training loss: 1.486700 acc: 56.250, mean grad: 0.000003\n",
      "training loss: 1.524744 acc: 44.444, mean grad: -0.000026\n",
      "training loss: 1.501315 acc: 56.250, mean grad: 0.000002\n",
      "training loss: 1.498800 acc: 44.444, mean grad: -0.000025\n",
      "training loss: 1.476137 acc: 12.500, mean grad: 0.000017\n",
      "training loss: 2.391137 acc:  0.000, mean grad: -0.000003\n",
      "training loss: 1.483153 acc: 12.500, mean grad: 0.000014\n",
      "training loss: 2.220419 acc:  0.000, mean grad: -0.000007\n",
      "training loss: 1.501987 acc: 12.500, mean grad: 0.000007\n",
      "training loss: 2.005898 acc:  0.000, mean grad: -0.000011\n",
      "training loss: 1.528343 acc: 18.750, mean grad: -0.000000\n",
      "training loss: 1.844927 acc:  0.000, mean grad: -0.000013\n",
      "training loss: 1.551475 acc: 12.500, mean grad: -0.000003\n",
      "training loss: 1.740385 acc:  0.000, mean grad: -0.000013\n",
      "training loss: 1.569003 acc: 12.500, mean grad: -0.000005\n",
      "training loss: 1.672788 acc: 11.111, mean grad: -0.000014\n",
      "training loss: 1.581746 acc: 12.500, mean grad: -0.000006\n",
      "training loss: 1.628721 acc: 22.222, mean grad: -0.000013\n",
      "training loss: 1.590804 acc: 18.750, mean grad: -0.000006\n",
      "training loss: 1.600007 acc: 33.333, mean grad: -0.000013\n",
      "training loss: 1.416753 acc: 37.500, mean grad: 0.000005\n",
      "training loss: 2.474190 acc:  0.000, mean grad: -0.000010\n",
      "training loss: 1.414279 acc: 37.500, mean grad: 0.000001\n",
      "training loss: 2.225244 acc:  0.000, mean grad: -0.000008\n",
      "training loss: 1.424857 acc: 37.500, mean grad: -0.000006\n",
      "training loss: 1.924413 acc:  0.000, mean grad: -0.000014\n",
      "training loss: 1.453314 acc: 37.500, mean grad: -0.000011\n",
      "training loss: 1.710754 acc:  0.000, mean grad: -0.000019\n",
      "training loss: 1.482122 acc: 37.500, mean grad: -0.000013\n",
      "training loss: 1.586086 acc: 33.333, mean grad: -0.000020\n",
      "training loss: 1.505063 acc: 31.250, mean grad: -0.000015\n",
      "training loss: 1.514614 acc: 55.556, mean grad: -0.000019\n",
      "training loss: 1.522586 acc: 25.000, mean grad: -0.000016\n",
      "training loss: 1.474087 acc: 55.556, mean grad: -0.000019\n",
      "training loss: 1.536046 acc: 25.000, mean grad: -0.000016\n",
      "training loss: 1.452528 acc: 55.556, mean grad: -0.000018\n",
      "training loss: 1.368546 acc: 31.250, mean grad: -0.000002\n",
      "training loss: 2.422824 acc:  0.000, mean grad: -0.000014\n",
      "training loss: 1.383868 acc: 31.250, mean grad: -0.000002\n",
      "training loss: 2.215710 acc:  0.000, mean grad: -0.000015\n",
      "training loss: 1.420422 acc: 31.250, mean grad: -0.000003\n",
      "training loss: 1.955013 acc:  0.000, mean grad: -0.000018\n",
      "training loss: 1.465003 acc: 31.250, mean grad: -0.000005\n",
      "training loss: 1.768360 acc:  0.000, mean grad: -0.000023\n",
      "training loss: 1.501608 acc: 31.250, mean grad: -0.000003\n",
      "training loss: 1.656277 acc:  0.000, mean grad: -0.000022\n",
      "training loss: 1.528287 acc: 37.500, mean grad: -0.000004\n",
      "training loss: 1.589825 acc: 22.222, mean grad: -0.000020\n",
      "training loss: 1.547155 acc: 37.500, mean grad: -0.000004\n",
      "training loss: 1.550581 acc: 33.333, mean grad: -0.000021\n",
      "training loss: 1.560699 acc: 37.500, mean grad: -0.000004\n",
      "training loss: 1.528250 acc: 33.333, mean grad: -0.000021\n",
      "training loss: 1.212074 acc: 56.250, mean grad: 0.000016\n",
      "training loss: 2.332549 acc:  0.000, mean grad: -0.000003\n",
      "training loss: 1.224248 acc: 50.000, mean grad: 0.000011\n",
      "training loss: 2.117943 acc:  0.000, mean grad: -0.000003\n",
      "training loss: 1.259803 acc: 56.250, mean grad: 0.000003\n",
      "training loss: 1.854486 acc:  0.000, mean grad: -0.000009\n",
      "training loss: 1.309787 acc: 62.500, mean grad: -0.000002\n",
      "training loss: 1.670345 acc: 11.111, mean grad: -0.000011\n",
      "training loss: 1.355596 acc: 62.500, mean grad: -0.000004\n",
      "training loss: 1.563303 acc: 22.222, mean grad: -0.000010\n",
      "training loss: 1.393182 acc: 68.750, mean grad: -0.000006\n",
      "training loss: 1.502894 acc: 33.333, mean grad: -0.000011\n",
      "training loss: 1.423357 acc: 68.750, mean grad: -0.000008\n",
      "training loss: 1.469832 acc: 55.556, mean grad: -0.000008\n",
      "training loss: 1.448225 acc: 62.500, mean grad: -0.000010\n",
      "training loss: 1.453469 acc: 55.556, mean grad: -0.000010\n",
      "training loss: 1.332116 acc: 31.250, mean grad: 0.000006\n",
      "training loss: 2.576743 acc:  0.000, mean grad: -0.000023\n",
      "training loss: 1.350154 acc: 31.250, mean grad: 0.000002\n",
      "training loss: 2.342111 acc:  0.000, mean grad: -0.000020\n",
      "training loss: 1.391135 acc: 31.250, mean grad: 0.000003\n",
      "training loss: 2.048268 acc:  0.000, mean grad: -0.000014\n",
      "training loss: 1.439660 acc: 43.750, mean grad: 0.000004\n",
      "training loss: 1.839535 acc:  0.000, mean grad: -0.000011\n",
      "training loss: 1.478745 acc: 50.000, mean grad: 0.000001\n",
      "training loss: 1.713704 acc: 11.111, mean grad: -0.000009\n",
      "training loss: 1.506936 acc: 56.250, mean grad: 0.000001\n",
      "training loss: 1.638329 acc: 11.111, mean grad: -0.000009\n",
      "training loss: 1.527220 acc: 50.000, mean grad: 0.000000\n",
      "training loss: 1.592795 acc: 11.111, mean grad: -0.000008\n",
      "training loss: 1.542206 acc: 43.750, mean grad: -0.000001\n",
      "training loss: 1.565499 acc: 33.333, mean grad: -0.000008\n",
      "training loss: 1.350635 acc: 31.250, mean grad: 0.000007\n",
      "training loss: 2.702749 acc:  0.000, mean grad: 0.000019\n",
      "training loss: 1.362110 acc: 31.250, mean grad: 0.000005\n",
      "training loss: 2.433535 acc:  0.000, mean grad: 0.000018\n",
      "training loss: 1.398605 acc: 37.500, mean grad: 0.000000\n",
      "training loss: 2.100031 acc:  0.000, mean grad: 0.000015\n",
      "training loss: 1.448521 acc: 43.750, mean grad: -0.000004\n",
      "training loss: 1.864037 acc:  0.000, mean grad: 0.000011\n",
      "training loss: 1.490557 acc: 43.750, mean grad: -0.000007\n",
      "training loss: 1.721502 acc:  0.000, mean grad: 0.000010\n",
      "training loss: 1.521768 acc: 43.750, mean grad: -0.000009\n",
      "training loss: 1.635334 acc: 11.111, mean grad: 0.000010\n",
      "training loss: 1.544239 acc: 43.750, mean grad: -0.000009\n",
      "training loss: 1.582890 acc: 22.222, mean grad: 0.000008\n",
      "training loss: 1.560278 acc: 37.500, mean grad: -0.000010\n",
      "training loss: 1.551269 acc: 44.444, mean grad: 0.000007\n",
      "training loss: 1.444032 acc: 25.000, mean grad: 0.000009\n",
      "training loss: 2.612146 acc:  0.000, mean grad: 0.000027\n",
      "training loss: 1.431999 acc: 25.000, mean grad: 0.000007\n",
      "training loss: 2.358232 acc:  0.000, mean grad: 0.000020\n",
      "training loss: 1.429485 acc: 31.250, mean grad: 0.000004\n",
      "training loss: 2.053726 acc:  0.000, mean grad: 0.000017\n",
      "training loss: 1.450960 acc: 50.000, mean grad: 0.000004\n",
      "training loss: 1.838705 acc:  0.000, mean grad: 0.000015\n",
      "training loss: 1.476756 acc: 50.000, mean grad: 0.000002\n",
      "training loss: 1.708989 acc:  0.000, mean grad: 0.000011\n",
      "training loss: 1.498655 acc: 43.750, mean grad: 0.000003\n",
      "training loss: 1.631145 acc: 22.222, mean grad: 0.000011\n",
      "training loss: 1.515860 acc: 37.500, mean grad: 0.000002\n",
      "training loss: 1.584355 acc: 33.333, mean grad: 0.000012\n",
      "training loss: 1.529487 acc: 37.500, mean grad: 0.000001\n",
      "training loss: 1.556736 acc: 44.444, mean grad: 0.000011\n",
      "training loss: 1.452092 acc: 25.000, mean grad: 0.000012\n",
      "training loss: 2.565068 acc:  0.000, mean grad: -0.000003\n",
      "training loss: 1.446500 acc: 25.000, mean grad: 0.000008\n",
      "training loss: 2.305635 acc:  0.000, mean grad: -0.000007\n",
      "training loss: 1.454427 acc: 25.000, mean grad: 0.000008\n",
      "training loss: 1.996188 acc:  0.000, mean grad: -0.000010\n",
      "training loss: 1.480129 acc: 31.250, mean grad: 0.000004\n",
      "training loss: 1.785914 acc: 11.111, mean grad: -0.000010\n",
      "training loss: 1.505974 acc: 31.250, mean grad: 0.000005\n",
      "training loss: 1.663133 acc: 11.111, mean grad: -0.000009\n",
      "training loss: 1.526197 acc: 37.500, mean grad: 0.000004\n",
      "training loss: 1.591399 acc: 11.111, mean grad: -0.000008\n",
      "training loss: 1.541412 acc: 56.250, mean grad: 0.000004\n",
      "training loss: 1.549580 acc: 44.444, mean grad: -0.000006\n",
      "training loss: 1.552993 acc: 50.000, mean grad: 0.000002\n",
      "training loss: 1.525942 acc: 44.444, mean grad: -0.000005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.588018 acc:  6.250, mean grad: 0.000019\n",
      "training loss: 2.665292 acc:  0.000, mean grad: 0.000009\n",
      "training loss: 1.558011 acc:  0.000, mean grad: 0.000017\n",
      "training loss: 2.385941 acc:  0.000, mean grad: 0.000006\n",
      "training loss: 1.520133 acc:  0.000, mean grad: 0.000016\n",
      "training loss: 2.060694 acc:  0.000, mean grad: 0.000002\n",
      "training loss: 1.511334 acc:  6.250, mean grad: 0.000010\n",
      "training loss: 1.841950 acc:  0.000, mean grad: -0.000001\n",
      "training loss: 1.517405 acc: 18.750, mean grad: 0.000006\n",
      "training loss: 1.712693 acc:  0.000, mean grad: -0.000004\n",
      "training loss: 1.526611 acc: 50.000, mean grad: 0.000004\n",
      "training loss: 1.635880 acc: 11.111, mean grad: -0.000004\n",
      "training loss: 1.535395 acc: 50.000, mean grad: 0.000002\n",
      "training loss: 1.589340 acc: 44.444, mean grad: -0.000005\n",
      "training loss: 1.543261 acc: 43.750, mean grad: 0.000000\n",
      "training loss: 1.561487 acc: 44.444, mean grad: -0.000003\n",
      "training loss: 1.402900 acc: 31.250, mean grad: 0.000003\n",
      "training loss: 2.858279 acc:  0.000, mean grad: 0.000012\n",
      "training loss: 1.404621 acc: 37.500, mean grad: -0.000001\n",
      "training loss: 2.516554 acc:  0.000, mean grad: 0.000006\n",
      "training loss: 1.427995 acc: 43.750, mean grad: -0.000007\n",
      "training loss: 2.088823 acc:  0.000, mean grad: 0.000002\n",
      "training loss: 1.470581 acc: 43.750, mean grad: -0.000007\n",
      "training loss: 1.789895 acc:  0.000, mean grad: -0.000001\n",
      "training loss: 1.506353 acc: 37.500, mean grad: -0.000008\n",
      "training loss: 1.616055 acc: 11.111, mean grad: -0.000002\n",
      "training loss: 1.530242 acc: 31.250, mean grad: -0.000008\n",
      "training loss: 1.516380 acc: 22.222, mean grad: -0.000004\n",
      "training loss: 1.545243 acc: 31.250, mean grad: -0.000009\n",
      "training loss: 1.459645 acc: 44.444, mean grad: -0.000005\n",
      "training loss: 1.555048 acc: 31.250, mean grad: -0.000008\n",
      "training loss: 1.428789 acc: 44.444, mean grad: -0.000007\n",
      "training loss: 1.419363 acc: 31.250, mean grad: 0.000021\n",
      "training loss: 2.682752 acc:  0.000, mean grad: -0.000004\n",
      "training loss: 1.421625 acc: 31.250, mean grad: 0.000019\n",
      "training loss: 2.398637 acc:  0.000, mean grad: -0.000006\n",
      "training loss: 1.441820 acc: 31.250, mean grad: 0.000016\n",
      "training loss: 2.039465 acc:  0.000, mean grad: -0.000006\n",
      "training loss: 1.478755 acc: 25.000, mean grad: 0.000014\n",
      "training loss: 1.786359 acc:  0.000, mean grad: -0.000009\n",
      "training loss: 1.512238 acc: 31.250, mean grad: 0.000013\n",
      "training loss: 1.636645 acc: 22.222, mean grad: -0.000011\n",
      "training loss: 1.536563 acc: 18.750, mean grad: 0.000012\n",
      "training loss: 1.550054 acc: 22.222, mean grad: -0.000008\n",
      "training loss: 1.553209 acc: 18.750, mean grad: 0.000010\n",
      "training loss: 1.500205 acc: 66.667, mean grad: -0.000009\n",
      "training loss: 1.564541 acc: 18.750, mean grad: 0.000010\n",
      "training loss: 1.472954 acc: 66.667, mean grad: -0.000009\n",
      "training loss: 1.360388 acc: 31.250, mean grad: 0.000004\n",
      "training loss: 2.707621 acc:  0.000, mean grad: -0.000001\n",
      "training loss: 1.366269 acc: 31.250, mean grad: 0.000002\n",
      "training loss: 2.381511 acc:  0.000, mean grad: -0.000004\n",
      "training loss: 1.391907 acc: 31.250, mean grad: -0.000004\n",
      "training loss: 1.962156 acc:  0.000, mean grad: 0.000003\n",
      "training loss: 1.436419 acc: 37.500, mean grad: -0.000007\n",
      "training loss: 1.668992 acc: 22.222, mean grad: -0.000001\n",
      "training loss: 1.478293 acc: 37.500, mean grad: -0.000009\n",
      "training loss: 1.498013 acc: 33.333, mean grad: 0.000004\n",
      "training loss: 1.510460 acc: 37.500, mean grad: -0.000009\n",
      "training loss: 1.401059 acc: 88.889, mean grad: 0.000000\n",
      "training loss: 1.533529 acc: 25.000, mean grad: -0.000009\n",
      "training loss: 1.346733 acc: 100.000, mean grad: -0.000003\n",
      "training loss: 1.549783 acc: 18.750, mean grad: -0.000008\n",
      "training loss: 1.318439 acc: 100.000, mean grad: -0.000006\n",
      "training loss: 1.433527 acc: 37.500, mean grad: 0.000007\n",
      "training loss: 2.835834 acc:  0.000, mean grad: 0.000001\n",
      "training loss: 1.425838 acc: 37.500, mean grad: 0.000003\n",
      "training loss: 2.525018 acc:  0.000, mean grad: -0.000001\n",
      "training loss: 1.426844 acc: 37.500, mean grad: -0.000001\n",
      "training loss: 2.128993 acc:  0.000, mean grad: -0.000006\n",
      "training loss: 1.449922 acc: 31.250, mean grad: -0.000002\n",
      "training loss: 1.842300 acc:  0.000, mean grad: -0.000010\n",
      "training loss: 1.478560 acc: 37.500, mean grad: -0.000004\n",
      "training loss: 1.671644 acc: 11.111, mean grad: -0.000018\n",
      "training loss: 1.503227 acc: 31.250, mean grad: -0.000005\n",
      "training loss: 1.571712 acc: 33.333, mean grad: -0.000017\n",
      "training loss: 1.521681 acc: 37.500, mean grad: -0.000007\n",
      "training loss: 1.512801 acc: 44.444, mean grad: -0.000021\n",
      "training loss: 1.535164 acc: 37.500, mean grad: -0.000008\n",
      "training loss: 1.478733 acc: 55.556, mean grad: -0.000023\n",
      "training loss: 1.573390 acc: 18.750, mean grad: 0.000018\n",
      "training loss: 2.813300 acc:  0.000, mean grad: -0.000011\n",
      "training loss: 1.540056 acc: 25.000, mean grad: 0.000015\n",
      "training loss: 2.491468 acc:  0.000, mean grad: -0.000009\n",
      "training loss: 1.499514 acc: 43.750, mean grad: 0.000009\n",
      "training loss: 2.092991 acc:  0.000, mean grad: -0.000010\n",
      "training loss: 1.491972 acc: 43.750, mean grad: 0.000002\n",
      "training loss: 1.807210 acc:  0.000, mean grad: -0.000007\n",
      "training loss: 1.502954 acc: 50.000, mean grad: 0.000000\n",
      "training loss: 1.635460 acc: 11.111, mean grad: -0.000000\n",
      "training loss: 1.515719 acc: 37.500, mean grad: -0.000003\n",
      "training loss: 1.530993 acc: 33.333, mean grad: 0.000003\n",
      "training loss: 1.525364 acc: 37.500, mean grad: -0.000005\n",
      "training loss: 1.464352 acc: 55.556, mean grad: 0.000002\n",
      "training loss: 1.531941 acc: 37.500, mean grad: -0.000007\n",
      "training loss: 1.420371 acc: 55.556, mean grad: -0.000002\n",
      "training loss: 1.436039 acc: 12.500, mean grad: -0.000010\n",
      "training loss: 2.587916 acc:  0.000, mean grad: 0.000009\n",
      "training loss: 1.418214 acc: 18.750, mean grad: -0.000011\n",
      "training loss: 2.329547 acc:  0.000, mean grad: 0.000010\n",
      "training loss: 1.400901 acc: 18.750, mean grad: -0.000011\n",
      "training loss: 2.015143 acc:  0.000, mean grad: 0.000015\n",
      "training loss: 1.412366 acc: 37.500, mean grad: -0.000011\n",
      "training loss: 1.800905 acc: 22.222, mean grad: 0.000012\n",
      "training loss: 1.435150 acc: 37.500, mean grad: -0.000010\n",
      "training loss: 1.674891 acc: 22.222, mean grad: 0.000015\n",
      "training loss: 1.457042 acc: 56.250, mean grad: -0.000010\n",
      "training loss: 1.600185 acc: 22.222, mean grad: 0.000015\n",
      "training loss: 1.475612 acc: 56.250, mean grad: -0.000011\n",
      "training loss: 1.555155 acc: 22.222, mean grad: 0.000018\n",
      "training loss: 1.491066 acc: 62.500, mean grad: -0.000011\n",
      "training loss: 1.527869 acc: 33.333, mean grad: 0.000017\n",
      "training loss: 1.320072 acc: 50.000, mean grad: 0.000027\n",
      "training loss: 2.931974 acc:  0.000, mean grad: 0.000034\n",
      "training loss: 1.327352 acc: 50.000, mean grad: 0.000027\n",
      "training loss: 2.613389 acc:  0.000, mean grad: 0.000027\n",
      "training loss: 1.349281 acc: 56.250, mean grad: 0.000019\n",
      "training loss: 2.192601 acc:  0.000, mean grad: 0.000015\n",
      "training loss: 1.386847 acc: 50.000, mean grad: 0.000013\n",
      "training loss: 1.890162 acc:  0.000, mean grad: 0.000004\n",
      "training loss: 1.424103 acc: 43.750, mean grad: 0.000010\n",
      "training loss: 1.710505 acc:  0.000, mean grad: -0.000003\n",
      "training loss: 1.454279 acc: 50.000, mean grad: 0.000008\n",
      "training loss: 1.604572 acc: 22.222, mean grad: -0.000008\n",
      "training loss: 1.477326 acc: 37.500, mean grad: 0.000006\n",
      "training loss: 1.541074 acc: 33.333, mean grad: -0.000009\n",
      "training loss: 1.494777 acc: 43.750, mean grad: 0.000005\n",
      "training loss: 1.503162 acc: 44.444, mean grad: -0.000011\n",
      "training loss: 1.401270 acc: 37.500, mean grad: 0.000007\n",
      "training loss: 2.896807 acc:  0.000, mean grad: -0.000028\n",
      "training loss: 1.395387 acc: 37.500, mean grad: 0.000003\n",
      "training loss: 2.570468 acc:  0.000, mean grad: -0.000030\n",
      "training loss: 1.399164 acc: 43.750, mean grad: 0.000000\n",
      "training loss: 2.152582 acc:  0.000, mean grad: -0.000027\n",
      "training loss: 1.428282 acc: 37.500, mean grad: -0.000002\n",
      "training loss: 1.856107 acc:  0.000, mean grad: -0.000022\n",
      "training loss: 1.461927 acc: 37.500, mean grad: -0.000005\n",
      "training loss: 1.676641 acc: 22.222, mean grad: -0.000014\n",
      "training loss: 1.489317 acc: 43.750, mean grad: -0.000005\n",
      "training loss: 1.568711 acc: 22.222, mean grad: -0.000011\n",
      "training loss: 1.509607 acc: 43.750, mean grad: -0.000005\n",
      "training loss: 1.502233 acc: 33.333, mean grad: -0.000007\n",
      "training loss: 1.524508 acc: 37.500, mean grad: -0.000005\n",
      "training loss: 1.461966 acc: 33.333, mean grad: -0.000005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.403172 acc: 25.000, mean grad: -0.000007\n",
      "training loss: 2.980939 acc:  0.000, mean grad: 0.000016\n",
      "training loss: 1.402938 acc: 25.000, mean grad: -0.000006\n",
      "training loss: 2.637930 acc:  0.000, mean grad: 0.000019\n",
      "training loss: 1.416363 acc: 31.250, mean grad: -0.000008\n",
      "training loss: 2.192138 acc: 11.111, mean grad: 0.000019\n",
      "training loss: 1.452586 acc: 37.500, mean grad: -0.000008\n",
      "training loss: 1.872547 acc: 11.111, mean grad: 0.000021\n",
      "training loss: 1.490016 acc: 37.500, mean grad: -0.000007\n",
      "training loss: 1.681942 acc: 11.111, mean grad: 0.000022\n",
      "training loss: 1.518232 acc: 25.000, mean grad: -0.000005\n",
      "training loss: 1.569527 acc: 22.222, mean grad: 0.000023\n",
      "training loss: 1.537352 acc: 25.000, mean grad: -0.000003\n",
      "training loss: 1.502456 acc: 44.444, mean grad: 0.000023\n",
      "training loss: 1.550048 acc: 18.750, mean grad: -0.000002\n",
      "training loss: 1.462709 acc: 55.556, mean grad: 0.000021\n",
      "training loss: 1.427338 acc: 25.000, mean grad: -0.000022\n",
      "training loss: 3.036361 acc:  0.000, mean grad: 0.000040\n",
      "training loss: 1.419977 acc: 31.250, mean grad: -0.000020\n",
      "training loss: 2.703067 acc:  0.000, mean grad: 0.000029\n",
      "training loss: 1.420619 acc: 37.500, mean grad: -0.000019\n",
      "training loss: 2.271769 acc:  0.000, mean grad: 0.000020\n",
      "training loss: 1.447419 acc: 62.500, mean grad: -0.000015\n",
      "training loss: 1.960302 acc:  0.000, mean grad: 0.000007\n",
      "training loss: 1.479937 acc: 50.000, mean grad: -0.000011\n",
      "training loss: 1.768790 acc: 11.111, mean grad: 0.000002\n",
      "training loss: 1.505912 acc: 50.000, mean grad: -0.000006\n",
      "training loss: 1.650848 acc: 11.111, mean grad: -0.000002\n",
      "training loss: 1.524738 acc: 50.000, mean grad: -0.000006\n",
      "training loss: 1.576436 acc: 33.333, mean grad: -0.000005\n",
      "training loss: 1.538300 acc: 50.000, mean grad: -0.000006\n",
      "training loss: 1.529064 acc: 55.556, mean grad: -0.000008\n",
      "training loss: 1.362677 acc: 37.500, mean grad: 0.000004\n",
      "training loss: 2.824321 acc:  0.000, mean grad: -0.000014\n",
      "training loss: 1.342375 acc: 43.750, mean grad: -0.000000\n",
      "training loss: 2.528457 acc:  0.000, mean grad: -0.000015\n",
      "training loss: 1.326770 acc: 37.500, mean grad: -0.000002\n",
      "training loss: 2.158064 acc: 11.111, mean grad: -0.000017\n",
      "training loss: 1.348813 acc: 50.000, mean grad: -0.000005\n",
      "training loss: 1.893742 acc: 11.111, mean grad: -0.000022\n",
      "training loss: 1.383063 acc: 56.250, mean grad: -0.000006\n",
      "training loss: 1.730731 acc: 11.111, mean grad: -0.000025\n",
      "training loss: 1.413869 acc: 50.000, mean grad: -0.000005\n",
      "training loss: 1.632969 acc: 22.222, mean grad: -0.000028\n",
      "training loss: 1.438713 acc: 56.250, mean grad: -0.000005\n",
      "training loss: 1.574155 acc: 22.222, mean grad: -0.000026\n",
      "training loss: 1.458484 acc: 50.000, mean grad: -0.000004\n",
      "training loss: 1.538906 acc: 22.222, mean grad: -0.000027\n",
      "training loss: 1.345921 acc: 31.250, mean grad: 0.000010\n",
      "training loss: 3.157042 acc:  0.000, mean grad: 0.000055\n",
      "training loss: 1.338199 acc: 31.250, mean grad: 0.000003\n",
      "training loss: 2.759082 acc:  0.000, mean grad: 0.000046\n",
      "training loss: 1.351997 acc: 37.500, mean grad: -0.000004\n",
      "training loss: 2.244494 acc:  0.000, mean grad: 0.000033\n",
      "training loss: 1.400053 acc: 37.500, mean grad: -0.000008\n",
      "training loss: 1.884153 acc:  0.000, mean grad: 0.000023\n",
      "training loss: 1.450308 acc: 50.000, mean grad: -0.000008\n",
      "training loss: 1.678924 acc:  0.000, mean grad: 0.000019\n",
      "training loss: 1.488914 acc: 37.500, mean grad: -0.000005\n",
      "training loss: 1.562499 acc: 11.111, mean grad: 0.000013\n",
      "training loss: 1.516097 acc: 25.000, mean grad: -0.000006\n",
      "training loss: 1.496735 acc: 44.444, mean grad: 0.000010\n",
      "training loss: 1.535274 acc: 31.250, mean grad: -0.000009\n",
      "training loss: 1.459124 acc: 55.556, mean grad: 0.000009\n",
      "training loss: 1.747279 acc:  6.250, mean grad: 0.000065\n",
      "training loss: 2.957785 acc:  0.000, mean grad: -0.000009\n",
      "training loss: 1.669547 acc:  6.250, mean grad: 0.000058\n",
      "training loss: 2.578411 acc:  0.000, mean grad: -0.000010\n",
      "training loss: 1.544428 acc: 18.750, mean grad: 0.000036\n",
      "training loss: 2.112226 acc:  0.000, mean grad: -0.000007\n",
      "training loss: 1.487227 acc: 31.250, mean grad: 0.000011\n",
      "training loss: 1.783319 acc: 22.222, mean grad: -0.000007\n",
      "training loss: 1.480856 acc: 43.750, mean grad: -0.000003\n",
      "training loss: 1.588667 acc: 33.333, mean grad: -0.000004\n",
      "training loss: 1.489440 acc: 50.000, mean grad: -0.000008\n",
      "training loss: 1.475685 acc: 33.333, mean grad: -0.000006\n",
      "training loss: 1.499681 acc: 37.500, mean grad: -0.000008\n",
      "training loss: 1.409969 acc: 33.333, mean grad: -0.000006\n",
      "training loss: 1.508452 acc: 25.000, mean grad: -0.000013\n",
      "training loss: 1.371829 acc: 33.333, mean grad: -0.000008\n",
      "training loss: 1.260580 acc: 43.750, mean grad: -0.000017\n",
      "training loss: 3.101057 acc:  0.000, mean grad: 0.000024\n",
      "training loss: 1.268722 acc: 43.750, mean grad: -0.000015\n",
      "training loss: 2.729948 acc:  0.000, mean grad: 0.000020\n",
      "training loss: 1.302701 acc: 43.750, mean grad: -0.000015\n",
      "training loss: 2.238197 acc:  0.000, mean grad: 0.000011\n",
      "training loss: 1.360481 acc: 56.250, mean grad: -0.000014\n",
      "training loss: 1.887463 acc:  0.000, mean grad: 0.000001\n",
      "training loss: 1.418203 acc: 50.000, mean grad: -0.000015\n",
      "training loss: 1.681811 acc:  0.000, mean grad: -0.000005\n",
      "training loss: 1.464028 acc: 56.250, mean grad: -0.000015\n",
      "training loss: 1.564351 acc: 33.333, mean grad: -0.000009\n",
      "training loss: 1.497249 acc: 50.000, mean grad: -0.000015\n",
      "training loss: 1.497236 acc: 55.556, mean grad: -0.000010\n",
      "training loss: 1.520589 acc: 50.000, mean grad: -0.000014\n",
      "training loss: 1.459450 acc: 55.556, mean grad: -0.000012\n",
      "training loss: 1.401128 acc: 25.000, mean grad: 0.000025\n",
      "training loss: 3.109484 acc:  0.000, mean grad: 0.000047\n",
      "training loss: 1.399974 acc: 25.000, mean grad: 0.000024\n",
      "training loss: 2.722607 acc:  0.000, mean grad: 0.000045\n",
      "training loss: 1.400757 acc: 37.500, mean grad: 0.000016\n",
      "training loss: 2.184347 acc:  0.000, mean grad: 0.000039\n",
      "training loss: 1.415652 acc: 43.750, mean grad: 0.000015\n",
      "training loss: 1.779928 acc: 11.111, mean grad: 0.000023\n",
      "training loss: 1.438336 acc: 50.000, mean grad: 0.000012\n",
      "training loss: 1.534836 acc: 22.222, mean grad: 0.000006\n",
      "training loss: 1.457832 acc: 50.000, mean grad: 0.000007\n",
      "training loss: 1.399368 acc: 22.222, mean grad: -0.000001\n",
      "training loss: 1.469577 acc: 50.000, mean grad: 0.000004\n",
      "training loss: 1.324527 acc: 66.667, mean grad: -0.000004\n",
      "training loss: 1.475653 acc: 37.500, mean grad: 0.000004\n",
      "training loss: 1.283440 acc: 88.889, mean grad: -0.000005\n",
      "training loss: 1.240650 acc: 56.250, mean grad: -0.000034\n",
      "training loss: 3.272933 acc:  0.000, mean grad: -0.000049\n",
      "training loss: 1.227881 acc: 62.500, mean grad: -0.000030\n",
      "training loss: 2.871047 acc:  0.000, mean grad: -0.000039\n",
      "training loss: 1.222623 acc: 75.000, mean grad: -0.000022\n",
      "training loss: 2.336257 acc:  0.000, mean grad: -0.000035\n",
      "training loss: 1.250782 acc: 81.250, mean grad: -0.000014\n",
      "training loss: 1.939826 acc: 11.111, mean grad: -0.000031\n",
      "training loss: 1.291964 acc: 81.250, mean grad: -0.000006\n",
      "training loss: 1.704668 acc: 11.111, mean grad: -0.000034\n",
      "training loss: 1.329965 acc: 75.000, mean grad: -0.000008\n",
      "training loss: 1.570608 acc: 22.222, mean grad: -0.000030\n",
      "training loss: 1.360674 acc: 81.250, mean grad: -0.000009\n",
      "training loss: 1.493502 acc: 33.333, mean grad: -0.000026\n",
      "training loss: 1.385478 acc: 81.250, mean grad: -0.000010\n",
      "training loss: 1.449085 acc: 55.556, mean grad: -0.000024\n",
      "training loss: 1.503111 acc: 18.750, mean grad: -0.000030\n",
      "training loss: 3.067734 acc:  0.000, mean grad: -0.000082\n",
      "training loss: 1.475975 acc: 18.750, mean grad: -0.000024\n",
      "training loss: 2.675545 acc:  0.000, mean grad: -0.000072\n",
      "training loss: 1.452139 acc: 31.250, mean grad: -0.000018\n",
      "training loss: 2.177997 acc:  0.000, mean grad: -0.000062\n",
      "training loss: 1.463392 acc: 37.500, mean grad: -0.000015\n",
      "training loss: 1.834115 acc:  0.000, mean grad: -0.000046\n",
      "training loss: 1.486670 acc: 31.250, mean grad: -0.000011\n",
      "training loss: 1.645731 acc: 22.222, mean grad: -0.000032\n",
      "training loss: 1.506981 acc: 37.500, mean grad: -0.000009\n",
      "training loss: 1.545463 acc: 33.333, mean grad: -0.000024\n",
      "training loss: 1.521857 acc: 43.750, mean grad: -0.000008\n",
      "training loss: 1.488970 acc: 44.444, mean grad: -0.000021\n",
      "training loss: 1.533010 acc: 43.750, mean grad: -0.000007\n",
      "training loss: 1.456369 acc: 44.444, mean grad: -0.000014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.549265 acc: 18.750, mean grad: 0.000026\n",
      "training loss: 3.086335 acc:  0.000, mean grad: 0.000047\n",
      "training loss: 1.516106 acc: 31.250, mean grad: 0.000022\n",
      "training loss: 2.722183 acc:  0.000, mean grad: 0.000035\n",
      "training loss: 1.483240 acc: 31.250, mean grad: 0.000015\n",
      "training loss: 2.258080 acc:  0.000, mean grad: 0.000020\n",
      "training loss: 1.491593 acc: 31.250, mean grad: 0.000008\n",
      "training loss: 1.922149 acc: 11.111, mean grad: 0.000008\n",
      "training loss: 1.518305 acc: 31.250, mean grad: 0.000003\n",
      "training loss: 1.716406 acc: 22.222, mean grad: 0.000006\n",
      "training loss: 1.545050 acc: 25.000, mean grad: 0.000000\n",
      "training loss: 1.594480 acc: 22.222, mean grad: 0.000004\n",
      "training loss: 1.565626 acc: 25.000, mean grad: -0.000001\n",
      "training loss: 1.521824 acc: 55.556, mean grad: -0.000003\n",
      "training loss: 1.580047 acc: 12.500, mean grad: -0.000001\n",
      "training loss: 1.478067 acc: 66.667, mean grad: -0.000004\n",
      "training loss: 1.336753 acc: 31.250, mean grad: 0.000008\n",
      "training loss: 3.323504 acc:  0.000, mean grad: 0.000011\n",
      "training loss: 1.323191 acc: 43.750, mean grad: 0.000002\n",
      "training loss: 2.909509 acc:  0.000, mean grad: -0.000001\n",
      "training loss: 1.328255 acc: 43.750, mean grad: -0.000007\n",
      "training loss: 2.369916 acc:  0.000, mean grad: -0.000010\n",
      "training loss: 1.367004 acc: 37.500, mean grad: -0.000011\n",
      "training loss: 1.972916 acc:  0.000, mean grad: -0.000017\n",
      "training loss: 1.411912 acc: 43.750, mean grad: -0.000012\n",
      "training loss: 1.728950 acc:  0.000, mean grad: -0.000020\n",
      "training loss: 1.450271 acc: 37.500, mean grad: -0.000013\n",
      "training loss: 1.582175 acc: 44.444, mean grad: -0.000022\n",
      "training loss: 1.479329 acc: 43.750, mean grad: -0.000012\n",
      "training loss: 1.493185 acc: 66.667, mean grad: -0.000021\n",
      "training loss: 1.500537 acc: 31.250, mean grad: -0.000011\n",
      "training loss: 1.438899 acc: 66.667, mean grad: -0.000021\n",
      "training loss: 1.592081 acc: 25.000, mean grad: 0.000001\n",
      "training loss: 3.265929 acc:  0.000, mean grad: -0.000023\n",
      "training loss: 1.556173 acc: 25.000, mean grad: -0.000000\n",
      "training loss: 2.798486 acc:  0.000, mean grad: -0.000014\n",
      "training loss: 1.521806 acc: 31.250, mean grad: 0.000001\n",
      "training loss: 2.208522 acc:  0.000, mean grad: -0.000001\n",
      "training loss: 1.527754 acc: 31.250, mean grad: -0.000003\n",
      "training loss: 1.811779 acc:  0.000, mean grad: 0.000001\n",
      "training loss: 1.549154 acc: 37.500, mean grad: -0.000004\n",
      "training loss: 1.588020 acc: 11.111, mean grad: -0.000004\n",
      "training loss: 1.567006 acc: 12.500, mean grad: -0.000004\n",
      "training loss: 1.459033 acc: 33.333, mean grad: -0.000006\n",
      "training loss: 1.579386 acc: 12.500, mean grad: -0.000002\n",
      "training loss: 1.381863 acc: 66.667, mean grad: -0.000003\n",
      "training loss: 1.587095 acc:  6.250, mean grad: -0.000002\n",
      "training loss: 1.334410 acc: 77.778, mean grad: -0.000008\n",
      "training loss: 1.697049 acc: 12.500, mean grad: 0.000051\n",
      "training loss: 3.167047 acc:  0.000, mean grad: 0.000069\n",
      "training loss: 1.601907 acc: 12.500, mean grad: 0.000045\n",
      "training loss: 2.688895 acc:  0.000, mean grad: 0.000053\n",
      "training loss: 1.475563 acc: 18.750, mean grad: 0.000036\n",
      "training loss: 2.123849 acc:  0.000, mean grad: 0.000040\n",
      "training loss: 1.443211 acc: 31.250, mean grad: 0.000032\n",
      "training loss: 1.745360 acc: 11.111, mean grad: 0.000022\n",
      "training loss: 1.450489 acc: 25.000, mean grad: 0.000028\n",
      "training loss: 1.531181 acc: 33.333, mean grad: 0.000010\n",
      "training loss: 1.462903 acc: 25.000, mean grad: 0.000023\n",
      "training loss: 1.409777 acc: 44.444, mean grad: 0.000001\n",
      "training loss: 1.473529 acc: 31.250, mean grad: 0.000021\n",
      "training loss: 1.340250 acc: 66.667, mean grad: -0.000002\n",
      "training loss: 1.481875 acc: 31.250, mean grad: 0.000018\n",
      "training loss: 1.300564 acc: 66.667, mean grad: -0.000009\n",
      "training loss: 1.578169 acc: 25.000, mean grad: 0.000034\n",
      "training loss: 3.651301 acc:  0.000, mean grad: 0.000011\n",
      "training loss: 1.498721 acc: 25.000, mean grad: 0.000031\n",
      "training loss: 3.125501 acc:  0.000, mean grad: 0.000004\n",
      "training loss: 1.398591 acc: 31.250, mean grad: 0.000015\n",
      "training loss: 2.478351 acc:  0.000, mean grad: -0.000009\n",
      "training loss: 1.384520 acc: 50.000, mean grad: -0.000001\n",
      "training loss: 2.023003 acc: 11.111, mean grad: -0.000014\n",
      "training loss: 1.409233 acc: 43.750, mean grad: -0.000006\n",
      "training loss: 1.745621 acc: 11.111, mean grad: -0.000012\n",
      "training loss: 1.438289 acc: 50.000, mean grad: -0.000011\n",
      "training loss: 1.580686 acc: 33.333, mean grad: -0.000011\n",
      "training loss: 1.461005 acc: 50.000, mean grad: -0.000013\n",
      "training loss: 1.482142 acc: 55.556, mean grad: -0.000008\n",
      "training loss: 1.475818 acc: 37.500, mean grad: -0.000014\n",
      "training loss: 1.421624 acc: 77.778, mean grad: -0.000010\n",
      "training loss: 1.510339 acc: 31.250, mean grad: 0.000056\n",
      "training loss: 3.858593 acc:  0.000, mean grad: -0.000007\n",
      "training loss: 1.450335 acc: 31.250, mean grad: 0.000045\n",
      "training loss: 3.292064 acc:  0.000, mean grad: -0.000010\n",
      "training loss: 1.386864 acc: 31.250, mean grad: 0.000037\n",
      "training loss: 2.582979 acc:  0.000, mean grad: -0.000007\n",
      "training loss: 1.389367 acc: 37.500, mean grad: 0.000030\n",
      "training loss: 2.074931 acc:  0.000, mean grad: -0.000011\n",
      "training loss: 1.422165 acc: 43.750, mean grad: 0.000026\n",
      "training loss: 1.766948 acc:  0.000, mean grad: -0.000016\n",
      "training loss: 1.456958 acc: 68.750, mean grad: 0.000022\n",
      "training loss: 1.582842 acc: 33.333, mean grad: -0.000016\n",
      "training loss: 1.485296 acc: 62.500, mean grad: 0.000020\n",
      "training loss: 1.472727 acc: 44.444, mean grad: -0.000019\n",
      "training loss: 1.506814 acc: 43.750, mean grad: 0.000017\n",
      "training loss: 1.406521 acc: 77.778, mean grad: -0.000021\n",
      "training loss: 1.583788 acc: 18.750, mean grad: 0.000042\n",
      "training loss: 3.297466 acc:  0.000, mean grad: 0.000013\n",
      "training loss: 1.519083 acc: 25.000, mean grad: 0.000038\n",
      "training loss: 2.840240 acc:  0.000, mean grad: 0.000005\n",
      "training loss: 1.451595 acc: 31.250, mean grad: 0.000025\n",
      "training loss: 2.290905 acc:  0.000, mean grad: -0.000011\n",
      "training loss: 1.449687 acc: 31.250, mean grad: 0.000016\n",
      "training loss: 1.909686 acc:  0.000, mean grad: -0.000018\n",
      "training loss: 1.470772 acc: 37.500, mean grad: 0.000010\n",
      "training loss: 1.686008 acc: 11.111, mean grad: -0.000019\n",
      "training loss: 1.490722 acc: 37.500, mean grad: 0.000007\n",
      "training loss: 1.555887 acc: 33.333, mean grad: -0.000022\n",
      "training loss: 1.504874 acc: 31.250, mean grad: 0.000004\n",
      "training loss: 1.478468 acc: 44.444, mean grad: -0.000019\n",
      "training loss: 1.514329 acc: 31.250, mean grad: 0.000002\n",
      "training loss: 1.432449 acc: 66.667, mean grad: -0.000027\n",
      "training loss: 1.386619 acc: 37.500, mean grad: -0.000064\n",
      "training loss: 3.349784 acc:  0.000, mean grad: -0.000014\n",
      "training loss: 1.363784 acc: 43.750, mean grad: -0.000056\n",
      "training loss: 2.894218 acc:  0.000, mean grad: -0.000016\n",
      "training loss: 1.356954 acc: 50.000, mean grad: -0.000038\n",
      "training loss: 2.295419 acc:  0.000, mean grad: -0.000016\n",
      "training loss: 1.392703 acc: 50.000, mean grad: -0.000024\n",
      "training loss: 1.876544 acc:  0.000, mean grad: -0.000012\n",
      "training loss: 1.434259 acc: 43.750, mean grad: -0.000016\n",
      "training loss: 1.636271 acc: 22.222, mean grad: -0.000014\n",
      "training loss: 1.464267 acc: 50.000, mean grad: -0.000008\n",
      "training loss: 1.499882 acc: 33.333, mean grad: -0.000017\n",
      "training loss: 1.482586 acc: 50.000, mean grad: -0.000005\n",
      "training loss: 1.420082 acc: 44.444, mean grad: -0.000019\n",
      "training loss: 1.494483 acc: 50.000, mean grad: -0.000004\n",
      "training loss: 1.372967 acc: 55.556, mean grad: -0.000020\n",
      "training loss: 1.620685 acc: 18.750, mean grad: 0.000030\n",
      "training loss: 3.947110 acc:  0.000, mean grad: 0.000015\n",
      "training loss: 1.527326 acc: 18.750, mean grad: 0.000024\n",
      "training loss: 3.392341 acc:  0.000, mean grad: 0.000000\n",
      "training loss: 1.416443 acc: 25.000, mean grad: 0.000016\n",
      "training loss: 2.716199 acc:  0.000, mean grad: -0.000002\n",
      "training loss: 1.394300 acc: 31.250, mean grad: 0.000011\n",
      "training loss: 2.212561 acc:  0.000, mean grad: -0.000002\n",
      "training loss: 1.412423 acc: 50.000, mean grad: 0.000008\n",
      "training loss: 1.892180 acc:  0.000, mean grad: 0.000002\n",
      "training loss: 1.439256 acc: 56.250, mean grad: 0.000008\n",
      "training loss: 1.694374 acc: 33.333, mean grad: 0.000004\n",
      "training loss: 1.463658 acc: 50.000, mean grad: 0.000007\n",
      "training loss: 1.571794 acc: 44.444, mean grad: 0.000006\n",
      "training loss: 1.482539 acc: 37.500, mean grad: 0.000007\n",
      "training loss: 1.495529 acc: 44.444, mean grad: 0.000008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.389965 acc: 50.000, mean grad: 0.000050\n",
      "training loss: 3.349915 acc:  0.000, mean grad: 0.000025\n",
      "training loss: 1.351480 acc: 50.000, mean grad: 0.000046\n",
      "training loss: 2.826499 acc:  0.000, mean grad: 0.000015\n",
      "training loss: 1.328123 acc: 50.000, mean grad: 0.000034\n",
      "training loss: 2.153151 acc:  0.000, mean grad: 0.000005\n",
      "training loss: 1.367449 acc: 62.500, mean grad: 0.000020\n",
      "training loss: 1.688140 acc: 11.111, mean grad: 0.000003\n",
      "training loss: 1.422997 acc: 56.250, mean grad: 0.000015\n",
      "training loss: 1.434087 acc: 44.444, mean grad: -0.000002\n",
      "training loss: 1.464557 acc: 43.750, mean grad: 0.000012\n",
      "training loss: 1.296852 acc: 44.444, mean grad: -0.000003\n",
      "training loss: 1.489868 acc: 37.500, mean grad: 0.000008\n",
      "training loss: 1.221005 acc: 88.889, mean grad: -0.000003\n",
      "training loss: 1.505768 acc: 37.500, mean grad: 0.000006\n",
      "training loss: 1.178867 acc: 100.000, mean grad: -0.000006\n",
      "training loss: 1.365376 acc: 31.250, mean grad: 0.000028\n",
      "training loss: 3.282909 acc:  0.000, mean grad: -0.000029\n",
      "training loss: 1.337882 acc: 31.250, mean grad: 0.000024\n",
      "training loss: 2.794290 acc:  0.000, mean grad: -0.000038\n",
      "training loss: 1.320374 acc: 31.250, mean grad: 0.000014\n",
      "training loss: 2.149484 acc:  0.000, mean grad: -0.000048\n",
      "training loss: 1.352075 acc: 37.500, mean grad: 0.000006\n",
      "training loss: 1.701377 acc:  0.000, mean grad: -0.000051\n",
      "training loss: 1.397792 acc: 50.000, mean grad: 0.000002\n",
      "training loss: 1.460807 acc: 33.333, mean grad: -0.000049\n",
      "training loss: 1.432117 acc: 37.500, mean grad: 0.000001\n",
      "training loss: 1.335336 acc: 44.444, mean grad: -0.000045\n",
      "training loss: 1.455021 acc: 31.250, mean grad: -0.000002\n",
      "training loss: 1.267550 acc: 77.778, mean grad: -0.000044\n",
      "training loss: 1.470996 acc: 31.250, mean grad: -0.000004\n",
      "training loss: 1.230209 acc: 88.889, mean grad: -0.000041\n",
      "training loss: 1.426342 acc: 18.750, mean grad: -0.000010\n",
      "training loss: 3.879386 acc:  0.000, mean grad: 0.000022\n",
      "training loss: 1.381152 acc: 25.000, mean grad: -0.000008\n",
      "training loss: 3.384843 acc:  0.000, mean grad: 0.000019\n",
      "training loss: 1.334125 acc: 37.500, mean grad: -0.000006\n",
      "training loss: 2.747252 acc:  0.000, mean grad: 0.000010\n",
      "training loss: 1.338421 acc: 56.250, mean grad: -0.000006\n",
      "training loss: 2.266435 acc:  0.000, mean grad: 0.000001\n",
      "training loss: 1.367185 acc: 62.500, mean grad: -0.000009\n",
      "training loss: 1.956721 acc:  0.000, mean grad: -0.000003\n",
      "training loss: 1.401452 acc: 62.500, mean grad: -0.000011\n",
      "training loss: 1.763222 acc:  0.000, mean grad: -0.000007\n",
      "training loss: 1.431479 acc: 62.500, mean grad: -0.000010\n",
      "training loss: 1.641886 acc: 33.333, mean grad: -0.000010\n",
      "training loss: 1.455307 acc: 62.500, mean grad: -0.000010\n",
      "training loss: 1.565106 acc: 44.444, mean grad: -0.000013\n",
      "training loss: 1.888654 acc: 37.500, mean grad: 0.000060\n",
      "training loss: 3.546702 acc:  0.000, mean grad: 0.000008\n",
      "training loss: 1.772014 acc: 43.750, mean grad: 0.000051\n",
      "training loss: 3.036122 acc:  0.000, mean grad: 0.000004\n",
      "training loss: 1.592333 acc: 37.500, mean grad: 0.000033\n",
      "training loss: 2.394024 acc:  0.000, mean grad: -0.000014\n",
      "training loss: 1.497554 acc: 37.500, mean grad: 0.000016\n",
      "training loss: 1.942399 acc:  0.000, mean grad: -0.000027\n",
      "training loss: 1.476881 acc: 37.500, mean grad: 0.000006\n",
      "training loss: 1.684700 acc: 22.222, mean grad: -0.000030\n",
      "training loss: 1.483405 acc: 62.500, mean grad: 0.000002\n",
      "training loss: 1.539059 acc: 22.222, mean grad: -0.000032\n",
      "training loss: 1.495371 acc: 56.250, mean grad: -0.000001\n",
      "training loss: 1.455274 acc: 22.222, mean grad: -0.000033\n",
      "training loss: 1.506440 acc: 50.000, mean grad: -0.000002\n",
      "training loss: 1.405861 acc: 33.333, mean grad: -0.000029\n",
      "training loss: 1.563314 acc: 25.000, mean grad: -0.000044\n",
      "training loss: 3.810160 acc:  0.000, mean grad: 0.000024\n",
      "training loss: 1.509224 acc: 25.000, mean grad: -0.000040\n",
      "training loss: 3.254804 acc:  0.000, mean grad: 0.000017\n",
      "training loss: 1.448666 acc: 31.250, mean grad: -0.000026\n",
      "training loss: 2.556238 acc:  0.000, mean grad: 0.000017\n",
      "training loss: 1.432775 acc: 37.500, mean grad: -0.000021\n",
      "training loss: 2.065504 acc:  0.000, mean grad: 0.000005\n",
      "training loss: 1.447461 acc: 43.750, mean grad: -0.000017\n",
      "training loss: 1.775667 acc:  0.000, mean grad: -0.000000\n",
      "training loss: 1.467334 acc: 56.250, mean grad: -0.000013\n",
      "training loss: 1.608494 acc: 11.111, mean grad: -0.000003\n",
      "training loss: 1.482361 acc: 43.750, mean grad: -0.000012\n",
      "training loss: 1.510830 acc: 33.333, mean grad: -0.000004\n",
      "training loss: 1.493704 acc: 50.000, mean grad: -0.000013\n",
      "training loss: 1.450913 acc: 55.556, mean grad: -0.000004\n",
      "training loss: 1.552260 acc: 18.750, mean grad: 0.000065\n",
      "training loss: 3.723344 acc:  0.000, mean grad: -0.000007\n",
      "training loss: 1.475780 acc: 18.750, mean grad: 0.000056\n",
      "training loss: 3.209893 acc:  0.000, mean grad: -0.000018\n",
      "training loss: 1.392601 acc: 18.750, mean grad: 0.000039\n",
      "training loss: 2.578393 acc:  0.000, mean grad: -0.000021\n",
      "training loss: 1.395762 acc: 25.000, mean grad: 0.000027\n",
      "training loss: 2.135299 acc:  0.000, mean grad: -0.000019\n",
      "training loss: 1.430490 acc: 37.500, mean grad: 0.000020\n",
      "training loss: 1.871698 acc:  0.000, mean grad: -0.000017\n",
      "training loss: 1.463803 acc: 50.000, mean grad: 0.000013\n",
      "training loss: 1.715114 acc:  0.000, mean grad: -0.000017\n",
      "training loss: 1.488556 acc: 50.000, mean grad: 0.000009\n",
      "training loss: 1.619534 acc: 22.222, mean grad: -0.000015\n",
      "training loss: 1.505517 acc: 31.250, mean grad: 0.000005\n",
      "training loss: 1.560328 acc: 55.556, mean grad: -0.000012\n",
      "training loss: 1.572246 acc: 31.250, mean grad: 0.000050\n",
      "training loss: 3.423527 acc:  0.000, mean grad: 0.000044\n",
      "training loss: 1.505173 acc: 31.250, mean grad: 0.000044\n",
      "training loss: 2.918572 acc:  0.000, mean grad: 0.000029\n",
      "training loss: 1.428508 acc: 37.500, mean grad: 0.000029\n",
      "training loss: 2.272573 acc:  0.000, mean grad: 0.000007\n",
      "training loss: 1.434850 acc: 37.500, mean grad: 0.000017\n",
      "training loss: 1.828241 acc:  0.000, mean grad: -0.000007\n",
      "training loss: 1.471436 acc: 37.500, mean grad: 0.000016\n",
      "training loss: 1.579563 acc: 22.222, mean grad: -0.000014\n",
      "training loss: 1.502910 acc: 18.750, mean grad: 0.000010\n",
      "training loss: 1.442956 acc: 55.556, mean grad: -0.000021\n",
      "training loss: 1.523766 acc: 25.000, mean grad: 0.000011\n",
      "training loss: 1.365909 acc: 55.556, mean grad: -0.000023\n",
      "training loss: 1.536998 acc: 18.750, mean grad: 0.000009\n",
      "training loss: 1.322264 acc: 77.778, mean grad: -0.000027\n",
      "training loss: 1.439649 acc: 31.250, mean grad: 0.000024\n",
      "training loss: 3.862001 acc:  0.000, mean grad: 0.000001\n",
      "training loss: 1.378648 acc: 37.500, mean grad: 0.000019\n",
      "training loss: 3.311344 acc:  0.000, mean grad: -0.000004\n",
      "training loss: 1.313978 acc: 50.000, mean grad: 0.000013\n",
      "training loss: 2.595885 acc:  0.000, mean grad: -0.000014\n",
      "training loss: 1.323116 acc: 50.000, mean grad: 0.000005\n",
      "training loss: 2.073791 acc:  0.000, mean grad: -0.000020\n",
      "training loss: 1.368179 acc: 56.250, mean grad: 0.000004\n",
      "training loss: 1.760739 acc:  0.000, mean grad: -0.000023\n",
      "training loss: 1.415285 acc: 68.750, mean grad: -0.000002\n",
      "training loss: 1.580169 acc: 22.222, mean grad: -0.000023\n",
      "training loss: 1.452885 acc: 56.250, mean grad: -0.000002\n",
      "training loss: 1.476114 acc: 33.333, mean grad: -0.000024\n",
      "training loss: 1.480651 acc: 37.500, mean grad: -0.000003\n",
      "training loss: 1.415384 acc: 55.556, mean grad: -0.000025\n",
      "training loss: 1.439526 acc: 25.000, mean grad: 0.000024\n",
      "training loss: 3.806495 acc:  0.000, mean grad: 0.000006\n",
      "training loss: 1.407119 acc: 25.000, mean grad: 0.000019\n",
      "training loss: 3.268177 acc:  0.000, mean grad: 0.000002\n",
      "training loss: 1.370848 acc: 31.250, mean grad: 0.000012\n",
      "training loss: 2.547618 acc:  0.000, mean grad: -0.000009\n",
      "training loss: 1.379666 acc: 43.750, mean grad: 0.000000\n",
      "training loss: 2.033097 acc:  0.000, mean grad: -0.000011\n",
      "training loss: 1.417423 acc: 50.000, mean grad: -0.000006\n",
      "training loss: 1.732102 acc: 11.111, mean grad: -0.000018\n",
      "training loss: 1.457497 acc: 43.750, mean grad: -0.000010\n",
      "training loss: 1.558710 acc: 11.111, mean grad: -0.000019\n",
      "training loss: 1.488749 acc: 43.750, mean grad: -0.000015\n",
      "training loss: 1.457766 acc: 55.556, mean grad: -0.000014\n",
      "training loss: 1.511932 acc: 37.500, mean grad: -0.000016\n",
      "training loss: 1.398050 acc: 88.889, mean grad: -0.000014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.451885 acc: 25.000, mean grad: -0.000023\n",
      "training loss: 3.781022 acc:  0.000, mean grad: 0.000003\n",
      "training loss: 1.388539 acc: 18.750, mean grad: -0.000021\n",
      "training loss: 3.231961 acc:  0.000, mean grad: -0.000001\n",
      "training loss: 1.316175 acc: 18.750, mean grad: -0.000020\n",
      "training loss: 2.532265 acc:  0.000, mean grad: -0.000005\n",
      "training loss: 1.316269 acc: 43.750, mean grad: -0.000020\n",
      "training loss: 2.039227 acc:  0.000, mean grad: -0.000003\n",
      "training loss: 1.350302 acc: 50.000, mean grad: -0.000023\n",
      "training loss: 1.749289 acc: 11.111, mean grad: -0.000007\n",
      "training loss: 1.388495 acc: 62.500, mean grad: -0.000025\n",
      "training loss: 1.581099 acc: 11.111, mean grad: -0.000005\n",
      "training loss: 1.419435 acc: 56.250, mean grad: -0.000026\n",
      "training loss: 1.482603 acc: 33.333, mean grad: -0.000008\n",
      "training loss: 1.442812 acc: 56.250, mean grad: -0.000026\n",
      "training loss: 1.423997 acc: 66.667, mean grad: -0.000008\n",
      "training loss: 1.410143 acc: 31.250, mean grad: 0.000018\n",
      "training loss: 3.915694 acc:  0.000, mean grad: 0.000048\n",
      "training loss: 1.368399 acc: 37.500, mean grad: 0.000016\n",
      "training loss: 3.355474 acc:  0.000, mean grad: 0.000035\n",
      "training loss: 1.330018 acc: 43.750, mean grad: 0.000011\n",
      "training loss: 2.624344 acc:  0.000, mean grad: 0.000011\n",
      "training loss: 1.347780 acc: 43.750, mean grad: 0.000005\n",
      "training loss: 2.111453 acc:  0.000, mean grad: -0.000014\n",
      "training loss: 1.392797 acc: 50.000, mean grad: 0.000006\n",
      "training loss: 1.814197 acc:  0.000, mean grad: -0.000021\n",
      "training loss: 1.437135 acc: 62.500, mean grad: 0.000005\n",
      "training loss: 1.644454 acc: 11.111, mean grad: -0.000025\n",
      "training loss: 1.471684 acc: 56.250, mean grad: 0.000004\n",
      "training loss: 1.546161 acc: 44.444, mean grad: -0.000026\n",
      "training loss: 1.497217 acc: 56.250, mean grad: 0.000004\n",
      "training loss: 1.488851 acc: 66.667, mean grad: -0.000028\n",
      "training loss: 1.469534 acc: 31.250, mean grad: 0.000050\n",
      "training loss: 4.134532 acc:  0.000, mean grad: 0.000005\n",
      "training loss: 1.412784 acc: 31.250, mean grad: 0.000043\n",
      "training loss: 3.552144 acc:  0.000, mean grad: -0.000001\n",
      "training loss: 1.346263 acc: 31.250, mean grad: 0.000024\n",
      "training loss: 2.799266 acc:  0.000, mean grad: -0.000004\n",
      "training loss: 1.350837 acc: 50.000, mean grad: 0.000014\n",
      "training loss: 2.268333 acc:  0.000, mean grad: -0.000008\n",
      "training loss: 1.394552 acc: 43.750, mean grad: 0.000002\n",
      "training loss: 1.951247 acc:  0.000, mean grad: -0.000007\n",
      "training loss: 1.442284 acc: 62.500, mean grad: -0.000008\n",
      "training loss: 1.764748 acc:  0.000, mean grad: -0.000008\n",
      "training loss: 1.481652 acc: 68.750, mean grad: -0.000010\n",
      "training loss: 1.651972 acc: 11.111, mean grad: -0.000007\n",
      "training loss: 1.511120 acc: 43.750, mean grad: -0.000016\n",
      "training loss: 1.581769 acc: 33.333, mean grad: -0.000006\n",
      "training loss: 1.318147 acc: 37.500, mean grad: -0.000002\n",
      "training loss: 4.046031 acc:  0.000, mean grad: 0.000030\n",
      "training loss: 1.287080 acc: 37.500, mean grad: -0.000002\n",
      "training loss: 3.481711 acc:  0.000, mean grad: 0.000035\n",
      "training loss: 1.268721 acc: 37.500, mean grad: -0.000005\n",
      "training loss: 2.720843 acc:  0.000, mean grad: 0.000026\n",
      "training loss: 1.311307 acc: 43.750, mean grad: 0.000002\n",
      "training loss: 2.171571 acc:  0.000, mean grad: 0.000007\n",
      "training loss: 1.378044 acc: 56.250, mean grad: -0.000003\n",
      "training loss: 1.847855 acc:  0.000, mean grad: -0.000003\n",
      "training loss: 1.437376 acc: 56.250, mean grad: 0.000003\n",
      "training loss: 1.661338 acc: 11.111, mean grad: -0.000013\n",
      "training loss: 1.481681 acc: 43.750, mean grad: 0.000002\n",
      "training loss: 1.551872 acc: 44.444, mean grad: -0.000015\n",
      "training loss: 1.512719 acc: 25.000, mean grad: -0.000002\n",
      "training loss: 1.487048 acc: 44.444, mean grad: -0.000016\n",
      "training loss: 1.306736 acc: 25.000, mean grad: 0.000008\n",
      "training loss: 4.114223 acc:  0.000, mean grad: -0.000007\n",
      "training loss: 1.278538 acc: 25.000, mean grad: 0.000005\n",
      "training loss: 3.550259 acc:  0.000, mean grad: -0.000014\n",
      "training loss: 1.252651 acc: 25.000, mean grad: 0.000003\n",
      "training loss: 2.783448 acc:  0.000, mean grad: -0.000025\n",
      "training loss: 1.274253 acc: 31.250, mean grad: -0.000004\n",
      "training loss: 2.214418 acc:  0.000, mean grad: -0.000033\n",
      "training loss: 1.323449 acc: 37.500, mean grad: -0.000007\n",
      "training loss: 1.866998 acc:  0.000, mean grad: -0.000034\n",
      "training loss: 1.375162 acc: 50.000, mean grad: -0.000009\n",
      "training loss: 1.660546 acc:  0.000, mean grad: -0.000039\n",
      "training loss: 1.417680 acc: 50.000, mean grad: -0.000011\n",
      "training loss: 1.534456 acc: 44.444, mean grad: -0.000043\n",
      "training loss: 1.449369 acc: 43.750, mean grad: -0.000014\n",
      "training loss: 1.457701 acc: 44.444, mean grad: -0.000038\n",
      "training loss: 1.487150 acc: 18.750, mean grad: 0.000036\n",
      "training loss: 4.080143 acc:  0.000, mean grad: 0.000052\n",
      "training loss: 1.416663 acc: 18.750, mean grad: 0.000031\n",
      "training loss: 3.493198 acc:  0.000, mean grad: 0.000033\n",
      "training loss: 1.326046 acc: 25.000, mean grad: 0.000017\n",
      "training loss: 2.735505 acc:  0.000, mean grad: 0.000016\n",
      "training loss: 1.316463 acc: 43.750, mean grad: 0.000012\n",
      "training loss: 2.186692 acc:  0.000, mean grad: -0.000002\n",
      "training loss: 1.352935 acc: 50.000, mean grad: 0.000008\n",
      "training loss: 1.857374 acc:  0.000, mean grad: -0.000010\n",
      "training loss: 1.396152 acc: 56.250, mean grad: 0.000006\n",
      "training loss: 1.663915 acc: 11.111, mean grad: -0.000014\n",
      "training loss: 1.430804 acc: 50.000, mean grad: 0.000005\n",
      "training loss: 1.549242 acc: 44.444, mean grad: -0.000017\n",
      "training loss: 1.455719 acc: 43.750, mean grad: 0.000005\n",
      "training loss: 1.479708 acc: 55.556, mean grad: -0.000017\n",
      "training loss: 1.309407 acc: 31.250, mean grad: 0.000055\n",
      "training loss: 4.135717 acc:  0.000, mean grad: 0.000066\n",
      "training loss: 1.262296 acc: 50.000, mean grad: 0.000044\n",
      "training loss: 3.571345 acc:  0.000, mean grad: 0.000047\n",
      "training loss: 1.219531 acc: 50.000, mean grad: 0.000023\n",
      "training loss: 2.837913 acc:  0.000, mean grad: 0.000025\n",
      "training loss: 1.242621 acc: 50.000, mean grad: 0.000014\n",
      "training loss: 2.300479 acc:  0.000, mean grad: -0.000002\n",
      "training loss: 1.300641 acc: 56.250, mean grad: 0.000006\n",
      "training loss: 1.964108 acc:  0.000, mean grad: -0.000013\n",
      "training loss: 1.360568 acc: 62.500, mean grad: -0.000001\n",
      "training loss: 1.755855 acc:  0.000, mean grad: -0.000021\n",
      "training loss: 1.411245 acc: 56.250, mean grad: -0.000000\n",
      "training loss: 1.625392 acc:  0.000, mean grad: -0.000022\n",
      "training loss: 1.451025 acc: 43.750, mean grad: -0.000003\n",
      "training loss: 1.542034 acc: 22.222, mean grad: -0.000023\n",
      "training loss: 1.602591 acc:  6.250, mean grad: 0.000048\n",
      "training loss: 4.302184 acc:  0.000, mean grad: 0.000052\n",
      "training loss: 1.520958 acc:  6.250, mean grad: 0.000040\n",
      "training loss: 3.696188 acc:  0.000, mean grad: 0.000031\n",
      "training loss: 1.395525 acc:  6.250, mean grad: 0.000035\n",
      "training loss: 2.904183 acc:  0.000, mean grad: -0.000009\n",
      "training loss: 1.350544 acc: 37.500, mean grad: 0.000021\n",
      "training loss: 2.312213 acc:  0.000, mean grad: -0.000029\n",
      "training loss: 1.362295 acc: 43.750, mean grad: 0.000015\n",
      "training loss: 1.938764 acc:  0.000, mean grad: -0.000039\n",
      "training loss: 1.390972 acc: 43.750, mean grad: 0.000013\n",
      "training loss: 1.710978 acc:  0.000, mean grad: -0.000041\n",
      "training loss: 1.419199 acc: 62.500, mean grad: 0.000011\n",
      "training loss: 1.570834 acc: 22.222, mean grad: -0.000045\n",
      "training loss: 1.442648 acc: 50.000, mean grad: 0.000010\n",
      "training loss: 1.482605 acc: 44.444, mean grad: -0.000046\n",
      "training loss: 1.666440 acc: 12.500, mean grad: 0.000032\n",
      "training loss: 4.447417 acc:  0.000, mean grad: 0.000059\n",
      "training loss: 1.575315 acc: 12.500, mean grad: 0.000028\n",
      "training loss: 3.789396 acc:  0.000, mean grad: 0.000048\n",
      "training loss: 1.436981 acc: 12.500, mean grad: 0.000022\n",
      "training loss: 2.930338 acc:  0.000, mean grad: 0.000020\n",
      "training loss: 1.382615 acc: 18.750, mean grad: 0.000016\n",
      "training loss: 2.305277 acc:  0.000, mean grad: 0.000001\n",
      "training loss: 1.391517 acc: 31.250, mean grad: 0.000011\n",
      "training loss: 1.925530 acc:  0.000, mean grad: -0.000008\n",
      "training loss: 1.419832 acc: 37.500, mean grad: 0.000008\n",
      "training loss: 1.702831 acc: 11.111, mean grad: -0.000014\n",
      "training loss: 1.447787 acc: 37.500, mean grad: 0.000007\n",
      "training loss: 1.570888 acc: 33.333, mean grad: -0.000017\n",
      "training loss: 1.469985 acc: 43.750, mean grad: 0.000007\n",
      "training loss: 1.491702 acc: 55.556, mean grad: -0.000018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.294435 acc: 31.250, mean grad: 0.000021\n",
      "training loss: 4.318246 acc:  0.000, mean grad: 0.000072\n",
      "training loss: 1.271008 acc: 31.250, mean grad: 0.000014\n",
      "training loss: 3.763346 acc:  0.000, mean grad: 0.000063\n",
      "training loss: 1.242667 acc: 43.750, mean grad: -0.000002\n",
      "training loss: 2.989637 acc:  0.000, mean grad: 0.000050\n",
      "training loss: 1.250430 acc: 56.250, mean grad: -0.000006\n",
      "training loss: 2.396495 acc:  0.000, mean grad: 0.000030\n",
      "training loss: 1.290722 acc: 56.250, mean grad: -0.000010\n",
      "training loss: 2.021424 acc:  0.000, mean grad: 0.000017\n",
      "training loss: 1.339007 acc: 56.250, mean grad: -0.000013\n",
      "training loss: 1.793485 acc:  0.000, mean grad: 0.000010\n",
      "training loss: 1.382505 acc: 56.250, mean grad: -0.000015\n",
      "training loss: 1.654942 acc:  0.000, mean grad: 0.000002\n",
      "training loss: 1.417350 acc: 50.000, mean grad: -0.000015\n",
      "training loss: 1.569428 acc: 11.111, mean grad: -0.000002\n",
      "training loss: 1.636535 acc: 18.750, mean grad: 0.000058\n",
      "training loss: 4.410750 acc:  0.000, mean grad: -0.000008\n",
      "training loss: 1.563911 acc: 25.000, mean grad: 0.000051\n",
      "training loss: 3.757611 acc:  0.000, mean grad: -0.000012\n",
      "training loss: 1.454452 acc: 25.000, mean grad: 0.000031\n",
      "training loss: 2.884917 acc:  0.000, mean grad: -0.000021\n",
      "training loss: 1.411482 acc: 31.250, mean grad: 0.000021\n",
      "training loss: 2.253076 acc:  0.000, mean grad: -0.000023\n",
      "training loss: 1.422082 acc: 43.750, mean grad: 0.000012\n",
      "training loss: 1.878314 acc:  0.000, mean grad: -0.000022\n",
      "training loss: 1.449433 acc: 43.750, mean grad: 0.000008\n",
      "training loss: 1.661436 acc: 11.111, mean grad: -0.000023\n",
      "training loss: 1.476719 acc: 37.500, mean grad: 0.000005\n",
      "training loss: 1.533713 acc: 33.333, mean grad: -0.000022\n",
      "training loss: 1.498402 acc: 43.750, mean grad: 0.000001\n",
      "training loss: 1.458076 acc: 55.556, mean grad: -0.000018\n",
      "training loss: 1.499320 acc: 31.250, mean grad: -0.000008\n",
      "training loss: 4.254942 acc:  0.000, mean grad: 0.000041\n",
      "training loss: 1.442118 acc: 37.500, mean grad: -0.000010\n",
      "training loss: 3.693224 acc:  0.000, mean grad: 0.000034\n",
      "training loss: 1.360070 acc: 50.000, mean grad: -0.000013\n",
      "training loss: 2.941781 acc:  0.000, mean grad: 0.000019\n",
      "training loss: 1.337857 acc: 43.750, mean grad: -0.000014\n",
      "training loss: 2.373480 acc:  0.000, mean grad: 0.000009\n",
      "training loss: 1.360231 acc: 43.750, mean grad: -0.000014\n",
      "training loss: 2.013671 acc:  0.000, mean grad: -0.000001\n",
      "training loss: 1.397405 acc: 50.000, mean grad: -0.000014\n",
      "training loss: 1.792521 acc:  0.000, mean grad: -0.000004\n",
      "training loss: 1.433901 acc: 56.250, mean grad: -0.000014\n",
      "training loss: 1.656085 acc: 11.111, mean grad: -0.000007\n",
      "training loss: 1.464776 acc: 50.000, mean grad: -0.000014\n",
      "training loss: 1.570737 acc: 33.333, mean grad: -0.000010\n",
      "training loss: 1.798609 acc: 25.000, mean grad: 0.000073\n",
      "training loss: 4.296864 acc:  0.000, mean grad: 0.000059\n",
      "training loss: 1.696397 acc: 25.000, mean grad: 0.000066\n",
      "training loss: 3.697989 acc:  0.000, mean grad: 0.000044\n",
      "training loss: 1.518015 acc: 31.250, mean grad: 0.000043\n",
      "training loss: 2.915104 acc:  0.000, mean grad: 0.000033\n",
      "training loss: 1.425114 acc: 31.250, mean grad: 0.000022\n",
      "training loss: 2.341147 acc:  0.000, mean grad: 0.000023\n",
      "training loss: 1.413180 acc: 43.750, mean grad: 0.000013\n",
      "training loss: 1.986949 acc:  0.000, mean grad: 0.000017\n",
      "training loss: 1.434423 acc: 43.750, mean grad: 0.000008\n",
      "training loss: 1.771986 acc:  0.000, mean grad: 0.000012\n",
      "training loss: 1.461155 acc: 43.750, mean grad: 0.000003\n",
      "training loss: 1.640167 acc: 11.111, mean grad: 0.000008\n",
      "training loss: 1.485047 acc: 43.750, mean grad: 0.000002\n",
      "training loss: 1.557612 acc: 33.333, mean grad: 0.000008\n",
      "training loss: 1.516439 acc: 18.750, mean grad: 0.000038\n",
      "training loss: 4.474941 acc:  0.000, mean grad: 0.000071\n",
      "training loss: 1.457884 acc: 18.750, mean grad: 0.000031\n",
      "training loss: 3.821722 acc:  0.000, mean grad: 0.000052\n",
      "training loss: 1.372423 acc: 31.250, mean grad: 0.000020\n",
      "training loss: 2.946556 acc:  0.000, mean grad: 0.000036\n",
      "training loss: 1.355212 acc: 37.500, mean grad: 0.000009\n",
      "training loss: 2.313125 acc:  0.000, mean grad: 0.000016\n",
      "training loss: 1.387851 acc: 50.000, mean grad: 0.000001\n",
      "training loss: 1.935204 acc:  0.000, mean grad: 0.000003\n",
      "training loss: 1.432498 acc: 56.250, mean grad: -0.000002\n",
      "training loss: 1.716943 acc:  0.000, mean grad: -0.000003\n",
      "training loss: 1.472119 acc: 50.000, mean grad: -0.000004\n",
      "training loss: 1.589502 acc: 44.444, mean grad: -0.000006\n",
      "training loss: 1.502944 acc: 50.000, mean grad: -0.000005\n",
      "training loss: 1.514321 acc: 66.667, mean grad: -0.000008\n",
      "training loss: 1.626292 acc: 31.250, mean grad: -0.000038\n",
      "training loss: 4.535253 acc:  0.000, mean grad: 0.000045\n",
      "training loss: 1.556763 acc: 25.000, mean grad: -0.000034\n",
      "training loss: 3.881051 acc:  0.000, mean grad: 0.000041\n",
      "training loss: 1.440850 acc: 31.250, mean grad: -0.000028\n",
      "training loss: 3.009576 acc:  0.000, mean grad: 0.000030\n",
      "training loss: 1.389543 acc: 31.250, mean grad: -0.000021\n",
      "training loss: 2.368668 acc:  0.000, mean grad: 0.000015\n",
      "training loss: 1.396125 acc: 37.500, mean grad: -0.000017\n",
      "training loss: 1.976379 acc:  0.000, mean grad: 0.000003\n",
      "training loss: 1.424258 acc: 56.250, mean grad: -0.000017\n",
      "training loss: 1.744443 acc:  0.000, mean grad: -0.000003\n",
      "training loss: 1.451658 acc: 56.250, mean grad: -0.000014\n",
      "training loss: 1.604891 acc: 11.111, mean grad: -0.000002\n",
      "training loss: 1.474250 acc: 56.250, mean grad: -0.000013\n",
      "training loss: 1.520202 acc: 33.333, mean grad: -0.000004\n",
      "training loss: 1.345697 acc: 25.000, mean grad: 0.000035\n",
      "training loss: 4.178209 acc:  0.000, mean grad: 0.000043\n",
      "training loss: 1.313700 acc: 25.000, mean grad: 0.000032\n",
      "training loss: 3.577477 acc:  0.000, mean grad: 0.000032\n",
      "training loss: 1.278451 acc: 37.500, mean grad: 0.000022\n",
      "training loss: 2.767747 acc:  0.000, mean grad: 0.000015\n",
      "training loss: 1.299412 acc: 37.500, mean grad: 0.000013\n",
      "training loss: 2.182395 acc:  0.000, mean grad: 0.000002\n",
      "training loss: 1.352558 acc: 37.500, mean grad: 0.000008\n",
      "training loss: 1.833852 acc:  0.000, mean grad: -0.000005\n",
      "training loss: 1.406347 acc: 37.500, mean grad: 0.000006\n",
      "training loss: 1.630818 acc: 11.111, mean grad: -0.000010\n",
      "training loss: 1.449876 acc: 43.750, mean grad: 0.000006\n",
      "training loss: 1.510711 acc: 33.333, mean grad: -0.000013\n",
      "training loss: 1.482580 acc: 31.250, mean grad: 0.000007\n",
      "training loss: 1.438836 acc: 77.778, mean grad: -0.000017\n",
      "training loss: 1.462560 acc: 18.750, mean grad: 0.000003\n",
      "training loss: 4.682249 acc:  0.000, mean grad: 0.000089\n",
      "training loss: 1.405500 acc: 25.000, mean grad: 0.000005\n",
      "training loss: 4.003002 acc:  0.000, mean grad: 0.000077\n",
      "training loss: 1.305935 acc: 43.750, mean grad: 0.000005\n",
      "training loss: 3.072170 acc:  0.000, mean grad: 0.000045\n",
      "training loss: 1.268619 acc: 56.250, mean grad: 0.000001\n",
      "training loss: 2.385764 acc:  0.000, mean grad: 0.000016\n",
      "training loss: 1.289064 acc: 68.750, mean grad: -0.000000\n",
      "training loss: 1.964058 acc:  0.000, mean grad: -0.000003\n",
      "training loss: 1.326233 acc: 68.750, mean grad: 0.000001\n",
      "training loss: 1.714731 acc:  0.000, mean grad: -0.000015\n",
      "training loss: 1.361884 acc: 75.000, mean grad: 0.000001\n",
      "training loss: 1.566458 acc:  0.000, mean grad: -0.000024\n",
      "training loss: 1.391332 acc: 75.000, mean grad: -0.000001\n",
      "training loss: 1.477861 acc: 33.333, mean grad: -0.000028\n",
      "training loss: 1.317829 acc: 25.000, mean grad: 0.000010\n",
      "training loss: 4.351010 acc:  0.000, mean grad: 0.000043\n",
      "training loss: 1.277861 acc: 31.250, mean grad: 0.000008\n",
      "training loss: 3.796839 acc:  0.000, mean grad: 0.000030\n",
      "training loss: 1.214351 acc: 43.750, mean grad: 0.000005\n",
      "training loss: 3.046596 acc:  0.000, mean grad: 0.000012\n",
      "training loss: 1.200581 acc: 62.500, mean grad: 0.000001\n",
      "training loss: 2.485028 acc:  0.000, mean grad: -0.000004\n",
      "training loss: 1.227821 acc: 62.500, mean grad: -0.000003\n",
      "training loss: 2.128632 acc:  0.000, mean grad: -0.000019\n",
      "training loss: 1.267990 acc: 62.500, mean grad: -0.000006\n",
      "training loss: 1.907092 acc:  0.000, mean grad: -0.000024\n",
      "training loss: 1.306467 acc: 68.750, mean grad: -0.000007\n",
      "training loss: 1.767582 acc:  0.000, mean grad: -0.000028\n",
      "training loss: 1.339532 acc: 68.750, mean grad: -0.000009\n",
      "training loss: 1.677280 acc:  0.000, mean grad: -0.000029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.293408 acc: 31.250, mean grad: 0.000016\n",
      "training loss: 4.312894 acc:  0.000, mean grad: 0.000059\n",
      "training loss: 1.260348 acc: 31.250, mean grad: 0.000015\n",
      "training loss: 3.720060 acc:  0.000, mean grad: 0.000053\n",
      "training loss: 1.211076 acc: 37.500, mean grad: 0.000008\n",
      "training loss: 2.914840 acc:  0.000, mean grad: 0.000040\n",
      "training loss: 1.209865 acc: 37.500, mean grad: -0.000001\n",
      "training loss: 2.326945 acc:  0.000, mean grad: 0.000019\n",
      "training loss: 1.246050 acc: 56.250, mean grad: -0.000007\n",
      "training loss: 1.967332 acc:  0.000, mean grad: 0.000008\n",
      "training loss: 1.291623 acc: 62.500, mean grad: -0.000006\n",
      "training loss: 1.751325 acc: 11.111, mean grad: -0.000001\n",
      "training loss: 1.333526 acc: 62.500, mean grad: -0.000007\n",
      "training loss: 1.619833 acc: 22.222, mean grad: -0.000006\n",
      "training loss: 1.368864 acc: 56.250, mean grad: -0.000006\n",
      "training loss: 1.538172 acc: 33.333, mean grad: -0.000009\n",
      "training loss: 1.283694 acc: 50.000, mean grad: -0.000011\n",
      "training loss: 4.156086 acc:  0.000, mean grad: 0.000020\n",
      "training loss: 1.249092 acc: 50.000, mean grad: -0.000013\n",
      "training loss: 3.543506 acc:  0.000, mean grad: 0.000014\n",
      "training loss: 1.204411 acc: 50.000, mean grad: -0.000014\n",
      "training loss: 2.697531 acc:  0.000, mean grad: 0.000007\n",
      "training loss: 1.210284 acc: 50.000, mean grad: -0.000014\n",
      "training loss: 2.081237 acc:  0.000, mean grad: -0.000002\n",
      "training loss: 1.253744 acc: 68.750, mean grad: -0.000013\n",
      "training loss: 1.723028 acc:  0.000, mean grad: -0.000010\n",
      "training loss: 1.303549 acc: 68.750, mean grad: -0.000012\n",
      "training loss: 1.523132 acc: 33.333, mean grad: -0.000014\n",
      "training loss: 1.345690 acc: 75.000, mean grad: -0.000011\n",
      "training loss: 1.409985 acc: 44.444, mean grad: -0.000015\n",
      "training loss: 1.378909 acc: 68.750, mean grad: -0.000012\n",
      "training loss: 1.344904 acc: 55.556, mean grad: -0.000015\n",
      "training loss: 1.175840 acc: 43.750, mean grad: 0.000014\n",
      "training loss: 4.759784 acc:  0.000, mean grad: 0.000003\n",
      "training loss: 1.150095 acc: 43.750, mean grad: 0.000008\n",
      "training loss: 4.075376 acc:  0.000, mean grad: 0.000000\n",
      "training loss: 1.118240 acc: 43.750, mean grad: -0.000001\n",
      "training loss: 3.105560 acc:  0.000, mean grad: -0.000005\n",
      "training loss: 1.134955 acc: 68.750, mean grad: -0.000014\n",
      "training loss: 2.363116 acc:  0.000, mean grad: -0.000009\n",
      "training loss: 1.189677 acc: 75.000, mean grad: -0.000023\n",
      "training loss: 1.900467 acc:  0.000, mean grad: -0.000015\n",
      "training loss: 1.251171 acc: 75.000, mean grad: -0.000025\n",
      "training loss: 1.632749 acc:  0.000, mean grad: -0.000019\n",
      "training loss: 1.302399 acc: 81.250, mean grad: -0.000021\n",
      "training loss: 1.481528 acc: 22.222, mean grad: -0.000018\n",
      "training loss: 1.341754 acc: 75.000, mean grad: -0.000020\n",
      "training loss: 1.395471 acc: 55.556, mean grad: -0.000014\n",
      "training loss: 1.568500 acc: 12.500, mean grad: 0.000044\n",
      "training loss: 4.389092 acc:  0.000, mean grad: 0.000065\n",
      "training loss: 1.484884 acc: 12.500, mean grad: 0.000035\n",
      "training loss: 3.742560 acc:  0.000, mean grad: 0.000054\n",
      "training loss: 1.354765 acc: 31.250, mean grad: 0.000021\n",
      "training loss: 2.878098 acc:  0.000, mean grad: 0.000042\n",
      "training loss: 1.313860 acc: 37.500, mean grad: 0.000007\n",
      "training loss: 2.238453 acc:  0.000, mean grad: 0.000021\n",
      "training loss: 1.335278 acc: 43.750, mean grad: -0.000002\n",
      "training loss: 1.848540 acc:  0.000, mean grad: 0.000009\n",
      "training loss: 1.373802 acc: 62.500, mean grad: -0.000008\n",
      "training loss: 1.620575 acc: 22.222, mean grad: -0.000001\n",
      "training loss: 1.409548 acc: 50.000, mean grad: -0.000013\n",
      "training loss: 1.488786 acc: 33.333, mean grad: -0.000008\n",
      "training loss: 1.438412 acc: 43.750, mean grad: -0.000016\n",
      "training loss: 1.412072 acc: 55.556, mean grad: -0.000012\n",
      "training loss: 1.425340 acc: 31.250, mean grad: 0.000024\n",
      "training loss: 4.571637 acc:  0.000, mean grad: 0.000061\n",
      "training loss: 1.370683 acc: 37.500, mean grad: 0.000018\n",
      "training loss: 3.933630 acc:  0.000, mean grad: 0.000050\n",
      "training loss: 1.286428 acc: 43.750, mean grad: 0.000004\n",
      "training loss: 3.073997 acc:  0.000, mean grad: 0.000042\n",
      "training loss: 1.265925 acc: 62.500, mean grad: -0.000002\n",
      "training loss: 2.448057 acc:  0.000, mean grad: 0.000023\n",
      "training loss: 1.289576 acc: 75.000, mean grad: -0.000004\n",
      "training loss: 2.063844 acc:  0.000, mean grad: 0.000011\n",
      "training loss: 1.327703 acc: 81.250, mean grad: -0.000007\n",
      "training loss: 1.834536 acc:  0.000, mean grad: -0.000001\n",
      "training loss: 1.365125 acc: 75.000, mean grad: -0.000009\n",
      "training loss: 1.694966 acc: 11.111, mean grad: -0.000005\n",
      "training loss: 1.396722 acc: 81.250, mean grad: -0.000010\n",
      "training loss: 1.608288 acc: 11.111, mean grad: -0.000009\n",
      "training loss: 1.331761 acc: 50.000, mean grad: 0.000044\n",
      "training loss: 4.471543 acc:  0.000, mean grad: 0.000077\n",
      "training loss: 1.285505 acc: 56.250, mean grad: 0.000038\n",
      "training loss: 3.746614 acc:  0.000, mean grad: 0.000064\n",
      "training loss: 1.218820 acc: 62.500, mean grad: 0.000028\n",
      "training loss: 2.747063 acc:  0.000, mean grad: 0.000035\n",
      "training loss: 1.206109 acc: 68.750, mean grad: 0.000014\n",
      "training loss: 2.045370 acc:  0.000, mean grad: 0.000010\n",
      "training loss: 1.238370 acc: 75.000, mean grad: 0.000002\n",
      "training loss: 1.655965 acc:  0.000, mean grad: -0.000007\n",
      "training loss: 1.281947 acc: 62.500, mean grad: -0.000006\n",
      "training loss: 1.447401 acc: 33.333, mean grad: -0.000015\n",
      "training loss: 1.318972 acc: 56.250, mean grad: -0.000011\n",
      "training loss: 1.332514 acc: 66.667, mean grad: -0.000020\n",
      "training loss: 1.347971 acc: 56.250, mean grad: -0.000011\n",
      "training loss: 1.268946 acc: 88.889, mean grad: -0.000026\n",
      "training loss: 1.375229 acc: 37.500, mean grad: -0.000019\n",
      "training loss: 4.668074 acc:  0.000, mean grad: 0.000114\n",
      "training loss: 1.319946 acc: 37.500, mean grad: -0.000022\n",
      "training loss: 3.950526 acc:  0.000, mean grad: 0.000091\n",
      "training loss: 1.255551 acc: 43.750, mean grad: -0.000025\n",
      "training loss: 2.985536 acc:  0.000, mean grad: 0.000051\n",
      "training loss: 1.265134 acc: 50.000, mean grad: -0.000026\n",
      "training loss: 2.281210 acc:  0.000, mean grad: 0.000016\n",
      "training loss: 1.317093 acc: 62.500, mean grad: -0.000025\n",
      "training loss: 1.851133 acc:  0.000, mean grad: -0.000002\n",
      "training loss: 1.373191 acc: 68.750, mean grad: -0.000024\n",
      "training loss: 1.598373 acc: 22.222, mean grad: -0.000019\n",
      "training loss: 1.418141 acc: 50.000, mean grad: -0.000022\n",
      "training loss: 1.450027 acc: 44.444, mean grad: -0.000021\n",
      "training loss: 1.450378 acc: 37.500, mean grad: -0.000020\n",
      "training loss: 1.362081 acc: 66.667, mean grad: -0.000020\n",
      "training loss: 1.422711 acc: 31.250, mean grad: 0.000002\n",
      "training loss: 4.310819 acc:  0.000, mean grad: 0.000056\n",
      "training loss: 1.359450 acc: 31.250, mean grad: -0.000001\n",
      "training loss: 3.719734 acc:  0.000, mean grad: 0.000042\n",
      "training loss: 1.263822 acc: 31.250, mean grad: -0.000008\n",
      "training loss: 2.932151 acc:  0.000, mean grad: 0.000022\n",
      "training loss: 1.237582 acc: 62.500, mean grad: -0.000011\n",
      "training loss: 2.358571 acc:  0.000, mean grad: 0.000008\n",
      "training loss: 1.262414 acc: 75.000, mean grad: -0.000014\n",
      "training loss: 2.005970 acc:  0.000, mean grad: -0.000005\n",
      "training loss: 1.302715 acc: 81.250, mean grad: -0.000016\n",
      "training loss: 1.792557 acc:  0.000, mean grad: -0.000007\n",
      "training loss: 1.341004 acc: 75.000, mean grad: -0.000017\n",
      "training loss: 1.661201 acc: 11.111, mean grad: -0.000010\n",
      "training loss: 1.373154 acc: 81.250, mean grad: -0.000018\n",
      "training loss: 1.578999 acc: 33.333, mean grad: -0.000013\n",
      "training loss: 1.550265 acc: 25.000, mean grad: 0.000020\n",
      "training loss: 4.623297 acc:  0.000, mean grad: 0.000076\n",
      "training loss: 1.480727 acc: 25.000, mean grad: 0.000017\n",
      "training loss: 3.868920 acc:  0.000, mean grad: 0.000048\n",
      "training loss: 1.378787 acc: 37.500, mean grad: 0.000011\n",
      "training loss: 2.846870 acc:  0.000, mean grad: 0.000021\n",
      "training loss: 1.351950 acc: 43.750, mean grad: 0.000002\n",
      "training loss: 2.116377 acc:  0.000, mean grad: -0.000007\n",
      "training loss: 1.379736 acc: 56.250, mean grad: -0.000003\n",
      "training loss: 1.695976 acc:  0.000, mean grad: -0.000018\n",
      "training loss: 1.417420 acc: 50.000, mean grad: -0.000003\n",
      "training loss: 1.464786 acc: 22.222, mean grad: -0.000026\n",
      "training loss: 1.446793 acc: 25.000, mean grad: -0.000003\n",
      "training loss: 1.336884 acc: 55.556, mean grad: -0.000031\n",
      "training loss: 1.466729 acc: 25.000, mean grad: -0.000001\n",
      "training loss: 1.265975 acc: 88.889, mean grad: -0.000035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.513526 acc: 25.000, mean grad: 0.000043\n",
      "training loss: 4.403507 acc:  0.000, mean grad: 0.000018\n",
      "training loss: 1.456786 acc: 25.000, mean grad: 0.000037\n",
      "training loss: 3.749807 acc:  0.000, mean grad: 0.000016\n",
      "training loss: 1.376466 acc: 31.250, mean grad: 0.000023\n",
      "training loss: 2.855262 acc:  0.000, mean grad: 0.000004\n",
      "training loss: 1.368292 acc: 37.500, mean grad: 0.000011\n",
      "training loss: 2.202459 acc:  0.000, mean grad: -0.000011\n",
      "training loss: 1.407878 acc: 31.250, mean grad: 0.000005\n",
      "training loss: 1.812854 acc:  0.000, mean grad: -0.000019\n",
      "training loss: 1.455032 acc: 25.000, mean grad: 0.000001\n",
      "training loss: 1.591144 acc: 11.111, mean grad: -0.000020\n",
      "training loss: 1.492543 acc: 31.250, mean grad: -0.000003\n",
      "training loss: 1.464581 acc: 44.444, mean grad: -0.000024\n",
      "training loss: 1.519831 acc: 25.000, mean grad: -0.000005\n",
      "training loss: 1.391081 acc: 88.889, mean grad: -0.000025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[  200/  500] loss: 1.6160 (1.6043), acc: 33.333% (23.410%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.327469 acc: 37.500, mean grad: 0.000034\n",
      "training loss: 4.517234 acc:  0.000, mean grad: 0.000083\n",
      "training loss: 1.280630 acc: 37.500, mean grad: 0.000023\n",
      "training loss: 3.809595 acc:  0.000, mean grad: 0.000061\n",
      "training loss: 1.233581 acc: 37.500, mean grad: 0.000005\n",
      "training loss: 2.890921 acc:  0.000, mean grad: 0.000038\n",
      "training loss: 1.242794 acc: 37.500, mean grad: -0.000010\n",
      "training loss: 2.258616 acc:  0.000, mean grad: 0.000010\n",
      "training loss: 1.285854 acc: 37.500, mean grad: -0.000018\n",
      "training loss: 1.886380 acc:  0.000, mean grad: -0.000015\n",
      "training loss: 1.340682 acc: 37.500, mean grad: -0.000011\n",
      "training loss: 1.669206 acc: 11.111, mean grad: -0.000020\n",
      "training loss: 1.387596 acc: 50.000, mean grad: -0.000012\n",
      "training loss: 1.535256 acc: 33.333, mean grad: -0.000026\n",
      "training loss: 1.421999 acc: 43.750, mean grad: -0.000010\n",
      "training loss: 1.448107 acc: 44.444, mean grad: -0.000033\n",
      "training loss: 1.525599 acc: 31.250, mean grad: 0.000025\n",
      "training loss: 5.102935 acc:  0.000, mean grad: 0.000078\n",
      "training loss: 1.444720 acc: 31.250, mean grad: 0.000014\n",
      "training loss: 4.277565 acc:  0.000, mean grad: 0.000069\n",
      "training loss: 1.344846 acc: 37.500, mean grad: -0.000002\n",
      "training loss: 3.171613 acc:  0.000, mean grad: 0.000048\n",
      "training loss: 1.344462 acc: 43.750, mean grad: -0.000001\n",
      "training loss: 2.390469 acc:  0.000, mean grad: 0.000025\n",
      "training loss: 1.393388 acc: 56.250, mean grad: -0.000001\n",
      "training loss: 1.934622 acc:  0.000, mean grad: 0.000010\n",
      "training loss: 1.448088 acc: 56.250, mean grad: -0.000004\n",
      "training loss: 1.678655 acc: 11.111, mean grad: -0.000002\n",
      "training loss: 1.492170 acc: 68.750, mean grad: -0.000005\n",
      "training loss: 1.533465 acc: 22.222, mean grad: -0.000009\n",
      "training loss: 1.523448 acc: 37.500, mean grad: -0.000005\n",
      "training loss: 1.449243 acc: 44.444, mean grad: -0.000014\n",
      "training loss: 1.206879 acc: 31.250, mean grad: -0.000012\n",
      "training loss: 4.707879 acc:  0.000, mean grad: 0.000092\n",
      "training loss: 1.186726 acc: 50.000, mean grad: -0.000011\n",
      "training loss: 3.993129 acc:  0.000, mean grad: 0.000087\n",
      "training loss: 1.178786 acc: 50.000, mean grad: -0.000008\n",
      "training loss: 2.989313 acc:  0.000, mean grad: 0.000065\n",
      "training loss: 1.225854 acc: 56.250, mean grad: -0.000005\n",
      "training loss: 2.255109 acc:  0.000, mean grad: 0.000043\n",
      "training loss: 1.297131 acc: 62.500, mean grad: -0.000002\n",
      "training loss: 1.818798 acc:  0.000, mean grad: 0.000027\n",
      "training loss: 1.362943 acc: 56.250, mean grad: -0.000002\n",
      "training loss: 1.572865 acc: 22.222, mean grad: 0.000013\n",
      "training loss: 1.412617 acc: 31.250, mean grad: -0.000003\n",
      "training loss: 1.436002 acc: 55.556, mean grad: 0.000009\n",
      "training loss: 1.445940 acc: 31.250, mean grad: -0.000002\n",
      "training loss: 1.358584 acc: 77.778, mean grad: 0.000001\n",
      "training loss: 1.529285 acc: 37.500, mean grad: 0.000056\n",
      "training loss: 4.297166 acc:  0.000, mean grad: -0.000020\n",
      "training loss: 1.468475 acc: 37.500, mean grad: 0.000071\n",
      "training loss: 3.586364 acc:  0.000, mean grad: -0.000015\n",
      "training loss: 1.356253 acc: 56.250, mean grad: 0.000025\n",
      "training loss: 2.649247 acc:  0.000, mean grad: 0.000009\n",
      "training loss: 1.291616 acc: 56.250, mean grad: 0.000003\n",
      "training loss: 2.023616 acc:  0.000, mean grad: -0.000003\n",
      "training loss: 1.294523 acc: 62.500, mean grad: -0.000009\n",
      "training loss: 1.667419 acc: 11.111, mean grad: -0.000014\n",
      "training loss: 1.320470 acc: 68.750, mean grad: -0.000013\n",
      "training loss: 1.483186 acc: 22.222, mean grad: -0.000020\n",
      "training loss: 1.346628 acc: 56.250, mean grad: -0.000014\n",
      "training loss: 1.385942 acc: 22.222, mean grad: -0.000023\n",
      "training loss: 1.366595 acc: 56.250, mean grad: -0.000015\n",
      "training loss: 1.331794 acc: 44.444, mean grad: -0.000025\n",
      "training loss: 1.481571 acc: 31.250, mean grad: 0.000051\n",
      "training loss: 4.813147 acc:  0.000, mean grad: 0.000074\n",
      "training loss: 1.417532 acc: 50.000, mean grad: 0.000043\n",
      "training loss: 4.075671 acc:  0.000, mean grad: 0.000053\n",
      "training loss: 1.332403 acc: 56.250, mean grad: 0.000027\n",
      "training loss: 3.068277 acc:  0.000, mean grad: 0.000029\n",
      "training loss: 1.314139 acc: 56.250, mean grad: 0.000008\n",
      "training loss: 2.331306 acc:  0.000, mean grad: 0.000019\n",
      "training loss: 1.349726 acc: 56.250, mean grad: 0.000001\n",
      "training loss: 1.887270 acc:  0.000, mean grad: -0.000015\n",
      "training loss: 1.387240 acc: 56.250, mean grad: -0.000009\n",
      "training loss: 1.645629 acc: 22.222, mean grad: 0.000019\n",
      "training loss: 1.430234 acc: 62.500, mean grad: -0.000008\n",
      "training loss: 1.508098 acc: 44.444, mean grad: -0.000027\n",
      "training loss: 1.455338 acc: 62.500, mean grad: -0.000011\n",
      "training loss: 1.415202 acc: 66.667, mean grad: -0.000023\n",
      "training loss: 1.429306 acc: 12.500, mean grad: 0.000016\n",
      "training loss: 4.568882 acc:  0.000, mean grad: 0.000067\n",
      "training loss: 1.369119 acc: 25.000, mean grad: 0.000023\n",
      "training loss: 3.913003 acc:  0.000, mean grad: 0.000049\n",
      "training loss: 1.287332 acc: 37.500, mean grad: 0.000026\n",
      "training loss: 3.024944 acc:  0.000, mean grad: 0.000036\n",
      "training loss: 1.267484 acc: 56.250, mean grad: 0.000006\n",
      "training loss: 2.348486 acc:  0.000, mean grad: 0.000022\n",
      "training loss: 1.298853 acc: 56.250, mean grad: -0.000000\n",
      "training loss: 1.927101 acc:  0.000, mean grad: 0.000012\n",
      "training loss: 1.344027 acc: 62.500, mean grad: -0.000004\n",
      "training loss: 1.685345 acc:  0.000, mean grad: 0.000003\n",
      "training loss: 1.384762 acc: 62.500, mean grad: -0.000004\n",
      "training loss: 1.550108 acc: 22.222, mean grad: -0.000002\n",
      "training loss: 1.415382 acc: 56.250, mean grad: -0.000003\n",
      "training loss: 1.472065 acc: 22.222, mean grad: -0.000005\n",
      "training loss: 1.298178 acc: 43.750, mean grad: 0.000002\n",
      "training loss: 4.394782 acc:  0.000, mean grad: 0.000015\n",
      "training loss: 1.282364 acc: 50.000, mean grad: -0.000033\n",
      "training loss: 3.639045 acc:  0.000, mean grad: 0.000139\n",
      "training loss: 1.244374 acc: 56.250, mean grad: -0.000012\n",
      "training loss: 2.893582 acc:  0.000, mean grad: -0.000030\n",
      "training loss: 1.262408 acc: 68.750, mean grad: -0.000014\n",
      "training loss: 2.262267 acc:  0.000, mean grad: 0.000004\n",
      "training loss: 1.270772 acc: 81.250, mean grad: -0.000006\n",
      "training loss: 1.931254 acc:  0.000, mean grad: -0.000008\n",
      "training loss: 1.297051 acc: 87.500, mean grad: 0.000000\n",
      "training loss: 1.731387 acc:  0.000, mean grad: -0.000015\n",
      "training loss: 1.324776 acc: 87.500, mean grad: 0.000005\n",
      "training loss: 1.614717 acc:  0.000, mean grad: -0.000020\n",
      "training loss: 1.345462 acc: 81.250, mean grad: 0.000008\n",
      "training loss: 1.545525 acc: 11.111, mean grad: -0.000022\n",
      "training loss: 1.500809 acc:  6.250, mean grad: -0.000014\n",
      "training loss: 4.475643 acc:  0.000, mean grad: 0.000034\n",
      "training loss: 1.397105 acc:  6.250, mean grad: -0.000010\n",
      "training loss: 3.770926 acc:  0.000, mean grad: 0.000003\n",
      "training loss: 1.279746 acc: 12.500, mean grad: -0.000008\n",
      "training loss: 2.837192 acc:  0.000, mean grad: 0.000075\n",
      "training loss: 1.221222 acc: 68.750, mean grad: -0.000006\n",
      "training loss: 2.252713 acc:  0.000, mean grad: -0.000028\n",
      "training loss: 1.287942 acc: 56.250, mean grad: -0.000016\n",
      "training loss: 1.802547 acc:  0.000, mean grad: 0.000034\n",
      "training loss: 1.315469 acc: 62.500, mean grad: -0.000011\n",
      "training loss: 1.617504 acc:  0.000, mean grad: -0.000049\n",
      "training loss: 1.354487 acc: 62.500, mean grad: -0.000020\n",
      "training loss: 1.490851 acc: 22.222, mean grad: 0.000011\n",
      "training loss: 1.373171 acc: 56.250, mean grad: -0.000020\n",
      "training loss: 1.408797 acc: 33.333, mean grad: 0.000007\n",
      "training loss: 1.609222 acc: 18.750, mean grad: -0.000019\n",
      "training loss: 4.265317 acc:  0.000, mean grad: -0.000010\n",
      "training loss: 1.489163 acc: 18.750, mean grad: -0.000032\n",
      "training loss: 3.569937 acc:  0.000, mean grad: 0.000015\n",
      "training loss: 1.321510 acc: 31.250, mean grad: 0.000035\n",
      "training loss: 2.654111 acc:  0.000, mean grad: -0.000035\n",
      "training loss: 1.347504 acc: 37.500, mean grad: 0.000027\n",
      "training loss: 2.060844 acc:  0.000, mean grad: -0.000006\n",
      "training loss: 1.386977 acc: 50.000, mean grad: -0.000017\n",
      "training loss: 1.763419 acc: 11.111, mean grad: -0.000004\n",
      "training loss: 1.443834 acc: 31.250, mean grad: 0.000032\n",
      "training loss: 1.594448 acc: 22.222, mean grad: -0.000008\n",
      "training loss: 1.440182 acc: 25.000, mean grad: 0.000031\n",
      "training loss: 1.507044 acc: 22.222, mean grad: -0.000012\n",
      "training loss: 1.447854 acc: 43.750, mean grad: 0.000004\n",
      "training loss: 1.449344 acc: 33.333, mean grad: -0.000011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.587500 acc: 25.000, mean grad: 0.000083\n",
      "training loss: 4.603962 acc:  0.000, mean grad: 0.000050\n",
      "training loss: 1.491851 acc: 25.000, mean grad: 0.000064\n",
      "training loss: 3.780923 acc:  0.000, mean grad: 0.000042\n",
      "training loss: 1.367273 acc: 31.250, mean grad: 0.000042\n",
      "training loss: 2.692118 acc:  0.000, mean grad: 0.000023\n",
      "training loss: 1.338830 acc: 37.500, mean grad: 0.000023\n",
      "training loss: 1.945354 acc:  0.000, mean grad: -0.000002\n",
      "training loss: 1.377036 acc: 43.750, mean grad: 0.000019\n",
      "training loss: 1.568284 acc: 33.333, mean grad: -0.000010\n",
      "training loss: 1.414522 acc: 25.000, mean grad: -0.000007\n",
      "training loss: 1.386450 acc: 55.556, mean grad: -0.000001\n",
      "training loss: 1.438939 acc: 31.250, mean grad: -0.000004\n",
      "training loss: 1.301220 acc: 66.667, mean grad: -0.000003\n",
      "training loss: 1.452508 acc: 25.000, mean grad: -0.000009\n",
      "training loss: 1.255824 acc: 77.778, mean grad: -0.000003\n",
      "training loss: 1.415954 acc: 18.750, mean grad: -0.000017\n",
      "training loss: 4.523097 acc:  0.000, mean grad: 0.000075\n",
      "training loss: 1.337457 acc: 25.000, mean grad: -0.000019\n",
      "training loss: 3.808237 acc:  0.000, mean grad: 0.000087\n",
      "training loss: 1.300432 acc: 37.500, mean grad: -0.000020\n",
      "training loss: 2.813344 acc:  0.000, mean grad: 0.000059\n",
      "training loss: 1.358116 acc: 43.750, mean grad: -0.000013\n",
      "training loss: 2.148509 acc:  0.000, mean grad: -0.000014\n",
      "training loss: 1.385428 acc: 43.750, mean grad: -0.000009\n",
      "training loss: 1.762543 acc:  0.000, mean grad: 0.000019\n",
      "training loss: 1.460713 acc: 43.750, mean grad: -0.000012\n",
      "training loss: 1.562579 acc: 33.333, mean grad: -0.000016\n",
      "training loss: 1.494299 acc: 31.250, mean grad: -0.000018\n",
      "training loss: 1.422763 acc: 55.556, mean grad: -0.000012\n",
      "training loss: 1.514418 acc: 31.250, mean grad: -0.000018\n",
      "training loss: 1.362066 acc: 77.778, mean grad: -0.000011\n",
      "training loss: 1.586222 acc: 18.750, mean grad: 0.000015\n",
      "training loss: 4.261901 acc:  0.000, mean grad: 0.000080\n",
      "training loss: 1.495789 acc: 25.000, mean grad: 0.000014\n",
      "training loss: 3.474427 acc:  0.000, mean grad: 0.000071\n",
      "training loss: 1.370306 acc: 31.250, mean grad: 0.000008\n",
      "training loss: 2.462685 acc:  0.000, mean grad: 0.000009\n",
      "training loss: 1.376192 acc: 56.250, mean grad: -0.000003\n",
      "training loss: 1.834329 acc:  0.000, mean grad: 0.000054\n",
      "training loss: 1.395841 acc: 50.000, mean grad: -0.000004\n",
      "training loss: 1.556330 acc: 22.222, mean grad: -0.000023\n",
      "training loss: 1.402815 acc: 37.500, mean grad: -0.000008\n",
      "training loss: 1.386600 acc: 55.556, mean grad: -0.000004\n",
      "training loss: 1.415538 acc: 37.500, mean grad: -0.000011\n",
      "training loss: 1.316522 acc: 66.667, mean grad: -0.000004\n",
      "training loss: 1.419913 acc: 37.500, mean grad: -0.000013\n",
      "training loss: 1.278437 acc: 66.667, mean grad: -0.000001\n",
      "training loss: 1.399981 acc: 50.000, mean grad: -0.000009\n",
      "training loss: 4.276392 acc:  0.000, mean grad: 0.000081\n",
      "training loss: 1.354888 acc: 37.500, mean grad: -0.000012\n",
      "training loss: 3.558335 acc:  0.000, mean grad: 0.000089\n",
      "training loss: 1.325479 acc: 50.000, mean grad: -0.000005\n",
      "training loss: 2.613698 acc:  0.000, mean grad: 0.000019\n",
      "training loss: 1.346194 acc: 50.000, mean grad: -0.000014\n",
      "training loss: 1.982293 acc:  0.000, mean grad: 0.000062\n",
      "training loss: 1.424588 acc: 56.250, mean grad: -0.000014\n",
      "training loss: 1.630680 acc:  0.000, mean grad: 0.000002\n",
      "training loss: 1.470562 acc: 37.500, mean grad: -0.000020\n",
      "training loss: 1.432117 acc: 33.333, mean grad: -0.000010\n",
      "training loss: 1.488496 acc: 25.000, mean grad: -0.000009\n",
      "training loss: 1.338086 acc: 55.556, mean grad: -0.000010\n",
      "training loss: 1.501409 acc: 31.250, mean grad: -0.000004\n",
      "training loss: 1.285038 acc: 77.778, mean grad: -0.000020\n",
      "training loss: 1.474045 acc: 25.000, mean grad: 0.000092\n",
      "training loss: 4.362991 acc:  0.000, mean grad: 0.000162\n",
      "training loss: 1.359115 acc: 31.250, mean grad: 0.000060\n",
      "training loss: 3.594352 acc:  0.000, mean grad: 0.000080\n",
      "training loss: 1.302474 acc: 31.250, mean grad: 0.000040\n",
      "training loss: 2.528365 acc:  0.000, mean grad: 0.000050\n",
      "training loss: 1.293765 acc: 50.000, mean grad: -0.000001\n",
      "training loss: 1.892203 acc:  0.000, mean grad: 0.000027\n",
      "training loss: 1.342372 acc: 43.750, mean grad: -0.000016\n",
      "training loss: 1.581742 acc: 33.333, mean grad: 0.000015\n",
      "training loss: 1.382805 acc: 50.000, mean grad: -0.000018\n",
      "training loss: 1.436377 acc: 44.444, mean grad: 0.000005\n",
      "training loss: 1.406778 acc: 43.750, mean grad: -0.000020\n",
      "training loss: 1.361200 acc: 55.556, mean grad: -0.000001\n",
      "training loss: 1.422065 acc: 31.250, mean grad: -0.000020\n",
      "training loss: 1.318943 acc: 77.778, mean grad: -0.000004\n",
      "training loss: 1.453723 acc: 25.000, mean grad: 0.000033\n",
      "training loss: 4.150424 acc:  0.000, mean grad: 0.000185\n",
      "training loss: 1.484000 acc: 31.250, mean grad: 0.000031\n",
      "training loss: 3.403877 acc:  0.000, mean grad: 0.000108\n",
      "training loss: 1.325927 acc: 50.000, mean grad: 0.000015\n",
      "training loss: 2.564582 acc:  0.000, mean grad: 0.000041\n",
      "training loss: 1.296727 acc: 50.000, mean grad: 0.000009\n",
      "training loss: 2.039258 acc:  0.000, mean grad: 0.000004\n",
      "training loss: 1.364169 acc: 50.000, mean grad: 0.000002\n",
      "training loss: 1.751461 acc:  0.000, mean grad: -0.000009\n",
      "training loss: 1.413227 acc: 50.000, mean grad: -0.000005\n",
      "training loss: 1.595497 acc:  0.000, mean grad: -0.000012\n",
      "training loss: 1.447848 acc: 56.250, mean grad: -0.000015\n",
      "training loss: 1.506710 acc: 22.222, mean grad: -0.000016\n",
      "training loss: 1.471365 acc: 50.000, mean grad: -0.000009\n",
      "training loss: 1.460876 acc: 33.333, mean grad: -0.000015\n",
      "training loss: 1.256759 acc: 56.250, mean grad: -0.000017\n",
      "training loss: 4.172816 acc:  0.000, mean grad: 0.000102\n",
      "training loss: 1.232883 acc: 62.500, mean grad: -0.000018\n",
      "training loss: 3.462274 acc:  0.000, mean grad: 0.000030\n",
      "training loss: 1.223734 acc: 68.750, mean grad: -0.000013\n",
      "training loss: 2.432290 acc:  0.000, mean grad: 0.000017\n",
      "training loss: 1.275913 acc: 75.000, mean grad: -0.000013\n",
      "training loss: 1.774013 acc: 11.111, mean grad: -0.000001\n",
      "training loss: 1.351597 acc: 75.000, mean grad: -0.000017\n",
      "training loss: 1.487340 acc: 11.111, mean grad: -0.000010\n",
      "training loss: 1.401725 acc: 62.500, mean grad: -0.000015\n",
      "training loss: 1.353341 acc: 11.111, mean grad: -0.000016\n",
      "training loss: 1.426063 acc: 50.000, mean grad: -0.000013\n",
      "training loss: 1.280962 acc: 55.556, mean grad: -0.000018\n",
      "training loss: 1.441514 acc: 50.000, mean grad: -0.000009\n",
      "training loss: 1.240483 acc: 66.667, mean grad: -0.000021\n",
      "training loss: 1.363183 acc: 25.000, mean grad: -0.000055\n",
      "training loss: 4.499015 acc:  0.000, mean grad: 0.000144\n",
      "training loss: 1.325258 acc: 31.250, mean grad: -0.000017\n",
      "training loss: 3.692487 acc:  0.000, mean grad: 0.000043\n",
      "training loss: 1.293142 acc: 31.250, mean grad: -0.000002\n",
      "training loss: 2.575735 acc:  0.000, mean grad: 0.000066\n",
      "training loss: 1.298209 acc: 56.250, mean grad: -0.000002\n",
      "training loss: 1.861510 acc:  0.000, mean grad: -0.000016\n",
      "training loss: 1.431286 acc: 43.750, mean grad: 0.000005\n",
      "training loss: 1.493113 acc: 22.222, mean grad: -0.000011\n",
      "training loss: 1.448403 acc: 31.250, mean grad: 0.000021\n",
      "training loss: 1.336041 acc: 44.444, mean grad: -0.000025\n",
      "training loss: 1.469192 acc: 31.250, mean grad: 0.000022\n",
      "training loss: 1.255233 acc: 77.778, mean grad: -0.000029\n",
      "training loss: 1.475291 acc: 37.500, mean grad: 0.000019\n",
      "training loss: 1.202308 acc: 88.889, mean grad: -0.000028\n",
      "training loss: 1.224314 acc: 50.000, mean grad: 0.000027\n",
      "training loss: 4.542638 acc:  0.000, mean grad: -0.000024\n",
      "training loss: 1.231550 acc: 43.750, mean grad: 0.000026\n",
      "training loss: 3.637738 acc:  0.000, mean grad: 0.000009\n",
      "training loss: 1.235409 acc: 62.500, mean grad: 0.000011\n",
      "training loss: 2.487815 acc:  0.000, mean grad: 0.000003\n",
      "training loss: 1.309404 acc: 62.500, mean grad: -0.000006\n",
      "training loss: 1.836619 acc:  0.000, mean grad: -0.000017\n",
      "training loss: 1.391216 acc: 56.250, mean grad: -0.000005\n",
      "training loss: 1.537022 acc: 11.111, mean grad: -0.000020\n",
      "training loss: 1.425171 acc: 43.750, mean grad: 0.000004\n",
      "training loss: 1.391361 acc: 33.333, mean grad: -0.000009\n",
      "training loss: 1.441374 acc: 31.250, mean grad: -0.000006\n",
      "training loss: 1.308886 acc: 44.444, mean grad: -0.000016\n",
      "training loss: 1.452685 acc: 31.250, mean grad: -0.000004\n",
      "training loss: 1.262269 acc: 88.889, mean grad: -0.000011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.298726 acc: 50.000, mean grad: 0.000061\n",
      "training loss: 4.554849 acc:  0.000, mean grad: 0.000215\n",
      "training loss: 1.209955 acc: 50.000, mean grad: 0.000018\n",
      "training loss: 3.613500 acc:  0.000, mean grad: 0.000066\n",
      "training loss: 1.186527 acc: 56.250, mean grad: 0.000015\n",
      "training loss: 2.624664 acc:  0.000, mean grad: 0.000029\n",
      "training loss: 1.234597 acc: 68.750, mean grad: 0.000012\n",
      "training loss: 1.990861 acc:  0.000, mean grad: 0.000007\n",
      "training loss: 1.306048 acc: 93.750, mean grad: 0.000013\n",
      "training loss: 1.663482 acc:  0.000, mean grad: -0.000005\n",
      "training loss: 1.361376 acc: 93.750, mean grad: 0.000014\n",
      "training loss: 1.501121 acc: 44.444, mean grad: -0.000010\n",
      "training loss: 1.397456 acc: 75.000, mean grad: 0.000013\n",
      "training loss: 1.416961 acc: 55.556, mean grad: -0.000012\n",
      "training loss: 1.421760 acc: 75.000, mean grad: 0.000014\n",
      "training loss: 1.372232 acc: 55.556, mean grad: -0.000014\n",
      "training loss: 1.495074 acc: 18.750, mean grad: 0.000040\n",
      "training loss: 4.610948 acc:  0.000, mean grad: 0.000090\n",
      "training loss: 1.424417 acc: 25.000, mean grad: 0.000038\n",
      "training loss: 3.742836 acc:  0.000, mean grad: 0.000099\n",
      "training loss: 1.272348 acc: 56.250, mean grad: 0.000007\n",
      "training loss: 2.708881 acc:  0.000, mean grad: 0.000025\n",
      "training loss: 1.315112 acc: 50.000, mean grad: -0.000001\n",
      "training loss: 2.046503 acc:  0.000, mean grad: -0.000000\n",
      "training loss: 1.390922 acc: 56.250, mean grad: -0.000004\n",
      "training loss: 1.715367 acc:  0.000, mean grad: -0.000008\n",
      "training loss: 1.452724 acc: 43.750, mean grad: -0.000008\n",
      "training loss: 1.543993 acc: 33.333, mean grad: -0.000015\n",
      "training loss: 1.492662 acc: 43.750, mean grad: -0.000006\n",
      "training loss: 1.447458 acc: 44.444, mean grad: -0.000021\n",
      "training loss: 1.516733 acc: 25.000, mean grad: -0.000003\n",
      "training loss: 1.391627 acc: 55.556, mean grad: -0.000023\n",
      "training loss: 1.657886 acc: 31.250, mean grad: 0.000105\n",
      "training loss: 4.195115 acc:  0.000, mean grad: 0.000076\n",
      "training loss: 1.502881 acc: 37.500, mean grad: 0.000115\n",
      "training loss: 3.580420 acc:  0.000, mean grad: 0.000047\n",
      "training loss: 1.276113 acc: 43.750, mean grad: 0.000001\n",
      "training loss: 2.733087 acc:  0.000, mean grad: 0.000027\n",
      "training loss: 1.275116 acc: 56.250, mean grad: 0.000003\n",
      "training loss: 2.146228 acc:  0.000, mean grad: 0.000009\n",
      "training loss: 1.325475 acc: 62.500, mean grad: -0.000003\n",
      "training loss: 1.799257 acc:  0.000, mean grad: -0.000006\n",
      "training loss: 1.373619 acc: 56.250, mean grad: -0.000005\n",
      "training loss: 1.607471 acc: 22.222, mean grad: -0.000010\n",
      "training loss: 1.405046 acc: 56.250, mean grad: -0.000010\n",
      "training loss: 1.499937 acc: 33.333, mean grad: -0.000008\n",
      "training loss: 1.426896 acc: 56.250, mean grad: -0.000011\n",
      "training loss: 1.438806 acc: 44.444, mean grad: -0.000012\n",
      "training loss: 1.266061 acc: 25.000, mean grad: -0.000056\n",
      "training loss: 4.629443 acc:  0.000, mean grad: 0.000042\n",
      "training loss: 1.234760 acc: 25.000, mean grad: -0.000052\n",
      "training loss: 3.855789 acc:  0.000, mean grad: 0.000027\n",
      "training loss: 1.191141 acc: 50.000, mean grad: -0.000030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sjadon/anaconda3/envs/mysite/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/sjadon/anaconda3/envs/mysite/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/sjadon/anaconda3/envs/mysite/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/sjadon/anaconda3/envs/mysite/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/Users/sjadon/anaconda3/envs/mysite/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/sjadon/anaconda3/envs/mysite/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/sjadon/anaconda3/envs/mysite/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/sjadon/anaconda3/envs/mysite/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/Users/sjadon/anaconda3/envs/mysite/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/sjadon/anaconda3/envs/mysite/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/sjadon/anaconda3/envs/mysite/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/sjadon/anaconda3/envs/mysite/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sjadon/anaconda3/envs/mysite/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/sjadon/anaconda3/envs/mysite/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/sjadon/anaconda3/envs/mysite/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/sjadon/anaconda3/envs/mysite/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-cbc54f70c37d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" BEGIN TRAINING: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-fbd39c409c0a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mlearner_w_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mlearner_wo_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mcI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_learner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearner_w_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetalearner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# Train meta-learner with validation loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-1443ec7d4e4e>\u001b[0m in \u001b[0;36mtrain_learner\u001b[0;34m(learner_w_grad, metalearner, train_input, train_target, args)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mlearner_w_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlearner_w_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mysite/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mysite/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    args_train={'mode':'train','n_shot':5,'n_eval':15,'n_class':5,'input_size':4,'hidden_size':20,'lr':1e-3,'episode':500,\n",
    "      'episode_val':100,'epoch':8,'batch_size':16,'image_size':84,'grad_clip':0.25,'bn_momentum': 0.95,'bn_eps': 1e-3,\n",
    "       'data': \"miniimagenet\",'data_root': \"./data/miniImagenet/\", 'resume': None}\n",
    "    \n",
    "    \n",
    "    args_test={'mode':'test','n_shot':5,'n_eval':15,'n_class':5,'input_size':4,'hidden_size':20,'lr':1e-3,'episode':500,\n",
    "      'episode_val':100,'epoch':8,'batch_size':16,'image_size':84,'grad_clip':0.25,'bn_momentum': 0.95,'bn_eps': 1e-3,\n",
    "       'data': \"miniimagenet\",'data_root': \"./data/miniImagenet/\", 'resume': None}\n",
    "    \n",
    "    \n",
    "    print (\" BEGIN TRAINING: \")\n",
    "    main(args_train)\n",
    "    \n",
    "    \n",
    "    print (\"BEGIN TESTING\")\n",
    "    main(args_test)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
